{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1887a19b",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "execution": {
     "iopub.execute_input": "2024-11-19T14:58:40.871473Z",
     "iopub.status.busy": "2024-11-19T14:58:40.868621Z",
     "iopub.status.idle": "2024-11-19T15:02:00.014669Z",
     "shell.execute_reply": "2024-11-19T15:02:00.012202Z"
    },
    "papermill": {
     "duration": 199.1555,
     "end_time": "2024-11-19T15:02:00.017737",
     "exception": false,
     "start_time": "2024-11-19T14:58:40.862237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:httr’:\n",
      "\n",
      "    progress\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:xgboost’:\n",
      "\n",
      "    slice\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: magrittr\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘imager’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:magrittr’:\n",
      "\n",
      "    add\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    where\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    convolve, spectrum\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:graphics’:\n",
      "\n",
      "    frame\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    save.image\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘EBImage’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:imager’:\n",
      "\n",
      "    channel, dilate, display, erode, resize, watershed\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in training dataset:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benign  Early    Pre    Pro \n",
      "   323    631    617    515 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Class Weights (based on training dataset):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$`0`\n",
      "[1] 1.614551\n",
      "\n",
      "$`1`\n",
      "[1] 0.8264659\n",
      "\n",
      "$`2`\n",
      "[1] 0.8452188\n",
      "\n",
      "$`3`\n",
      "[1] 1.012621\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in training dataset: 2086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of class counts: 2086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum weight ratio: 1.95356 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of weight calculation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign: count=323, weight=1.6146, count*weight=521.5000\n",
      "Early: count=631, weight=0.8265, count*weight=521.5000\n",
      "Pre: count=617, weight=0.8452, count*weight=521.5000\n",
      "Pro: count=515, weight=1.0126, count*weight=521.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing features from 1000 training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in get_feature_params(train_df$image_filename, sample_size):\n",
      "“Inconsistent feature lengths detected in training data!”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_lengths\n",
      " 50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69 \n",
      "  2   7  25  49  53  38  46  26  28  19  15  51 218 206  34  22  12   1   1 106 \n",
      " 70  71 \n",
      " 39   2 \n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "  50.00   58.00   62.00   61.33   63.00   71.00 \n",
      "\n",
      "Feature extraction analysis summary:\n",
      "Modal feature length: 62 \n",
      "Success rate: 100.00% \n",
      "Length consistency: Inconsistent \n",
      "\n",
      "Feature length distribution:\n",
      "feature_lengths\n",
      " 50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69 \n",
      "  2   7  25  49  53  38  46  26  28  19  15  51 218 206  34  22  12   1   1 106 \n",
      " 70  71 \n",
      " 39   2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training image 100 of 2086\n",
      "Processing training image 200 of 2086\n",
      "Processing training image 300 of 2086\n",
      "Processing training image 400 of 2086\n",
      "Processing training image 500 of 2086\n",
      "Processing training image 600 of 2086\n",
      "Processing training image 700 of 2086\n",
      "Processing training image 800 of 2086\n",
      "Processing training image 900 of 2086\n",
      "Processing training image 1000 of 2086\n",
      "Processing training image 1100 of 2086\n",
      "Processing training image 1200 of 2086\n",
      "Processing training image 1300 of 2086\n",
      "Processing training image 1400 of 2086\n",
      "Processing training image 1500 of 2086\n",
      "Processing training image 1600 of 2086\n",
      "Processing training image 1700 of 2086\n",
      "Processing training image 1800 of 2086\n",
      "Processing training image 1900 of 2086\n",
      "Processing training image 2000 of 2086\n",
      "\n",
      "NA counts in training dataset:\n",
      "feature_51 feature_52 feature_53 feature_54 feature_55 feature_56 feature_57 \n",
      "         3         13         58        162        272        373        460 \n",
      "feature_58 feature_59 feature_60 feature_61 feature_62 \n",
      "       514        563        606        647        743 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting validation features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation image 100 of 652\n",
      "Processing validation image 200 of 652\n",
      "Processing validation image 300 of 652\n",
      "Processing validation image 400 of 652\n",
      "Processing validation image 500 of 652\n",
      "Processing validation image 600 of 652\n",
      "\n",
      "NA counts in validation dataset:\n",
      "feature_51 feature_52 feature_53 feature_54 feature_55 feature_56 feature_57 \n",
      "         1          4         15         59         86        120        142 \n",
      "feature_58 feature_59 feature_60 feature_61 feature_62 \n",
      "       158        171        186        199        220 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test image 100 of 518\n",
      "Processing test image 200 of 518\n",
      "Processing test image 300 of 518\n",
      "Processing test image 400 of 518\n",
      "Processing test image 500 of 518\n",
      "\n",
      "NA counts in test dataset:\n",
      "feature_52 feature_53 feature_54 feature_55 feature_56 feature_57 feature_58 \n",
      "         1          9         26         58         82        101        117 \n",
      "feature_59 feature_60 feature_61 feature_62 \n",
      "       121        130        138        169 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features removed due to near-zero variance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t2086 obs. of  62 variables:\n",
      " $ feature_1 : num  1.78e-04 1.33e-04 4.44e-05 2.67e-04 4.44e-05 ...\n",
      " $ feature_2 : num  9.20e-03 8.89e-05 0.00 5.33e-04 8.89e-05 ...\n",
      " $ feature_3 : num  2.93e-02 1.33e-04 0.00 1.51e-03 4.44e-05 ...\n",
      " $ feature_4 : num  3.35e-02 8.89e-05 4.44e-05 4.71e-03 3.11e-04 ...\n",
      " $ feature_5 : num  3.16e-02 1.78e-04 8.89e-05 1.34e-02 5.33e-03 ...\n",
      " $ feature_6 : num  3.04e-02 2.36e-03 4.44e-05 6.04e-02 3.43e-02 ...\n",
      " $ feature_7 : num  2.82e-02 4.67e-03 8.89e-05 1.81e-01 1.46e-01 ...\n",
      " $ feature_8 : num  2.76e-02 1.20e-02 4.44e-05 2.04e-01 1.47e-01 ...\n",
      " $ feature_9 : num  0.023022 0.043333 0.000978 0.1372 0.072533 ...\n",
      " $ feature_10: num  0.0225 0.0687 0.0144 0.0884 0.0597 ...\n",
      " $ feature_11: num  0.0243 0.138 0.055 0.068 0.0721 ...\n",
      " $ feature_12: num  0.027 0.117 0.0738 0.0779 0.1145 ...\n",
      " $ feature_13: num  0.0565 0.0708 0.0882 0.1634 0.1958 ...\n",
      " $ feature_14: num  2.48e-01 7.47e-02 9.41e-02 8.89e-05 1.52e-01 ...\n",
      " $ feature_15: num  4.09e-01 1.03e-01 7.18e-02 4.44e-05 4.44e-05 ...\n",
      " $ feature_16: num  2.22e-04 1.73e-01 7.88e-02 2.67e-04 4.44e-05 ...\n",
      " $ feature_17: num  0.001778 0.191956 0.522667 0.000756 0.000222 ...\n",
      " $ feature_18: num  3.56e-03 4.44e-05 4.44e-05 1.87e-03 4.53e-03 ...\n",
      " $ feature_19: num  0.006533 0.000178 0 0.004311 0.016622 ...\n",
      " $ feature_20: num  0.01782 0 0 0.00929 0.01244 ...\n",
      " $ feature_21: num  3.30e-02 2.67e-04 4.44e-05 2.07e-02 1.96e-02 ...\n",
      " $ feature_22: num  3.57e-02 1.24e-03 8.89e-05 5.22e-02 6.04e-02 ...\n",
      " $ feature_23: num  3.31e-02 4.58e-03 8.89e-05 1.08e-01 1.22e-01 ...\n",
      " $ feature_24: num  0.0303 0.0105 0.0004 0.1651 0.1301 ...\n",
      " $ feature_25: num  0.02631 0.03124 0.00151 0.15476 0.08929 ...\n",
      " $ feature_26: num  0.02636 0.02596 0.00516 0.11 0.08578 ...\n",
      " $ feature_27: num  0.02711 0.02102 0.00791 0.08107 0.10227 ...\n",
      " $ feature_28: num  0.0355 0.0217 0.0194 0.069 0.3568 ...\n",
      " $ feature_29: num  7.03e-02 4.14e-02 5.42e-02 2.22e-01 4.44e-05 ...\n",
      " $ feature_30: num  6.52e-01 8.98e-02 9.01e-02 4.44e-05 0.00 ...\n",
      " $ feature_31: num  4.44e-05 1.09e-01 9.50e-02 4.44e-05 8.89e-05 ...\n",
      " $ feature_32: num  4.44e-05 8.49e-02 8.45e-02 8.89e-05 4.44e-05 ...\n",
      " $ feature_33: num  8.89e-05 7.12e-02 8.90e-02 8.89e-05 8.89e-05 ...\n",
      " $ feature_34: num  4.44e-05 9.73e-02 5.53e-01 1.78e-04 4.44e-05 ...\n",
      " $ feature_35: num  8.89e-05 3.90e-01 4.44e-05 8.89e-05 4.44e-05 ...\n",
      " $ feature_36: num  0.00 4.44e-05 0.00 5.78e-04 2.67e-04 ...\n",
      " $ feature_37: num  1.96e-03 0.00 4.44e-05 1.55e-02 6.67e-04 ...\n",
      " $ feature_38: num  1.67e-02 8.89e-05 0.00 1.57e-01 1.10e-02 ...\n",
      " $ feature_39: num  4.70e-02 4.44e-05 8.89e-05 2.48e-01 1.11e-01 ...\n",
      " $ feature_40: num  7.39e-02 8.89e-05 4.44e-05 1.80e-01 2.06e-01 ...\n",
      " $ feature_41: num  8.51e-02 4.44e-05 8.89e-05 2.73e-01 1.15e-01 ...\n",
      " $ feature_42: num  0.240533 0.000133 0 0.124133 0.555822 ...\n",
      " $ feature_43: num  5.13e-01 8.89e-05 0.00 1.28e-02 1.41e-02 ...\n",
      " $ feature_44: num  0.021378 0.000444 0 0.139606 -0.360766 ...\n",
      " $ feature_45: num  0.02406 0.010667 0.000178 -0.925475 -1.184829 ...\n",
      " $ feature_46: num  -1.454022 0.100933 0.000133 0.811285 0.848871 ...\n",
      " $ feature_47: num  0.657622 0.144889 0.000756 0.113079 0.118915 ...\n",
      " $ feature_48: num  0.879 0.109 0.016 0.996 0.997 ...\n",
      " $ feature_49: num  0.155 0.634 0.114 0.383 0.418 ...\n",
      " $ feature_50: num  0.991 0.0181 0.2556 0.036 0.0276 ...\n",
      " $ feature_51: num  0.3899 -0.6919 0.6129 0.0286 0.0278 ...\n",
      " $ feature_52: num  2.76e-02 -4.67e-01 9.28e-03 2.25e+04 2.25e+04 ...\n",
      " $ feature_53: num  0.0415 0.8467 -0.9 598 596 ...\n",
      " $ feature_54: num  4363.2 0.135 -0.311 0.789 0.796 ...\n",
      " $ feature_55: num  137 0.998 0.914 0 0 ...\n",
      " $ feature_56: num  5.9679 0.2123 0.0963 0 0 ...\n",
      " $ feature_57: num  0 0.0262 1 0 0 ...\n",
      " $ feature_58: num  0 0.0289 0.1967 0 0 ...\n",
      " $ feature_59: num  0.00 2.23e+04 2.92e-02 0.00 0.00 ...\n",
      " $ feature_60: num  0 607 0.0318 0 0 ...\n",
      " $ feature_61: num  0.00 7.62e-01 2.25e+04 0.00 0.00 ...\n",
      " $ feature_62: num  0 0 596 0 0 ...\n"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries\n",
    "library(xgboost)\n",
    "library(caret)\n",
    "library(dplyr)\n",
    "library(imager)\n",
    "library(EBImage)  # For bwlabel and shape feature extraction\n",
    "library(ggplot2)\n",
    "library(e1071)  # For skewness and kurtosis\n",
    "\n",
    "# Set the path to your dataset\n",
    "data_dir <- \"/kaggle/input/leukemia-images/Original\"\n",
    "\n",
    "# Define image size and parameters\n",
    "img_height <- 150\n",
    "img_width <- 150\n",
    "batch_size <- 32  # Note: batch_size is defined but not used in the code\n",
    "\n",
    "# Create a list to hold all file paths and corresponding labels\n",
    "file_paths <- character()  # More explicit initialization\n",
    "class_labels <- character()\n",
    "\n",
    "# Loop through each class to gather file paths and labels\n",
    "for (class in c(\"Benign\", \"Early\", \"Pre\", \"Pro\")) {\n",
    "  class_path <- file.path(data_dir, class)\n",
    "  \n",
    "  # Add error handling for directory access\n",
    "  if (!dir.exists(class_path)) {\n",
    "    stop(paste(\"Directory not found:\", class_path))\n",
    "  }\n",
    "  \n",
    "  files <- list.files(class_path, full.names = TRUE, pattern = \"\\\\.(?i)(jpg|jpeg|png|tiff)$\")\n",
    "  \n",
    "  # Ensure files are unique\n",
    "  unique_files <- unique(files)\n",
    "  \n",
    "  file_paths <- c(file_paths, unique_files)\n",
    "  class_labels <- c(class_labels, rep(class, length(unique_files)))\n",
    "}\n",
    "\n",
    "# Create a DataFrame with only the required structure\n",
    "full_df <- data.frame(\n",
    "  image_filename = file_paths,\n",
    "  class = factor(class_labels, levels = c(\"Benign\", \"Early\", \"Pre\", \"Pro\")),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "# Define class-to-index mapping\n",
    "classes <- c(\"Benign\", \"Early\", \"Pre\", \"Pro\")\n",
    "class_to_index <- setNames(0:(length(classes) - 1), classes)\n",
    "\n",
    "# Add numeric class column (0-based indexing)\n",
    "full_df$class_numeric <- as.numeric(full_df$class) - 1\n",
    "\n",
    "# Stratified split into training (64%), validation (20%), and test (16%) sets\n",
    "set.seed(123)\n",
    "trainIndex <- createDataPartition(full_df$class, p = 0.64, list = FALSE)\n",
    "train_df <- full_df[trainIndex, ]\n",
    "remaining_df <- full_df[-trainIndex, ]\n",
    "\n",
    "# Shuffle only train_df\n",
    "train_df <- train_df[sample(nrow(train_df)), ]\n",
    "\n",
    "validationIndex <- createDataPartition(remaining_df$class, p = 20/36, list = FALSE)\n",
    "val_df <- remaining_df[validationIndex, ]\n",
    "test_df <- remaining_df[-validationIndex, ]\n",
    "\n",
    "# Calculate class weights using only the training dataset\n",
    "n_samples <- nrow(train_df)  \n",
    "n_classes <- length(unique(train_df$class))\n",
    "\n",
    "# Get class frequencies from training dataset\n",
    "class_counts <- table(train_df$class_numeric)\n",
    "\n",
    "# Calculate balanced weights\n",
    "class_weights <- n_samples / (n_classes * class_counts)\n",
    "\n",
    "# Convert to a named list with numeric indices as names\n",
    "class_weights <- as.list(class_weights)\n",
    "names(class_weights) <- as.character(0:(n_classes-1))\n",
    "\n",
    "# Print diagnostics\n",
    "cat(\"\\nClass distribution in training dataset:\\n\")\n",
    "print(table(train_df$class))\n",
    "\n",
    "cat(\"\\nFinal Class Weights (based on training dataset):\\n\")\n",
    "print(class_weights)\n",
    "\n",
    "# Validation steps\n",
    "cat(\"\\nValidation:\\n\")\n",
    "cat(\"Total samples in training dataset:\", n_samples, \"\\n\")\n",
    "cat(\"Sum of class counts:\", sum(class_counts), \"\\n\")\n",
    "cat(\"Number of classes:\", n_classes, \"\\n\")\n",
    "cat(\"Maximum weight ratio:\", max(unlist(class_weights)) / min(unlist(class_weights)), \"\\n\\n\")\n",
    "\n",
    "# Additional validation checks\n",
    "cat(\"Verification of weight calculation:\\n\")\n",
    "for(i in 0:(n_classes-1)) {\n",
    "  class_name <- classes[i+1]\n",
    "  weight <- class_weights[[as.character(i)]]\n",
    "  count <- class_counts[as.character(i)]\n",
    "  cat(sprintf(\"%s: count=%d, weight=%.4f, count*weight=%.4f\\n\", \n",
    "              class_name, count, weight, count*weight))\n",
    "}\n",
    "\n",
    "# Features extraction function \n",
    "extract_features <- function(image_path, bins = 16, feature_params = NULL) {\n",
    "  # Load and resize the image using imager\n",
    "  image <- load.image(image_path)\n",
    "  image <- resize(image, img_width, img_height)\n",
    "  \n",
    "  # Convert image to array and split into color channels\n",
    "  red_channel <- as.vector(image[,,1,1])\n",
    "  green_channel <- as.vector(image[,,1,2])\n",
    "  blue_channel <- as.vector(image[,,1,3])\n",
    "  \n",
    "  # Color histogram features for each channel\n",
    "  red_hist <- hist(red_channel, breaks = bins, plot = FALSE)$counts / length(red_channel)\n",
    "  green_hist <- hist(green_channel, breaks = bins, plot = FALSE)$counts / length(green_channel)\n",
    "  blue_hist <- hist(blue_channel, breaks = bins, plot = FALSE)$counts / length(blue_channel)\n",
    "  \n",
    "  # Convert to grayscale for texture and edge features\n",
    "  gray_image <- 0.299 * red_channel + 0.587 * green_channel + 0.114 * blue_channel\n",
    "  \n",
    "  # Texture features: contrast, skewness, and kurtosis\n",
    "  contrast <- var(gray_image)\n",
    "  skewness <- e1071::skewness(gray_image)\n",
    "  kurtosis <- e1071::kurtosis(gray_image)\n",
    "  \n",
    "  # Statistical descriptors\n",
    "  mean_intensity <- mean(gray_image)\n",
    "  sd_intensity <- sd(gray_image)\n",
    "  max_intensity <- max(gray_image)\n",
    "  min_intensity <- min(gray_image)\n",
    "  \n",
    "  # Edge detection using gradient magnitude approximation (Sobel-like)\n",
    "  gray_img_cimg <- as.cimg(gray_image, x = img_width, y = img_height)\n",
    "  grad_x <- imgradient(gray_img_cimg, \"x\")\n",
    "  grad_y <- imgradient(gray_img_cimg, \"y\")\n",
    "  edge_magnitude <- sqrt(grad_x^2 + grad_y^2)  # Approximate edge strength\n",
    "  edge_intensity_mean <- mean(edge_magnitude)\n",
    "  edge_intensity_sd <- sd(edge_magnitude)\n",
    "  \n",
    "  # Shape features using segmentation (using EBImage)\n",
    "  gray_img_matrix <- matrix(gray_image, nrow = img_height, ncol = img_width)\n",
    "  binary_image <- gray_img_matrix > 0.5  # Apply threshold to get binary mask\n",
    "  binary_image <- as.Image(binary_image)  # Convert to EBImage object\n",
    "  \n",
    "  # Label the binary image\n",
    "  labeled_image <- bwlabel(binary_image)\n",
    "  \n",
    "  # Compute shape features\n",
    "  shape_features <- computeFeatures.shape(labeled_image)\n",
    "  \n",
    "  # Shape descriptors - Mean values of area, perimeter, and circularity\n",
    "  area_mean <- mean(shape_features[,\"s.area\"], na.rm = TRUE)\n",
    "  perimeter_mean <- mean(shape_features[,\"s.perimeter\"], na.rm = TRUE)\n",
    "  circularity_mean <- mean((4 * pi * shape_features[,\"s.area\"]) / (shape_features[,\"s.perimeter\"]^2), na.rm = TRUE)\n",
    "  \n",
    "  # Combine all features into a single vector\n",
    "  feature_vector <- c(red_hist, green_hist, blue_hist, contrast, skewness, kurtosis, \n",
    "                      mean_intensity, sd_intensity, max_intensity, min_intensity,\n",
    "                      edge_intensity_mean, edge_intensity_sd,\n",
    "                      area_mean, perimeter_mean, circularity_mean)\n",
    "  \n",
    "  return(feature_vector)\n",
    "}\n",
    "\n",
    "# Function to analyze feature lengths across training data\n",
    "get_feature_params <- function(train_image_paths, sample_size = NULL) {\n",
    "  # If sample_size is provided, randomly sample images for efficiency\n",
    "  if (!is.null(sample_size) && sample_size < length(train_image_paths)) {\n",
    "    set.seed(123)  # for reproducibility\n",
    "    image_paths <- sample(train_image_paths, sample_size)\n",
    "  } else {\n",
    "    image_paths <- train_image_paths\n",
    "  }\n",
    "  \n",
    "  cat(\"Analyzing features from\", length(image_paths), \"training images...\\n\")\n",
    "  \n",
    "  # Extract features from all training images and analyze\n",
    "  feature_lengths <- sapply(image_paths, function(path) {\n",
    "    tryCatch({\n",
    "      features <- extract_features(path)\n",
    "      length(features)\n",
    "    }, error = function(e) {\n",
    "      warning(sprintf(\"Error processing image %s: %s\", path, e$message))\n",
    "      return(NA)\n",
    "    })\n",
    "  })\n",
    "  \n",
    "  # Analyze feature length distribution\n",
    "  length_summary <- summary(feature_lengths)\n",
    "  length_table <- table(feature_lengths)\n",
    "  \n",
    "  # Check for inconsistencies\n",
    "  if (length(unique(feature_lengths[!is.na(feature_lengths)])) > 1) {\n",
    "    warning(\"Inconsistent feature lengths detected in training data!\")\n",
    "    print(length_table)\n",
    "    print(length_summary)\n",
    "  }\n",
    "  \n",
    "  # Get most common feature length (mode)\n",
    "  modal_length <- as.numeric(names(length_table)[which.max(length_table)])\n",
    "  \n",
    "  # Get feature names from a successful extraction\n",
    "  sample_features <- NULL\n",
    "  for (path in image_paths) {\n",
    "    tryCatch({\n",
    "      sample_features <- extract_features(path)\n",
    "      if (length(sample_features) == modal_length) break\n",
    "    }, error = function(e) {\n",
    "      next\n",
    "    })\n",
    "  }\n",
    "  \n",
    "  params <- list(\n",
    "    feature_length = modal_length,\n",
    "    feature_names = paste0(\"feature_\", seq_len(modal_length)),\n",
    "    length_distribution = length_table,\n",
    "    length_summary = length_summary,\n",
    "    sample_size = length(image_paths),\n",
    "    success_rate = sum(!is.na(feature_lengths)) / length(feature_lengths),\n",
    "    consistent_length = length(unique(feature_lengths[!is.na(feature_lengths)])) == 1\n",
    "  )\n",
    "  \n",
    "  # Print summary\n",
    "  cat(\"\\nFeature extraction analysis summary:\\n\")\n",
    "  cat(\"Modal feature length:\", params$feature_length, \"\\n\")\n",
    "  cat(\"Success rate:\", sprintf(\"%.2f%%\", params$success_rate * 100), \"\\n\")\n",
    "  cat(\"Length consistency:\", ifelse(params$consistent_length, \"Consistent\", \"Inconsistent\"), \"\\n\")\n",
    "  \n",
    "  if (!params$consistent_length) {\n",
    "    cat(\"\\nFeature length distribution:\\n\")\n",
    "    print(params$length_distribution)\n",
    "  }\n",
    "  \n",
    "  return(params)\n",
    "}\n",
    "\n",
    "# Modified feature extraction pipeline\n",
    "set.seed(123)  # for reproducibility\n",
    "\n",
    "# Step 1: Get feature parameters from training data\n",
    "# Option to sample if training set is very large\n",
    "sample_size <- if(nrow(train_df) > 1000) 1000 else NULL\n",
    "feature_params <- get_feature_params(train_df$image_filename, sample_size)\n",
    "\n",
    "# Enhanced feature extraction function with validation\n",
    "extract_and_validate_features <- function(image_paths, feature_params, dataset_name) {\n",
    "  features_list <- lapply(seq_along(image_paths), function(i) {\n",
    "    path <- image_paths[i]\n",
    "    \n",
    "    if (i %% 100 == 0) {\n",
    "      cat(sprintf(\"Processing %s image %d of %d\\n\", \n",
    "                 dataset_name, i, length(image_paths)))\n",
    "    }\n",
    "    \n",
    "    tryCatch({\n",
    "      features <- extract_features(path)\n",
    "      \n",
    "      # Validate feature length\n",
    "      if (length(features) != feature_params$feature_length) {\n",
    "        \n",
    "        # Handle inconsistent length\n",
    "        if (length(features) < feature_params$feature_length) {\n",
    "          # Pad with NA if shorter\n",
    "          features <- c(features, rep(NA, feature_params$feature_length - length(features)))\n",
    "        } else {\n",
    "          # Truncate if longer\n",
    "          features <- features[1:feature_params$feature_length]\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      return(features)\n",
    "      \n",
    "    }, error = function(e) {\n",
    "      warning(sprintf(\"Error processing %s image %s: %s\", \n",
    "                     dataset_name, path, e$message))\n",
    "      return(rep(NA, feature_params$feature_length))\n",
    "    })\n",
    "  })\n",
    "  \n",
    "  # Convert to matrix/data frame\n",
    "  features_matrix <- do.call(rbind, features_list)\n",
    "  colnames(features_matrix) <- feature_params$feature_names\n",
    "  \n",
    "  # Report statistics about NA values\n",
    "  na_counts <- colSums(is.na(features_matrix))\n",
    "  if (any(na_counts > 0)) {\n",
    "    cat(\"\\nNA counts in\", dataset_name, \"dataset:\\n\")\n",
    "    print(na_counts[na_counts > 0])\n",
    "  }\n",
    "  \n",
    "  return(features_matrix)\n",
    "}\n",
    "\n",
    "# Extract features for each dataset\n",
    "cat(\"\\nExtracting training features...\\n\")\n",
    "train_features <- extract_and_validate_features(\n",
    "  train_df$image_filename, \n",
    "  feature_params,\n",
    "  \"training\"\n",
    ")\n",
    "\n",
    "cat(\"\\nExtracting validation features...\\n\")\n",
    "val_features <- extract_and_validate_features(\n",
    "  val_df$image_filename, \n",
    "  feature_params,\n",
    "  \"validation\"\n",
    ")\n",
    "\n",
    "cat(\"\\nExtracting test features...\\n\")\n",
    "test_features <- extract_and_validate_features(\n",
    "  test_df$image_filename, \n",
    "  feature_params,\n",
    "  \"test\"\n",
    ")\n",
    "\n",
    "# Ensure that class labels are factors with consistent levels\n",
    "train_df$class <- factor(train_df$class, levels = c(\"Benign\", \"Early\", \"Pre\", \"Pro\"))\n",
    "test_df$class <- factor(test_df$class, levels = c(\"Benign\", \"Early\", \"Pre\", \"Pro\"))\n",
    "val_df$class <- factor(val_df$class, levels = c(\"Benign\", \"Early\", \"Pre\", \"Pro\"))\n",
    "\n",
    "train_data <- cbind(as.data.frame(train_features), class_numeric = train_df$class_numeric)\n",
    "validation_data <- cbind(as.data.frame(val_features), class_numeric = val_df$class_numeric)\n",
    "test_data <- cbind(as.data.frame(test_features), class_numeric = test_df$class_numeric)\n",
    "\n",
    "train_data[is.na(train_data)] <- 0\n",
    "validation_data[is.na(validation_data)] <- 0\n",
    "test_data[is.na(test_data)] <- 0\n",
    "\n",
    "# Convert class_numeric to factor with proper labels for training and testing\n",
    "train_data$class <- factor(train_data$class_numeric, \n",
    "                                levels = 0:3, \n",
    "                                labels = classes)\n",
    "validation_data$class <- factor(validation_data$class_numeric, \n",
    "                                     levels = 0:3, \n",
    "                                     labels = classes)\n",
    "test_data$class <- factor(test_data$class_numeric, \n",
    "                                levels = 0:3, \n",
    "                                labels = classes)\n",
    "\n",
    "library(caret)\n",
    "library(e1071)\n",
    "\n",
    "\n",
    "# Function to identify NZV features from training data\n",
    "identify_nzv_features <- function(train_data) {\n",
    "  nzv <- nearZeroVar(train_data, saveMetrics = TRUE)\n",
    "  nzv_features <- rownames(nzv)[nzv$zeroVar | nzv$nzv]\n",
    "  \n",
    "  # Log the features being removed and their properties\n",
    "  cat(\"Features removed due to near-zero variance:\\n\")\n",
    "  for(feature in nzv_features) {\n",
    "    cat(sprintf(\"Feature: %s, Variance: %.6f\\n\", \n",
    "                feature, \n",
    "                var(train_data[[feature]], na.rm = TRUE)))\n",
    "  }\n",
    "  \n",
    "  return(nzv_features)\n",
    "}\n",
    "\n",
    "# Function to check for differences in feature variance between datasets\n",
    "check_feature_variance <- function(train_data, val_data, test_data, nzv_features) {\n",
    "  warning_features <- list()\n",
    "  \n",
    "  for(feature in nzv_features) {\n",
    "    if(feature %in% names(val_data) && feature %in% names(test_data)) {\n",
    "      train_var <- var(train_data[[feature]], na.rm = TRUE)\n",
    "      val_var <- var(val_data[[feature]], na.rm = TRUE)\n",
    "      test_var <- var(test_data[[feature]], na.rm = TRUE)\n",
    "      \n",
    "      # Check if feature has significant variance in validation or test\n",
    "      if(val_var > 0.1 || test_var > 0.1) {  # Threshold can be adjusted\n",
    "        warning_features[[feature]] <- list(\n",
    "          train_variance = train_var,\n",
    "          val_variance = val_var,\n",
    "          test_variance = test_var\n",
    "        )\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(warning_features)\n",
    "}\n",
    "\n",
    "# Apply the fixed NZV filtering process\n",
    "# First, identify NZV features from training data only\n",
    "nzv_features <- identify_nzv_features(train_data[, -ncol(train_data)])\n",
    "\n",
    "# Check for potential issues in validation and test sets\n",
    "variance_warnings <- check_feature_variance(\n",
    "  train_data[, -ncol(train_data)],\n",
    "  validation_data[, -ncol(validation_data)],\n",
    "  test_data[, -ncol(test_data)],\n",
    "  nzv_features\n",
    ")\n",
    "\n",
    "# Print warnings if any features show different behavior in val/test\n",
    "if(length(variance_warnings) > 0) {\n",
    "  cat(\"\\nWARNING: Some features show different variance patterns in validation/test sets:\\n\")\n",
    "  print(variance_warnings)\n",
    "  cat(\"\\nConsider keeping these features in the model.\\n\")\n",
    "}\n",
    "\n",
    "# Remove NZV features and 'class_numeric' from all datasets\n",
    "train_data_filtered <- train_data[, !names(train_data) %in% c(nzv_features, \"class_numeric\", \"class\")]\n",
    "validation_data_filtered <- validation_data[, !names(validation_data) %in% c(nzv_features, \"class_numeric\", \"class\")]\n",
    "test_data_filtered <- test_data[, !names(test_data) %in% c(nzv_features, \"class_numeric\", \"class\")]\n",
    "\n",
    "# Ensure 'class_numeric' is preserved separately for labels\n",
    "train_labels <- train_data$class_numeric\n",
    "validation_labels <- validation_data$class_numeric\n",
    "test_labels <- test_data$class_numeric\n",
    "\n",
    "# Ensure the label vector is preserved separately\n",
    "train_labels <- train_data$class_numeric\n",
    "validation_labels <- validation_data$class_numeric\n",
    "test_labels <- test_data$class_numeric\n",
    "\n",
    "str(train_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d604037a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T15:02:00.076466Z",
     "iopub.status.busy": "2024-11-19T15:02:00.045151Z",
     "iopub.status.idle": "2024-11-19T15:02:00.235841Z",
     "shell.execute_reply": "2024-11-19T15:02:00.233991Z"
    },
    "papermill": {
     "duration": 0.215584,
     "end_time": "2024-11-19T15:02:00.238359",
     "exception": false,
     "start_time": "2024-11-19T15:02:00.022775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 62</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>feature_1</th><th scope=col>feature_2</th><th scope=col>feature_3</th><th scope=col>feature_4</th><th scope=col>feature_5</th><th scope=col>feature_6</th><th scope=col>feature_7</th><th scope=col>feature_8</th><th scope=col>feature_9</th><th scope=col>feature_10</th><th scope=col>⋯</th><th scope=col>feature_53</th><th scope=col>feature_54</th><th scope=col>feature_55</th><th scope=col>feature_56</th><th scope=col>feature_57</th><th scope=col>feature_58</th><th scope=col>feature_59</th><th scope=col>feature_60</th><th scope=col>feature_61</th><th scope=col>feature_62</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1.777778e-04</td><td>9.200000e-03</td><td>2.928889e-02</td><td>3.346667e-02</td><td>3.160000e-02</td><td>3.035556e-02</td><td>2.817778e-02</td><td>2.760000e-02</td><td>0.0230222222</td><td>0.022533333</td><td>⋯</td><td>  0.04149962</td><td>4363.2000000</td><td>137.00000000</td><td>5.96785310</td><td>0.00000000</td><td>0.00000000</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>  0.0000000</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1.333333e-04</td><td>8.888889e-05</td><td>1.333333e-04</td><td>8.888889e-05</td><td>1.777778e-04</td><td>2.355556e-03</td><td>4.666667e-03</td><td>1.200000e-02</td><td>0.0433333333</td><td>0.068711111</td><td>⋯</td><td>  0.84667133</td><td>   0.1346286</td><td>  0.99796863</td><td>0.21231420</td><td>0.02622837</td><td>0.02886068</td><td>2.233100e+04</td><td>6.070000e+02</td><td>7.616241e-01</td><td>  0.0000000</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.444444e-05</td><td>0.000000e+00</td><td>0.000000e+00</td><td>4.444444e-05</td><td>8.888889e-05</td><td>4.444444e-05</td><td>8.888889e-05</td><td>4.444444e-05</td><td>0.0009777778</td><td>0.014355556</td><td>⋯</td><td> -0.89997674</td><td>  -0.3108987</td><td>  0.91405783</td><td>0.09632424</td><td>1.00000000</td><td>0.19667728</td><td>2.920255e-02</td><td>3.177244e-02</td><td>2.249300e+04</td><td>596.0000000</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>2.666667e-04</td><td>5.333333e-04</td><td>1.511111e-03</td><td>4.711111e-03</td><td>1.337778e-02</td><td>6.035556e-02</td><td>1.806667e-01</td><td>2.037333e-01</td><td>0.1372000000</td><td>0.088400000</td><td>⋯</td><td>598.00000000</td><td>   0.7892548</td><td>  0.00000000</td><td>0.00000000</td><td>0.00000000</td><td>0.00000000</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>  0.0000000</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>4.444444e-05</td><td>8.888889e-05</td><td>4.444444e-05</td><td>3.111111e-04</td><td>5.333333e-03</td><td>3.426667e-02</td><td>1.457333e-01</td><td>1.473778e-01</td><td>0.0725333333</td><td>0.059733333</td><td>⋯</td><td>596.00000000</td><td>   0.7957989</td><td>  0.00000000</td><td>0.00000000</td><td>0.00000000</td><td>0.00000000</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>  0.0000000</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>4.444444e-05</td><td>0.000000e+00</td><td>0.000000e+00</td><td>4.444444e-05</td><td>8.888889e-05</td><td>4.444444e-05</td><td>8.888889e-05</td><td>0.000000e+00</td><td>0.0004000000</td><td>0.007688889</td><td>⋯</td><td> -0.01839539</td><td>   0.9149045</td><td>  0.09230061</td><td>1.00000000</td><td>0.19919756</td><td>0.02809956</td><td>2.992068e-02</td><td>2.249300e+04</td><td>5.960000e+02</td><td>  0.7957282</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 62\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & feature\\_1 & feature\\_2 & feature\\_3 & feature\\_4 & feature\\_5 & feature\\_6 & feature\\_7 & feature\\_8 & feature\\_9 & feature\\_10 & ⋯ & feature\\_53 & feature\\_54 & feature\\_55 & feature\\_56 & feature\\_57 & feature\\_58 & feature\\_59 & feature\\_60 & feature\\_61 & feature\\_62\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 1.777778e-04 & 9.200000e-03 & 2.928889e-02 & 3.346667e-02 & 3.160000e-02 & 3.035556e-02 & 2.817778e-02 & 2.760000e-02 & 0.0230222222 & 0.022533333 & ⋯ &   0.04149962 & 4363.2000000 & 137.00000000 & 5.96785310 & 0.00000000 & 0.00000000 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 &   0.0000000\\\\\n",
       "\t2 & 1.333333e-04 & 8.888889e-05 & 1.333333e-04 & 8.888889e-05 & 1.777778e-04 & 2.355556e-03 & 4.666667e-03 & 1.200000e-02 & 0.0433333333 & 0.068711111 & ⋯ &   0.84667133 &    0.1346286 &   0.99796863 & 0.21231420 & 0.02622837 & 0.02886068 & 2.233100e+04 & 6.070000e+02 & 7.616241e-01 &   0.0000000\\\\\n",
       "\t3 & 4.444444e-05 & 0.000000e+00 & 0.000000e+00 & 4.444444e-05 & 8.888889e-05 & 4.444444e-05 & 8.888889e-05 & 4.444444e-05 & 0.0009777778 & 0.014355556 & ⋯ &  -0.89997674 &   -0.3108987 &   0.91405783 & 0.09632424 & 1.00000000 & 0.19667728 & 2.920255e-02 & 3.177244e-02 & 2.249300e+04 & 596.0000000\\\\\n",
       "\t4 & 2.666667e-04 & 5.333333e-04 & 1.511111e-03 & 4.711111e-03 & 1.337778e-02 & 6.035556e-02 & 1.806667e-01 & 2.037333e-01 & 0.1372000000 & 0.088400000 & ⋯ & 598.00000000 &    0.7892548 &   0.00000000 & 0.00000000 & 0.00000000 & 0.00000000 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 &   0.0000000\\\\\n",
       "\t5 & 4.444444e-05 & 8.888889e-05 & 4.444444e-05 & 3.111111e-04 & 5.333333e-03 & 3.426667e-02 & 1.457333e-01 & 1.473778e-01 & 0.0725333333 & 0.059733333 & ⋯ & 596.00000000 &    0.7957989 &   0.00000000 & 0.00000000 & 0.00000000 & 0.00000000 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 &   0.0000000\\\\\n",
       "\t6 & 4.444444e-05 & 0.000000e+00 & 0.000000e+00 & 4.444444e-05 & 8.888889e-05 & 4.444444e-05 & 8.888889e-05 & 0.000000e+00 & 0.0004000000 & 0.007688889 & ⋯ &  -0.01839539 &    0.9149045 &   0.09230061 & 1.00000000 & 0.19919756 & 0.02809956 & 2.992068e-02 & 2.249300e+04 & 5.960000e+02 &   0.7957282\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 62\n",
       "\n",
       "| <!--/--> | feature_1 &lt;dbl&gt; | feature_2 &lt;dbl&gt; | feature_3 &lt;dbl&gt; | feature_4 &lt;dbl&gt; | feature_5 &lt;dbl&gt; | feature_6 &lt;dbl&gt; | feature_7 &lt;dbl&gt; | feature_8 &lt;dbl&gt; | feature_9 &lt;dbl&gt; | feature_10 &lt;dbl&gt; | ⋯ ⋯ | feature_53 &lt;dbl&gt; | feature_54 &lt;dbl&gt; | feature_55 &lt;dbl&gt; | feature_56 &lt;dbl&gt; | feature_57 &lt;dbl&gt; | feature_58 &lt;dbl&gt; | feature_59 &lt;dbl&gt; | feature_60 &lt;dbl&gt; | feature_61 &lt;dbl&gt; | feature_62 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1.777778e-04 | 9.200000e-03 | 2.928889e-02 | 3.346667e-02 | 3.160000e-02 | 3.035556e-02 | 2.817778e-02 | 2.760000e-02 | 0.0230222222 | 0.022533333 | ⋯ |   0.04149962 | 4363.2000000 | 137.00000000 | 5.96785310 | 0.00000000 | 0.00000000 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 |   0.0000000 |\n",
       "| 2 | 1.333333e-04 | 8.888889e-05 | 1.333333e-04 | 8.888889e-05 | 1.777778e-04 | 2.355556e-03 | 4.666667e-03 | 1.200000e-02 | 0.0433333333 | 0.068711111 | ⋯ |   0.84667133 |    0.1346286 |   0.99796863 | 0.21231420 | 0.02622837 | 0.02886068 | 2.233100e+04 | 6.070000e+02 | 7.616241e-01 |   0.0000000 |\n",
       "| 3 | 4.444444e-05 | 0.000000e+00 | 0.000000e+00 | 4.444444e-05 | 8.888889e-05 | 4.444444e-05 | 8.888889e-05 | 4.444444e-05 | 0.0009777778 | 0.014355556 | ⋯ |  -0.89997674 |   -0.3108987 |   0.91405783 | 0.09632424 | 1.00000000 | 0.19667728 | 2.920255e-02 | 3.177244e-02 | 2.249300e+04 | 596.0000000 |\n",
       "| 4 | 2.666667e-04 | 5.333333e-04 | 1.511111e-03 | 4.711111e-03 | 1.337778e-02 | 6.035556e-02 | 1.806667e-01 | 2.037333e-01 | 0.1372000000 | 0.088400000 | ⋯ | 598.00000000 |    0.7892548 |   0.00000000 | 0.00000000 | 0.00000000 | 0.00000000 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 |   0.0000000 |\n",
       "| 5 | 4.444444e-05 | 8.888889e-05 | 4.444444e-05 | 3.111111e-04 | 5.333333e-03 | 3.426667e-02 | 1.457333e-01 | 1.473778e-01 | 0.0725333333 | 0.059733333 | ⋯ | 596.00000000 |    0.7957989 |   0.00000000 | 0.00000000 | 0.00000000 | 0.00000000 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 |   0.0000000 |\n",
       "| 6 | 4.444444e-05 | 0.000000e+00 | 0.000000e+00 | 4.444444e-05 | 8.888889e-05 | 4.444444e-05 | 8.888889e-05 | 0.000000e+00 | 0.0004000000 | 0.007688889 | ⋯ |  -0.01839539 |    0.9149045 |   0.09230061 | 1.00000000 | 0.19919756 | 0.02809956 | 2.992068e-02 | 2.249300e+04 | 5.960000e+02 |   0.7957282 |\n",
       "\n"
      ],
      "text/plain": [
       "  feature_1    feature_2    feature_3    feature_4    feature_5    feature_6   \n",
       "1 1.777778e-04 9.200000e-03 2.928889e-02 3.346667e-02 3.160000e-02 3.035556e-02\n",
       "2 1.333333e-04 8.888889e-05 1.333333e-04 8.888889e-05 1.777778e-04 2.355556e-03\n",
       "3 4.444444e-05 0.000000e+00 0.000000e+00 4.444444e-05 8.888889e-05 4.444444e-05\n",
       "4 2.666667e-04 5.333333e-04 1.511111e-03 4.711111e-03 1.337778e-02 6.035556e-02\n",
       "5 4.444444e-05 8.888889e-05 4.444444e-05 3.111111e-04 5.333333e-03 3.426667e-02\n",
       "6 4.444444e-05 0.000000e+00 0.000000e+00 4.444444e-05 8.888889e-05 4.444444e-05\n",
       "  feature_7    feature_8    feature_9    feature_10  ⋯ feature_53  \n",
       "1 2.817778e-02 2.760000e-02 0.0230222222 0.022533333 ⋯   0.04149962\n",
       "2 4.666667e-03 1.200000e-02 0.0433333333 0.068711111 ⋯   0.84667133\n",
       "3 8.888889e-05 4.444444e-05 0.0009777778 0.014355556 ⋯  -0.89997674\n",
       "4 1.806667e-01 2.037333e-01 0.1372000000 0.088400000 ⋯ 598.00000000\n",
       "5 1.457333e-01 1.473778e-01 0.0725333333 0.059733333 ⋯ 596.00000000\n",
       "6 8.888889e-05 0.000000e+00 0.0004000000 0.007688889 ⋯  -0.01839539\n",
       "  feature_54   feature_55   feature_56 feature_57 feature_58 feature_59  \n",
       "1 4363.2000000 137.00000000 5.96785310 0.00000000 0.00000000 0.000000e+00\n",
       "2    0.1346286   0.99796863 0.21231420 0.02622837 0.02886068 2.233100e+04\n",
       "3   -0.3108987   0.91405783 0.09632424 1.00000000 0.19667728 2.920255e-02\n",
       "4    0.7892548   0.00000000 0.00000000 0.00000000 0.00000000 0.000000e+00\n",
       "5    0.7957989   0.00000000 0.00000000 0.00000000 0.00000000 0.000000e+00\n",
       "6    0.9149045   0.09230061 1.00000000 0.19919756 0.02809956 2.992068e-02\n",
       "  feature_60   feature_61   feature_62 \n",
       "1 0.000000e+00 0.000000e+00   0.0000000\n",
       "2 6.070000e+02 7.616241e-01   0.0000000\n",
       "3 3.177244e-02 2.249300e+04 596.0000000\n",
       "4 0.000000e+00 0.000000e+00   0.0000000\n",
       "5 0.000000e+00 0.000000e+00   0.0000000\n",
       "6 2.249300e+04 5.960000e+02   0.7957282"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 62</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>feature_1</th><th scope=col>feature_2</th><th scope=col>feature_3</th><th scope=col>feature_4</th><th scope=col>feature_5</th><th scope=col>feature_6</th><th scope=col>feature_7</th><th scope=col>feature_8</th><th scope=col>feature_9</th><th scope=col>feature_10</th><th scope=col>⋯</th><th scope=col>feature_53</th><th scope=col>feature_54</th><th scope=col>feature_55</th><th scope=col>feature_56</th><th scope=col>feature_57</th><th scope=col>feature_58</th><th scope=col>feature_59</th><th scope=col>feature_60</th><th scope=col>feature_61</th><th scope=col>feature_62</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0005333333</td><td>1.600000e-03</td><td>0.0016888889</td><td>0.007244444</td><td>0.014400000</td><td>0.04000000</td><td>0.059733333</td><td>0.081155556</td><td>⋯</td><td>-0.72063618</td><td> 0.09900467</td><td> 0.7573208</td><td>0.1344301</td><td>0.9996799</td><td>0.1961524</td><td>0.03135625</td><td>0.03081814</td><td>2.143600e+03</td><td>6.160000e+01</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0000000000</td><td>4.444444e-05</td><td>0.0012444444</td><td>0.002222222</td><td>0.003733333</td><td>0.01017778</td><td>0.009511111</td><td>0.006488889</td><td>⋯</td><td> 0.01607724</td><td>-2.90546663</td><td> 7.9453205</td><td>0.9296345</td><td>0.1267961</td><td>1.0000000</td><td>0.19855085</td><td>0.02378104</td><td>4.134360e-02</td><td>3.122714e+03</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0000000000</td><td>9.777778e-04</td><td>0.0111111111</td><td>0.014400000</td><td>0.014755556</td><td>0.01355556</td><td>0.010711111</td><td>0.021466667</td><td>⋯</td><td> 0.02576232</td><td>-1.45620894</td><td> 1.6885739</td><td>0.8469369</td><td>0.1605064</td><td>1.0000000</td><td>0.19865621</td><td>0.04047891</td><td>4.891586e-02</td><td>3.022286e+03</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0012444444</td><td>3.688889e-03</td><td>0.0087555556</td><td>0.009244444</td><td>0.007866667</td><td>0.00320000</td><td>0.003288889</td><td>0.007733333</td><td>⋯</td><td> 0.01968015</td><td>-2.28102416</td><td> 6.0087276</td><td>0.8921082</td><td>0.1402859</td><td>1.0000000</td><td>0.19813004</td><td>0.03597165</td><td>4.806516e-02</td><td>3.606667e+03</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0000000000</td><td>4.444444e-05</td><td>0.0005333333</td><td>0.002977778</td><td>0.008800000</td><td>0.01311111</td><td>0.011600000</td><td>0.012622222</td><td>⋯</td><td>-1.90820209</td><td> 3.40479587</td><td> 0.8887411</td><td>0.1384541</td><td>1.0000000</td><td>0.1993923</td><td>0.03608865</td><td>0.04203537</td><td>2.170000e+04</td><td>5.960000e+02</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>4.444444e-05</td><td>0.0003555556</td><td>0.0046222222</td><td>8.311111e-03</td><td>0.0068000000</td><td>0.008533333</td><td>0.013377778</td><td>0.01688889</td><td>0.028088889</td><td>0.044000000</td><td>⋯</td><td> 0.21066667</td><td> 0.02681166</td><td>-1.3054793</td><td>1.2982976</td><td>0.8128584</td><td>0.1637427</td><td>0.99979153</td><td>0.19686936</td><td>3.973655e-02</td><td>4.400242e-02</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 62\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & feature\\_1 & feature\\_2 & feature\\_3 & feature\\_4 & feature\\_5 & feature\\_6 & feature\\_7 & feature\\_8 & feature\\_9 & feature\\_10 & ⋯ & feature\\_53 & feature\\_54 & feature\\_55 & feature\\_56 & feature\\_57 & feature\\_58 & feature\\_59 & feature\\_60 & feature\\_61 & feature\\_62\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 4.444444e-05 & 0.0000000000 & 0.0005333333 & 1.600000e-03 & 0.0016888889 & 0.007244444 & 0.014400000 & 0.04000000 & 0.059733333 & 0.081155556 & ⋯ & -0.72063618 &  0.09900467 &  0.7573208 & 0.1344301 & 0.9996799 & 0.1961524 & 0.03135625 & 0.03081814 & 2.143600e+03 & 6.160000e+01\\\\\n",
       "\t2 & 4.444444e-05 & 0.0000000000 & 0.0000000000 & 4.444444e-05 & 0.0012444444 & 0.002222222 & 0.003733333 & 0.01017778 & 0.009511111 & 0.006488889 & ⋯ &  0.01607724 & -2.90546663 &  7.9453205 & 0.9296345 & 0.1267961 & 1.0000000 & 0.19855085 & 0.02378104 & 4.134360e-02 & 3.122714e+03\\\\\n",
       "\t3 & 4.444444e-05 & 0.0000000000 & 0.0000000000 & 9.777778e-04 & 0.0111111111 & 0.014400000 & 0.014755556 & 0.01355556 & 0.010711111 & 0.021466667 & ⋯ &  0.02576232 & -1.45620894 &  1.6885739 & 0.8469369 & 0.1605064 & 1.0000000 & 0.19865621 & 0.04047891 & 4.891586e-02 & 3.022286e+03\\\\\n",
       "\t4 & 4.444444e-05 & 0.0000000000 & 0.0012444444 & 3.688889e-03 & 0.0087555556 & 0.009244444 & 0.007866667 & 0.00320000 & 0.003288889 & 0.007733333 & ⋯ &  0.01968015 & -2.28102416 &  6.0087276 & 0.8921082 & 0.1402859 & 1.0000000 & 0.19813004 & 0.03597165 & 4.806516e-02 & 3.606667e+03\\\\\n",
       "\t5 & 4.444444e-05 & 0.0000000000 & 0.0000000000 & 4.444444e-05 & 0.0005333333 & 0.002977778 & 0.008800000 & 0.01311111 & 0.011600000 & 0.012622222 & ⋯ & -1.90820209 &  3.40479587 &  0.8887411 & 0.1384541 & 1.0000000 & 0.1993923 & 0.03608865 & 0.04203537 & 2.170000e+04 & 5.960000e+02\\\\\n",
       "\t6 & 4.444444e-05 & 0.0003555556 & 0.0046222222 & 8.311111e-03 & 0.0068000000 & 0.008533333 & 0.013377778 & 0.01688889 & 0.028088889 & 0.044000000 & ⋯ &  0.21066667 &  0.02681166 & -1.3054793 & 1.2982976 & 0.8128584 & 0.1637427 & 0.99979153 & 0.19686936 & 3.973655e-02 & 4.400242e-02\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 62\n",
       "\n",
       "| <!--/--> | feature_1 &lt;dbl&gt; | feature_2 &lt;dbl&gt; | feature_3 &lt;dbl&gt; | feature_4 &lt;dbl&gt; | feature_5 &lt;dbl&gt; | feature_6 &lt;dbl&gt; | feature_7 &lt;dbl&gt; | feature_8 &lt;dbl&gt; | feature_9 &lt;dbl&gt; | feature_10 &lt;dbl&gt; | ⋯ ⋯ | feature_53 &lt;dbl&gt; | feature_54 &lt;dbl&gt; | feature_55 &lt;dbl&gt; | feature_56 &lt;dbl&gt; | feature_57 &lt;dbl&gt; | feature_58 &lt;dbl&gt; | feature_59 &lt;dbl&gt; | feature_60 &lt;dbl&gt; | feature_61 &lt;dbl&gt; | feature_62 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 4.444444e-05 | 0.0000000000 | 0.0005333333 | 1.600000e-03 | 0.0016888889 | 0.007244444 | 0.014400000 | 0.04000000 | 0.059733333 | 0.081155556 | ⋯ | -0.72063618 |  0.09900467 |  0.7573208 | 0.1344301 | 0.9996799 | 0.1961524 | 0.03135625 | 0.03081814 | 2.143600e+03 | 6.160000e+01 |\n",
       "| 2 | 4.444444e-05 | 0.0000000000 | 0.0000000000 | 4.444444e-05 | 0.0012444444 | 0.002222222 | 0.003733333 | 0.01017778 | 0.009511111 | 0.006488889 | ⋯ |  0.01607724 | -2.90546663 |  7.9453205 | 0.9296345 | 0.1267961 | 1.0000000 | 0.19855085 | 0.02378104 | 4.134360e-02 | 3.122714e+03 |\n",
       "| 3 | 4.444444e-05 | 0.0000000000 | 0.0000000000 | 9.777778e-04 | 0.0111111111 | 0.014400000 | 0.014755556 | 0.01355556 | 0.010711111 | 0.021466667 | ⋯ |  0.02576232 | -1.45620894 |  1.6885739 | 0.8469369 | 0.1605064 | 1.0000000 | 0.19865621 | 0.04047891 | 4.891586e-02 | 3.022286e+03 |\n",
       "| 4 | 4.444444e-05 | 0.0000000000 | 0.0012444444 | 3.688889e-03 | 0.0087555556 | 0.009244444 | 0.007866667 | 0.00320000 | 0.003288889 | 0.007733333 | ⋯ |  0.01968015 | -2.28102416 |  6.0087276 | 0.8921082 | 0.1402859 | 1.0000000 | 0.19813004 | 0.03597165 | 4.806516e-02 | 3.606667e+03 |\n",
       "| 5 | 4.444444e-05 | 0.0000000000 | 0.0000000000 | 4.444444e-05 | 0.0005333333 | 0.002977778 | 0.008800000 | 0.01311111 | 0.011600000 | 0.012622222 | ⋯ | -1.90820209 |  3.40479587 |  0.8887411 | 0.1384541 | 1.0000000 | 0.1993923 | 0.03608865 | 0.04203537 | 2.170000e+04 | 5.960000e+02 |\n",
       "| 6 | 4.444444e-05 | 0.0003555556 | 0.0046222222 | 8.311111e-03 | 0.0068000000 | 0.008533333 | 0.013377778 | 0.01688889 | 0.028088889 | 0.044000000 | ⋯ |  0.21066667 |  0.02681166 | -1.3054793 | 1.2982976 | 0.8128584 | 0.1637427 | 0.99979153 | 0.19686936 | 3.973655e-02 | 4.400242e-02 |\n",
       "\n"
      ],
      "text/plain": [
       "  feature_1    feature_2    feature_3    feature_4    feature_5    feature_6  \n",
       "1 4.444444e-05 0.0000000000 0.0005333333 1.600000e-03 0.0016888889 0.007244444\n",
       "2 4.444444e-05 0.0000000000 0.0000000000 4.444444e-05 0.0012444444 0.002222222\n",
       "3 4.444444e-05 0.0000000000 0.0000000000 9.777778e-04 0.0111111111 0.014400000\n",
       "4 4.444444e-05 0.0000000000 0.0012444444 3.688889e-03 0.0087555556 0.009244444\n",
       "5 4.444444e-05 0.0000000000 0.0000000000 4.444444e-05 0.0005333333 0.002977778\n",
       "6 4.444444e-05 0.0003555556 0.0046222222 8.311111e-03 0.0068000000 0.008533333\n",
       "  feature_7   feature_8  feature_9   feature_10  ⋯ feature_53  feature_54 \n",
       "1 0.014400000 0.04000000 0.059733333 0.081155556 ⋯ -0.72063618  0.09900467\n",
       "2 0.003733333 0.01017778 0.009511111 0.006488889 ⋯  0.01607724 -2.90546663\n",
       "3 0.014755556 0.01355556 0.010711111 0.021466667 ⋯  0.02576232 -1.45620894\n",
       "4 0.007866667 0.00320000 0.003288889 0.007733333 ⋯  0.01968015 -2.28102416\n",
       "5 0.008800000 0.01311111 0.011600000 0.012622222 ⋯ -1.90820209  3.40479587\n",
       "6 0.013377778 0.01688889 0.028088889 0.044000000 ⋯  0.21066667  0.02681166\n",
       "  feature_55 feature_56 feature_57 feature_58 feature_59 feature_60\n",
       "1  0.7573208 0.1344301  0.9996799  0.1961524  0.03135625 0.03081814\n",
       "2  7.9453205 0.9296345  0.1267961  1.0000000  0.19855085 0.02378104\n",
       "3  1.6885739 0.8469369  0.1605064  1.0000000  0.19865621 0.04047891\n",
       "4  6.0087276 0.8921082  0.1402859  1.0000000  0.19813004 0.03597165\n",
       "5  0.8887411 0.1384541  1.0000000  0.1993923  0.03608865 0.04203537\n",
       "6 -1.3054793 1.2982976  0.8128584  0.1637427  0.99979153 0.19686936\n",
       "  feature_61   feature_62  \n",
       "1 2.143600e+03 6.160000e+01\n",
       "2 4.134360e-02 3.122714e+03\n",
       "3 4.891586e-02 3.022286e+03\n",
       "4 4.806516e-02 3.606667e+03\n",
       "5 2.170000e+04 5.960000e+02\n",
       "6 3.973655e-02 4.400242e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 62</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>feature_1</th><th scope=col>feature_2</th><th scope=col>feature_3</th><th scope=col>feature_4</th><th scope=col>feature_5</th><th scope=col>feature_6</th><th scope=col>feature_7</th><th scope=col>feature_8</th><th scope=col>feature_9</th><th scope=col>feature_10</th><th scope=col>⋯</th><th scope=col>feature_53</th><th scope=col>feature_54</th><th scope=col>feature_55</th><th scope=col>feature_56</th><th scope=col>feature_57</th><th scope=col>feature_58</th><th scope=col>feature_59</th><th scope=col>feature_60</th><th scope=col>feature_61</th><th scope=col>feature_62</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0000000000</td><td>8.888889e-05</td><td>0.0005777778</td><td>0.001911111</td><td>0.005866667</td><td>0.013066667</td><td>0.018222222</td><td>0.015111111</td><td>⋯</td><td>-2.77062137</td><td> 7.31900272</td><td> 0.92711700</td><td> 0.1278960</td><td>1.0000000</td><td>0.19725757</td><td>0.02467900</td><td>3.804122e-02</td><td>2.711750e+03</td><td>7.725000e+01</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.444444e-05</td><td>0.0010666667</td><td>0.0024888889</td><td>7.733333e-03</td><td>0.0236000000</td><td>0.014488889</td><td>0.008622222</td><td>0.006888889</td><td>0.011777778</td><td>0.020622222</td><td>⋯</td><td> 0.38422222</td><td> 0.02986206</td><td>-1.67308901</td><td> 2.3843805</td><td>0.8399360</td><td>0.17280643</td><td>1.00000000</td><td>1.957527e-01</td><td>4.623507e-02</td><td>5.489312e-02</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0001333333</td><td>3.111111e-03</td><td>0.0092444444</td><td>0.009111111</td><td>0.013955556</td><td>0.014533333</td><td>0.008311111</td><td>0.011866667</td><td>⋯</td><td> 0.01662846</td><td>-1.86762989</td><td> 4.22952231</td><td> 0.8048085</td><td>0.1289514</td><td>1.00000000</td><td>0.19710840</td><td>3.215657e-02</td><td>3.910905e-02</td><td>3.044000e+03</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0000000000</td><td>6.666667e-04</td><td>0.0014666667</td><td>0.002755556</td><td>0.004000000</td><td>0.009511111</td><td>0.009733333</td><td>0.004044444</td><td>⋯</td><td> 8.95771716</td><td> 0.91496135</td><td> 0.11159925</td><td> 1.0000000</td><td>0.1983277</td><td>0.02994379</td><td>0.03680758</td><td>2.436778e+03</td><td>7.255556e+01</td><td>5.971678e+00</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>4.444444e-05</td><td>0.0001777778</td><td>0.0003555556</td><td>6.666667e-04</td><td>0.0016888889</td><td>0.005866667</td><td>0.005911111</td><td>0.007377778</td><td>0.010222222</td><td>0.014888889</td><td>⋯</td><td> 0.01725133</td><td>-1.83555546</td><td> 3.86902200</td><td> 0.8827094</td><td>0.1313443</td><td>1.00000000</td><td>0.19750980</td><td>3.397769e-02</td><td>3.890111e-02</td><td>4.379400e+03</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>4.444444e-05</td><td>0.0000000000</td><td>0.0000000000</td><td>5.600000e-03</td><td>0.0108000000</td><td>0.005733333</td><td>0.005555556</td><td>0.004355556</td><td>0.007333333</td><td>0.007111111</td><td>⋯</td><td> 0.28684444</td><td> 0.55186667</td><td> 0.02022319</td><td>-2.2201853</td><td>5.6228662</td><td>0.88571348</td><td>0.14220826</td><td>1.000000e+00</td><td>1.968674e-01</td><td>3.700832e-02</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 62\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & feature\\_1 & feature\\_2 & feature\\_3 & feature\\_4 & feature\\_5 & feature\\_6 & feature\\_7 & feature\\_8 & feature\\_9 & feature\\_10 & ⋯ & feature\\_53 & feature\\_54 & feature\\_55 & feature\\_56 & feature\\_57 & feature\\_58 & feature\\_59 & feature\\_60 & feature\\_61 & feature\\_62\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 4.444444e-05 & 0.0000000000 & 0.0000000000 & 8.888889e-05 & 0.0005777778 & 0.001911111 & 0.005866667 & 0.013066667 & 0.018222222 & 0.015111111 & ⋯ & -2.77062137 &  7.31900272 &  0.92711700 &  0.1278960 & 1.0000000 & 0.19725757 & 0.02467900 & 3.804122e-02 & 2.711750e+03 & 7.725000e+01\\\\\n",
       "\t2 & 4.444444e-05 & 0.0010666667 & 0.0024888889 & 7.733333e-03 & 0.0236000000 & 0.014488889 & 0.008622222 & 0.006888889 & 0.011777778 & 0.020622222 & ⋯ &  0.38422222 &  0.02986206 & -1.67308901 &  2.3843805 & 0.8399360 & 0.17280643 & 1.00000000 & 1.957527e-01 & 4.623507e-02 & 5.489312e-02\\\\\n",
       "\t3 & 4.444444e-05 & 0.0000000000 & 0.0001333333 & 3.111111e-03 & 0.0092444444 & 0.009111111 & 0.013955556 & 0.014533333 & 0.008311111 & 0.011866667 & ⋯ &  0.01662846 & -1.86762989 &  4.22952231 &  0.8048085 & 0.1289514 & 1.00000000 & 0.19710840 & 3.215657e-02 & 3.910905e-02 & 3.044000e+03\\\\\n",
       "\t4 & 4.444444e-05 & 0.0000000000 & 0.0000000000 & 6.666667e-04 & 0.0014666667 & 0.002755556 & 0.004000000 & 0.009511111 & 0.009733333 & 0.004044444 & ⋯ &  8.95771716 &  0.91496135 &  0.11159925 &  1.0000000 & 0.1983277 & 0.02994379 & 0.03680758 & 2.436778e+03 & 7.255556e+01 & 5.971678e+00\\\\\n",
       "\t5 & 4.444444e-05 & 0.0001777778 & 0.0003555556 & 6.666667e-04 & 0.0016888889 & 0.005866667 & 0.005911111 & 0.007377778 & 0.010222222 & 0.014888889 & ⋯ &  0.01725133 & -1.83555546 &  3.86902200 &  0.8827094 & 0.1313443 & 1.00000000 & 0.19750980 & 3.397769e-02 & 3.890111e-02 & 4.379400e+03\\\\\n",
       "\t6 & 4.444444e-05 & 0.0000000000 & 0.0000000000 & 5.600000e-03 & 0.0108000000 & 0.005733333 & 0.005555556 & 0.004355556 & 0.007333333 & 0.007111111 & ⋯ &  0.28684444 &  0.55186667 &  0.02022319 & -2.2201853 & 5.6228662 & 0.88571348 & 0.14220826 & 1.000000e+00 & 1.968674e-01 & 3.700832e-02\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 62\n",
       "\n",
       "| <!--/--> | feature_1 &lt;dbl&gt; | feature_2 &lt;dbl&gt; | feature_3 &lt;dbl&gt; | feature_4 &lt;dbl&gt; | feature_5 &lt;dbl&gt; | feature_6 &lt;dbl&gt; | feature_7 &lt;dbl&gt; | feature_8 &lt;dbl&gt; | feature_9 &lt;dbl&gt; | feature_10 &lt;dbl&gt; | ⋯ ⋯ | feature_53 &lt;dbl&gt; | feature_54 &lt;dbl&gt; | feature_55 &lt;dbl&gt; | feature_56 &lt;dbl&gt; | feature_57 &lt;dbl&gt; | feature_58 &lt;dbl&gt; | feature_59 &lt;dbl&gt; | feature_60 &lt;dbl&gt; | feature_61 &lt;dbl&gt; | feature_62 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 4.444444e-05 | 0.0000000000 | 0.0000000000 | 8.888889e-05 | 0.0005777778 | 0.001911111 | 0.005866667 | 0.013066667 | 0.018222222 | 0.015111111 | ⋯ | -2.77062137 |  7.31900272 |  0.92711700 |  0.1278960 | 1.0000000 | 0.19725757 | 0.02467900 | 3.804122e-02 | 2.711750e+03 | 7.725000e+01 |\n",
       "| 2 | 4.444444e-05 | 0.0010666667 | 0.0024888889 | 7.733333e-03 | 0.0236000000 | 0.014488889 | 0.008622222 | 0.006888889 | 0.011777778 | 0.020622222 | ⋯ |  0.38422222 |  0.02986206 | -1.67308901 |  2.3843805 | 0.8399360 | 0.17280643 | 1.00000000 | 1.957527e-01 | 4.623507e-02 | 5.489312e-02 |\n",
       "| 3 | 4.444444e-05 | 0.0000000000 | 0.0001333333 | 3.111111e-03 | 0.0092444444 | 0.009111111 | 0.013955556 | 0.014533333 | 0.008311111 | 0.011866667 | ⋯ |  0.01662846 | -1.86762989 |  4.22952231 |  0.8048085 | 0.1289514 | 1.00000000 | 0.19710840 | 3.215657e-02 | 3.910905e-02 | 3.044000e+03 |\n",
       "| 4 | 4.444444e-05 | 0.0000000000 | 0.0000000000 | 6.666667e-04 | 0.0014666667 | 0.002755556 | 0.004000000 | 0.009511111 | 0.009733333 | 0.004044444 | ⋯ |  8.95771716 |  0.91496135 |  0.11159925 |  1.0000000 | 0.1983277 | 0.02994379 | 0.03680758 | 2.436778e+03 | 7.255556e+01 | 5.971678e+00 |\n",
       "| 5 | 4.444444e-05 | 0.0001777778 | 0.0003555556 | 6.666667e-04 | 0.0016888889 | 0.005866667 | 0.005911111 | 0.007377778 | 0.010222222 | 0.014888889 | ⋯ |  0.01725133 | -1.83555546 |  3.86902200 |  0.8827094 | 0.1313443 | 1.00000000 | 0.19750980 | 3.397769e-02 | 3.890111e-02 | 4.379400e+03 |\n",
       "| 6 | 4.444444e-05 | 0.0000000000 | 0.0000000000 | 5.600000e-03 | 0.0108000000 | 0.005733333 | 0.005555556 | 0.004355556 | 0.007333333 | 0.007111111 | ⋯ |  0.28684444 |  0.55186667 |  0.02022319 | -2.2201853 | 5.6228662 | 0.88571348 | 0.14220826 | 1.000000e+00 | 1.968674e-01 | 3.700832e-02 |\n",
       "\n"
      ],
      "text/plain": [
       "  feature_1    feature_2    feature_3    feature_4    feature_5    feature_6  \n",
       "1 4.444444e-05 0.0000000000 0.0000000000 8.888889e-05 0.0005777778 0.001911111\n",
       "2 4.444444e-05 0.0010666667 0.0024888889 7.733333e-03 0.0236000000 0.014488889\n",
       "3 4.444444e-05 0.0000000000 0.0001333333 3.111111e-03 0.0092444444 0.009111111\n",
       "4 4.444444e-05 0.0000000000 0.0000000000 6.666667e-04 0.0014666667 0.002755556\n",
       "5 4.444444e-05 0.0001777778 0.0003555556 6.666667e-04 0.0016888889 0.005866667\n",
       "6 4.444444e-05 0.0000000000 0.0000000000 5.600000e-03 0.0108000000 0.005733333\n",
       "  feature_7   feature_8   feature_9   feature_10  ⋯ feature_53  feature_54 \n",
       "1 0.005866667 0.013066667 0.018222222 0.015111111 ⋯ -2.77062137  7.31900272\n",
       "2 0.008622222 0.006888889 0.011777778 0.020622222 ⋯  0.38422222  0.02986206\n",
       "3 0.013955556 0.014533333 0.008311111 0.011866667 ⋯  0.01662846 -1.86762989\n",
       "4 0.004000000 0.009511111 0.009733333 0.004044444 ⋯  8.95771716  0.91496135\n",
       "5 0.005911111 0.007377778 0.010222222 0.014888889 ⋯  0.01725133 -1.83555546\n",
       "6 0.005555556 0.004355556 0.007333333 0.007111111 ⋯  0.28684444  0.55186667\n",
       "  feature_55  feature_56 feature_57 feature_58 feature_59 feature_60  \n",
       "1  0.92711700  0.1278960 1.0000000  0.19725757 0.02467900 3.804122e-02\n",
       "2 -1.67308901  2.3843805 0.8399360  0.17280643 1.00000000 1.957527e-01\n",
       "3  4.22952231  0.8048085 0.1289514  1.00000000 0.19710840 3.215657e-02\n",
       "4  0.11159925  1.0000000 0.1983277  0.02994379 0.03680758 2.436778e+03\n",
       "5  3.86902200  0.8827094 0.1313443  1.00000000 0.19750980 3.397769e-02\n",
       "6  0.02022319 -2.2201853 5.6228662  0.88571348 0.14220826 1.000000e+00\n",
       "  feature_61   feature_62  \n",
       "1 2.711750e+03 7.725000e+01\n",
       "2 4.623507e-02 5.489312e-02\n",
       "3 3.910905e-02 3.044000e+03\n",
       "4 7.255556e+01 5.971678e+00\n",
       "5 3.890111e-02 4.379400e+03\n",
       "6 1.968674e-01 3.700832e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(train_data_filtered)\n",
    "head(validation_data_filtered)\n",
    "head(test_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b1eedd",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "execution": {
     "iopub.execute_input": "2024-11-19T15:02:00.254711Z",
     "iopub.status.busy": "2024-11-19T15:02:00.252872Z",
     "iopub.status.idle": "2024-11-19T15:02:00.340879Z",
     "shell.execute_reply": "2024-11-19T15:02:00.338985Z"
    },
    "papermill": {
     "duration": 0.098654,
     "end_time": "2024-11-19T15:02:00.343178",
     "exception": false,
     "start_time": "2024-11-19T15:02:00.244524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num [1:4] 1.615 0.826 0.845 1.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2 1 3 0 1 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1.6145511 0.8264659 0.8452188 1.0126214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Train Weights (first few):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.8452188 0.8264659 1.0126214 1.6145511 0.8264659 1.0126214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NA values in train_weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in train_weights:  0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels and Corresponding Train Weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_labels train_weights\n",
      "1             2     0.8452188\n",
      "2             1     0.8264659\n",
      "3             3     1.0126214\n",
      "4             0     1.6145511\n",
      "5             1     0.8264659\n",
      "6             3     1.0126214\n",
      "7             1     0.8264659\n",
      "8             2     0.8452188\n",
      "9             2     0.8452188\n",
      "10            0     1.6145511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights <- as.numeric(class_weights)\n",
    "str(class_weights)\n",
    "\n",
    "train_weights <- class_weights[train_df$class_numeric + 1]\n",
    "\n",
    "# Check the train_labels values\n",
    "cat(\"Train Labels:\\n\")\n",
    "print(head(train_labels))  # Display the first few train labels\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Check the class_weights values\n",
    "cat(\"Class Weights:\\n\")\n",
    "print(class_weights)  # Display the class weights\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Ensure that the class_weights vector is correctly aligned with the class labels\n",
    "# If we have 4 classes, class_weights should correspond to the following:\n",
    "# 0 -> 1.615 (Benign), 1 -> 0.826 (Early), 2 -> 0.845 (Pre), 3 -> 1.013 (Pro)\n",
    "\n",
    "# Assign the weights based on train_labels\n",
    "train_weights <- class_weights[train_labels + 1]  # Adjusting for 1-based indexing in R\n",
    "\n",
    "# Check the first few train_weights to confirm they match the expected values\n",
    "cat(\"Assigned Train Weights (first few):\\n\")\n",
    "print(head(train_weights))  # Display the first few train weights\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Check for NA values in the train_weights\n",
    "cat(\"Checking for NA values in train_weights:\\n\")\n",
    "na_count <- sum(is.na(train_weights))  # Should return 0 if no NA values\n",
    "cat(\"Number of NA values in train_weights: \", na_count, \"\\n\")\n",
    "cat(\"\\n\")\n",
    "\n",
    "# If there are NA values, investigate further\n",
    "if (na_count > 0) {\n",
    "  cat(\"Investigate the indices that caused NAs:\\n\")\n",
    "  na_indices <- which(is.na(train_weights))\n",
    "  print(train_labels[na_indices])  # Investigate which train_labels caused the NA values\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Finally, print the first few rows of train_labels and train_weights to confirm alignment\n",
    "cat(\"Train Labels and Corresponding Train Weights:\\n\")\n",
    "df_check <- data.frame(train_labels = train_labels[1:10], train_weights = train_weights[1:10])  # First 10 rows for quick inspection\n",
    "print(df_check)\n",
    "cat(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f07c241",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "execution": {
     "iopub.execute_input": "2024-11-19T15:02:00.359984Z",
     "iopub.status.busy": "2024-11-19T15:02:00.358424Z",
     "iopub.status.idle": "2024-11-19T15:02:01.904336Z",
     "shell.execute_reply": "2024-11-19T15:02:01.902455Z"
    },
    "papermill": {
     "duration": 1.556986,
     "end_time": "2024-11-19T15:02:01.906675",
     "exception": false,
     "start_time": "2024-11-19T15:02:00.349689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-merror:0.052583\tval-merror:0.118098 \n",
      "Multiple eval metrics are present. Will use val_merror for early stopping.\n",
      "Will train until val_merror hasn't improved in 20 rounds.\n",
      "\n",
      "[2]\ttrain-merror:0.034334\tval-merror:0.088957 \n",
      "[3]\ttrain-merror:0.026926\tval-merror:0.082822 \n",
      "[4]\ttrain-merror:0.029644\tval-merror:0.087423 \n",
      "[5]\ttrain-merror:0.023739\tval-merror:0.076687 \n",
      "[6]\ttrain-merror:0.021417\tval-merror:0.076687 \n",
      "[7]\ttrain-merror:0.021048\tval-merror:0.069018 \n",
      "[8]\ttrain-merror:0.017879\tval-merror:0.072086 \n",
      "[9]\ttrain-merror:0.015512\tval-merror:0.064417 \n",
      "[10]\ttrain-merror:0.014701\tval-merror:0.069018 \n",
      "[11]\ttrain-merror:0.013900\tval-merror:0.067485 \n",
      "[12]\ttrain-merror:0.011091\tval-merror:0.064417 \n",
      "[13]\ttrain-merror:0.011064\tval-merror:0.067485 \n",
      "[14]\ttrain-merror:0.011064\tval-merror:0.065951 \n",
      "[15]\ttrain-merror:0.009884\tval-merror:0.062883 \n",
      "[16]\ttrain-merror:0.010253\tval-merror:0.061350 \n",
      "[17]\ttrain-merror:0.009884\tval-merror:0.061350 \n",
      "[18]\ttrain-merror:0.008714\tval-merror:0.061350 \n",
      "[19]\ttrain-merror:0.007535\tval-merror:0.062883 \n",
      "[20]\ttrain-merror:0.006725\tval-merror:0.059816 \n",
      "[21]\ttrain-merror:0.006725\tval-merror:0.059816 \n",
      "[22]\ttrain-merror:0.005545\tval-merror:0.053681 \n",
      "[23]\ttrain-merror:0.005545\tval-merror:0.056748 \n",
      "[24]\ttrain-merror:0.005951\tval-merror:0.056748 \n",
      "[25]\ttrain-merror:0.004366\tval-merror:0.055215 \n",
      "[26]\ttrain-merror:0.003961\tval-merror:0.059816 \n",
      "[27]\ttrain-merror:0.003952\tval-merror:0.058282 \n",
      "[28]\ttrain-merror:0.003151\tval-merror:0.055215 \n",
      "[29]\ttrain-merror:0.002755\tval-merror:0.053681 \n",
      "[30]\ttrain-merror:0.002755\tval-merror:0.056748 \n",
      "[31]\ttrain-merror:0.002755\tval-merror:0.055215 \n",
      "[32]\ttrain-merror:0.002755\tval-merror:0.055215 \n",
      "[33]\ttrain-merror:0.002386\tval-merror:0.055215 \n",
      "[34]\ttrain-merror:0.001207\tval-merror:0.058282 \n",
      "[35]\ttrain-merror:0.001207\tval-merror:0.059816 \n",
      "[36]\ttrain-merror:0.000801\tval-merror:0.055215 \n",
      "[37]\ttrain-merror:0.000801\tval-merror:0.059816 \n",
      "[38]\ttrain-merror:0.000000\tval-merror:0.061350 \n",
      "[39]\ttrain-merror:0.000000\tval-merror:0.058282 \n",
      "[40]\ttrain-merror:0.000000\tval-merror:0.056748 \n",
      "[41]\ttrain-merror:0.000000\tval-merror:0.055215 \n",
      "[42]\ttrain-merror:0.000000\tval-merror:0.055215 \n",
      "Stopping. Best iteration:\n",
      "[22]\ttrain-merror:0.005545\tval-merror:0.053681\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction Benign Early Pre Pro\n",
      "    Benign     66     2   0   0\n",
      "    Early      10   153   6   0\n",
      "    Pre         1     2 144   0\n",
      "    Pro         3     0   3 128\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.9479          \n",
      "                 95% CI : (0.9251, 0.9654)\n",
      "    No Information Rate : 0.3031          \n",
      "    P-Value [Acc > NIR] : < 2.2e-16       \n",
      "                                          \n",
      "                  Kappa : 0.9289          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: Benign Class: Early Class: Pre Class: Pro\n",
      "Sensitivity                 0.8250       0.9745     0.9412     1.0000\n",
      "Specificity                 0.9954       0.9557     0.9918     0.9846\n",
      "Pos Pred Value              0.9706       0.9053     0.9796     0.9552\n",
      "Neg Pred Value              0.9689       0.9885     0.9757     1.0000\n",
      "Prevalence                  0.1544       0.3031     0.2954     0.2471\n",
      "Detection Rate              0.1274       0.2954     0.2780     0.2471\n",
      "Detection Prevalence        0.1313       0.3263     0.2838     0.2587\n",
      "Balanced Accuracy           0.9102       0.9651     0.9665     0.9923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-Specific Statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sensitivity Specificity Pos Pred Value Neg Pred Value Precision\n",
      "Class: Benign   0.8250000   0.9954338      0.9705882      0.9688889 0.9705882\n",
      "Class: Early    0.9745223   0.9556787      0.9053254      0.9885387 0.9053254\n",
      "Class: Pre      0.9411765   0.9917808      0.9795918      0.9757412 0.9795918\n",
      "Class: Pro      1.0000000   0.9846154      0.9552239      1.0000000 0.9552239\n",
      "                 Recall        F1 Prevalence Detection Rate\n",
      "Class: Benign 0.8250000 0.8918919  0.1544402      0.1274131\n",
      "Class: Early  0.9745223 0.9386503  0.3030888      0.2953668\n",
      "Class: Pre    0.9411765 0.9600000  0.2953668      0.2779923\n",
      "Class: Pro    1.0000000 0.9770992  0.2471042      0.2471042\n",
      "              Detection Prevalence Balanced Accuracy\n",
      "Class: Benign            0.1312741         0.9102169\n",
      "Class: Early             0.3262548         0.9651005\n",
      "Class: Pre               0.2837838         0.9664786\n",
      "Class: Pro               0.2586873         0.9923077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Mode:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"sens_spec\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Arguments Passed to confusionMatrix():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9478764 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train Accuracy: 0.9947267 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAABmJLR0QA/wD/AP+gvaeTAAAg\nAElEQVR4nOzdeVRUd57//08VRYE4FVlKthAQBTM6USnXdksiQ3DaxNiCGLVDHDvmJPH00Box\njCRGY7e2RA+cOCeJnYQxJv3TnFa+aR1G3MYlNiEiQmt0XEJAROwIxWZpFbVQ9/cHpjsjhZRp\nkO7PfT7+CXX93Pd9f4BzeOVzN42iKAIAAAB//7R93QAAAAB6BsEOAABAEgQ7AAAASRDsAAAA\nJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4AAEASBDsAAABJEOwAAAAkQbAD\nAACQBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAE\nwQ4AAEASBDsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAA\nQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4AAEASBDsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7\nAAAASRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4AAEASBDsAAABJ\nEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAA\nACRBsAMAAJAEwQ4AAEASBDsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGw\nAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4AAEASBDsAAABJEOwAAAAkQbADAACQ\nBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4A\nAEASBDsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAAQBIE\nOwAAAEkQ7AAAACRBsAMAAJAEwQ4AAEASBDsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAA\nSRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4AAEASBDsAAABJEOwA\nAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRB\nsAMAAJAEwQ4AAEASBDsAAABJEOwAAAAkoevrBoBedOTIkb5uAQAgp2nTpvV1Cx6wYgcAACAJ\ngh0AAIAkCHYAAACSINgBAABIgmAHAAAgCYIdAACAJAh2AAAAkiDYqdGHz00Oi46vc7i9HP/0\nrNSebeBmzd65SePDQoKHT5l/vNnes8UBAFAtgp0aZX964nTlxQf13v70CwuLuh3jtlu9rKa0\nW1LHpv3jz/P/1Nj0bqo5bfpWL3cEAAB3R7BTnZzkyU0ud/KEMdcc7rpjW6aMijMGG0fPWHza\n4ugYsHvtorhwo9FofHjS7KONbSsSJwrFbjKZGq+sC4xd3zGmtTozOP49IYT1en7QkNwdGTMH\nxS8XQngseIeWyuwS/5fW/mSEVojHfrG7tOAn92XeAADIj2CnOlkHih/w0RwvKzfaT5hmrMnY\nVmxuvJad8NX0JzYLIezN+9JySnadvWo217819tLL2eUbD5cIjV9FRYVe47mgpXbD3uHLL1S9\n67CUdC7YWf3nFQGRg19JTYqPHpKYkmU2BPfeZAEAUBWCnXpd3feafvz7cxPChEY/+40dDWVv\nWt2KfsDUuppTCUZ/W/N1m6/WeqX7E6xa34HbXno8QKfxWLDz+LaGtvpTKx5anPvHC6UvDjv5\nL0m/7oXJAQCgRrq+bgB9prm8qen8qybT6o6PI0fENTjd0br2D1YuyN9/1hAZGx/e2uXOyl8S\nm94wXtt1wRg/nzt21Qfpg+Lylv14pBAibfWOBTlx1xyrIr2+4A8AAHSFYKdehqGGiKm5FTsT\nhRCK+9aXpV/H+PlUF6TllY47X7XHqNPWFCY9+s4dO92+kdbedFmIuI6vNRrdXQp2Pu7ASfHi\n7e9yoUYrhFbXxUleAABwT1gmUa/oWVkNRS/uOlcvhDicm5KyrEQI4bxh1QdFh+i0LmtV7poz\n7W1OIYRQXDa34qOPspkLqm0ut6tp4+LPvSzYWdCw9SFXsz744qpQnHt+NT9k1KpQX34PAQDo\nAfxBVS//4KfKtr+wYc7Y0IiIzINRhQeeF0IMmb81UcmLih0+MWnpqNUZ9pMLMitbFiaEDh46\nTBeavj41cEJMTERg5OUn07ws2JmP/sGju9/ctuRxY2h0Tvnog8f+vXfnCQCAamgUxcPl7UDX\n3C2tzsABfn3dhleOHDnS1y0AAOQ0bdq0vm7BA66xw73SepnqbOaCV1YdumOjb/8Rmzct6YWu\nAAAAK3aQGit2AIBe8re5Ysc1dgAAAJIg2AEAAEiCYAcAACAJgh0AAIAkCHYAAACS4K5YAAAA\nSbBiBwAAIAmCHQAAgCQIdgAAAJLglWKQGW+egEd/m8+LB4C/Hit2AAAAkiDYAQAASIJgBwAA\nIAmCHQAAgCQIdgAAAJIg2AEAAEiCYAcAACAJgp3kPnxuclh0fJ3D7eX4p2el9mwDZz953RQb\nGhgU8siUeXuv3OzY+FrMAM13wkyFPXtEAABUS6MoSl/3gF4UqteduekI13ub4LU+Ae52693H\nuO1WrV+AN9UcrceCwubmf1E2zxSxb9PMtLcjLFf/Uwgx7gG/31TVP9xPJ4TQaP0D+vl42d69\n4gHF8IgHFAOQFSt2MstJntzkcidPGHPN4RZC1B3bMmVUnDHYOHrG4tMWhxBi99pFceFGo9H4\n8KTZRxvbViROFIrdZDI5FGGpXRcYu76jTmt1ZnD8e9br+UFDcndkzBwUv9xjtc5aKvP8B62b\nN/ohodEl/2LrzbqtzS7F7Wo84wwbbRzQv3///v37916qAwBAbQh2Mss6UPyAj+Z4WXmkXuuw\nlJhmrMnYVmxuvJad8NX0Jzbbm/el5ZTsOnvVbK5/a+yll7PLNx4uERq/iooKvcZzQUvthr3D\nl1+oerdzNY/jQx7JP/eH9I6vWy5+7DfgsSCdxt58SNH4Tx8ZG/zAwDFJz3/R0NZL0wcAQG0I\ndmpxdd9r+vHvz00IExr97Dd2NJS96TJMqas5lWD0tzVft/lqrVe6OQMrhND6Dtz20uMBOk3n\nala3h3P6Pn4h4UY/IcTpz3KnTlm37LefCCHandcfe2zyr/Z91dh8eWnC6ScnrezxyQIAoE66\nvm4A90lzeVPT+VdNptUdH0eOiGtwOLevTM/ff9YQGRsf3nq3nb+7EFNvGK/tqprTHePn4aSq\ny/ZN9rOp+RcjN++/+NMfhQsh/iEy42DR7X+d/8sdC3MfrrVvesjTvgAA4J4Q7NTCMNQQMTW3\nYmeiEEJx3/qy9GulaF5e6bjzVXuMOm1NYdKj73Te6fa9tPamy0LECSE0Gl1X1TymOqXdkj5q\n7I20zXU70/2/Wx2u/6Kg2Oex2ROMQgiN1l+j8e3n08WpXwAAcC84FasW0bOyGope3HWuXghx\nODclZVmJ84ZVHxQdotO6rFW5a860tzmFEEJx2dyKEMJHH2UzF1TbXG5X08bFn3dbzeNBvy1e\nsl+zdM/aBb5KewchhFs58OzMReea7UJx/H7dvIHj1xl1/B4CANAD+IOqFv7BT5Vtf2HDnLGh\nERGZB6MKDzw/ZP7WRCUvKnb4xKSlo1Zn2E8uyKxsWZgQOnjoMIciAsLS16cGToiJiQiMvPxk\nWrfVPB605tPy5ktrdN8jhAifvCX/5ZAF44eGhg1++38nHz20tNcnDwCAOvAcO9ydu6XVGTjA\nr6/b+IF4jh084jl2AGTFNXa4O62Xqc5mLnhl1aE7Nvr2H7F505Je6AoAAHjAih1kxoodPGLF\nDoCsuMYOAABAEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAAQBLcFQsAACAJVuwAAAAkQbADAACQ\nBMEOAABAErxSDDLjzRPojNdOAJAYK3YAAACSINgBAABIgmAHAAAgCYIdAACAJAh2AAAAkiDY\nAQAASIJgBwAAIAmCneQ+fG5yWHR8ncPt5finZ6X2bAOH3/63odGhxoERE1OWVra5Oja+FjNA\n850wU2HPHhEAANXSKIrS1z2gF4XqdWduOsL13iZ4rU+Au9169zFuu1XrF+BNNWv9xwOHfnDs\n7N6xkbrfzH8kt1/+xY8eF0KMe8DvN1X1D/fTCSE0Wv+Afj5etneveEAxOuMBxQAkxoqdzHKS\nJze53MkTxlxzuIUQdce2TBkVZww2jp6x+LTFIYTYvXZRXLjRaDQ+PGn20ca2FYkThWI3mUwO\nRVhq1wXGru+o01qdGRz/nvV6ftCQ3B0ZMwfFL/dYrTPLNyVT5uSMjTIIbb/ZWf/UUPy/Qgi3\nq/GMM2y0cUD//v379+/fe6kOAAC1IdjJLOtA8QM+muNl5ZF6rcNSYpqxJmNbsbnxWnbCV9Of\n2Gxv3peWU7Lr7FWzuf6tsZdezi7feLhEaPwqKir0Gs8FLbUb9g5ffqHq3c7VPI4Pm/je/g8n\nuW59U3KsaO2SE0kr/lkIYW8+pGj8p4+MDX5g4Jik579oaOu97wAAAKpCsFOLq/te049/f25C\nmNDoZ7+xo6HsTZdhSl3NqQSjv635us1Xa73SzRlYIYTWd+C2lx4P0Gk6V7O6uzyn31r10euv\nr95+6aEZo0OEEO3O6489NvlX+75qbL68NOH0k5NW9uQ8AQBQMV1fN4D7pLm8qen8qybT6o6P\nI0fENTic21em5+8/a4iMjQ9vvdvO312IqTeM13ZVzemO8fN8UjVkxC//5/gvvy1+NXZ6+r82\nFv1DZMbBotv/NP+XOxbmPlxr3/RQF/sCAADvEezUwjDUEDE1t2JnohBCcd/6svRrpWheXum4\n81V7jDptTWHSo+903un2vbT2pstCxAkhNBpdV9U8prqKN1LyYnM+XhQvhAgeOdtx430hRP0X\nBcU+j82eYBRCaLT+Go1vP58uTv0CAIB7walYtYieldVQ9OKuc/VCiMO5KSnLSpw3rPqg6BCd\n1mWtyl1zpr3NKYQQisvmVoQQPvoom7mg2uZyu5o2Lv6822oeDxo27R8+e2P11zecQnHte3tl\n8PBMIYRbOfDszEXnmu1Ccfx+3byB49cZdfweAgDQA/iDqhb+wU+VbX9hw5yxoRERmQejCg88\nP2T+1kQlLyp2+MSkpaNWZ9hPLsisbFmYEDp46DCHIgLC0tenBk6IiYkIjLz8ZFq31TweNHLa\nR7nPuP55cGhoePT60kf++9gKIUT45C35L4csGD80NGzw2/87+eihpb0+eQAA1IHn2OHu3C2t\nzsABfn3dxg/Ec+zQGc+xAyAxrrHD3Wm9THU2c8Erqw7dsdG3/4jNm5b0QlcAAMADVuwgM1bs\n0BkrdgAkxjV2AAAAkiDYAQAASIJgBwAAIAmCHQAAgCQIdgAAAJLgrlgAAABJsGIHAAAgCYId\nAACAJAh2AAAAkuCVYpAZb56QA++KAAAvsWIHAAAgCYIdAACAJAh2AAAAkiDYAQAASIJgBwAA\nIAmCHQAAgCQIdgAAAJIg2KnRh89NDouOr3O4vRz/9KzUnm3g7Cevm2JDA4NCHpkyb++Vmz1b\nHAAA1SLYqVH2pydOV158UO/tT7+wsKjbMW671ctqjtZjE174IKvgVEvT9U2zWp+ZlOHljgAA\n4O4IdqqTkzy5yeVOnjDmmsNdd2zLlFFxxmDj6BmLT1scHQN2r10UF240Go0PT5p9tLFtReJE\nodhNJlPjlXWBses7xrRWZwbHvyeEsF7PDxqSuyNj5qD45UIIjwXv0FKZ5z9o3bzRDwmNLvkX\nW2/WbW12Kfdl6gAASI5gpzpZB4of8NEcLys32k+YZqzJ2FZsbryWnfDV9Cc2CyHszfvSckp2\nnb1qNte/NfbSy9nlGw+XCI1fRUWFXuO5oKV2w97hyy9UveuwlHQu2FnII/nn/pDe8XXLxY/9\nBjwWpOuiNAAAuBcEO/W6uu81/fj35yaECY1+9hs7GsretLoV/YCpdTWnEoz+tubrNl+t9Ur3\nJ1i1vgO3vfR4gE7jsWDn8T5+IeFGPyHE6c9yp05Zt+y3n/T83AAAUCVdXzeAPtNc3tR0/lWT\naXXHx5Ej4hqc7mhd+wcrF+TvP2uIjI0Pb+1yZ+UviU1vGK/tumCMn0/nvV22b7KfTc2/GLl5\n/8Wf/ii8x6YEAIC6EezUyzDUEDE1t2JnohBCcd/6svTrGD+f6oK0vNJx56v2GHXamsKkR9+5\nY6fbN9Lamy4LEdfxtUaju0vBzsdV2i3po8beSNtctzPdnyVjAAB6Dn9X1St6VlZD0Yu7ztUL\nIQ7npqQsKxFCOG9Y9UHRITqty1qVu+ZMe5tTCCEUl82t+OijbOaCapvL7WrauPhzLwt29m3x\nkv2apXvWLvBV2jv03hwBAFAVgp16+Qc/Vbb9hQ1zxoZGRGQejCo88LwQYsj8rYlKXlTs8IlJ\nS0etzrCfXJBZ2bIwIXTw0GG60PT1qYETYmIiAiMvP5nmZcHOaj4tb760Rvc9vTtPAABUQ6Mo\nPGkC98Td0uoMHODX12145ciRI33dAnrAtGnT+roFAPj7wGIJ7pXWy1RnMxe8surQHRt9+4/Y\nvGlJL3QFAABYsYPUWLGTAyt2AOAlrrEDAACQBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAAkAR3\nxQIAAEiCFTsAAABJEOwAAAAkwZsnIDMeUNx7eGgwAPwNYsUOAABAEgQ7AAAASRDsAAAAJEGw\nAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwU6NPnxuclh0fJ3D7eX4p2el9mwDr8UM\n0HwnzFTYs8UBAFAt3jyhRtmfnjhz0xGu9zbWFxYWdTvGbbdq/QK8LHigue1UQ8vD/XRCCI3W\n38u9AADA3bFipzo5yZObXO7kCWOuOdx1x7ZMGRVnDDaOnrH4tMXRMWD32kVx4Uaj0fjwpNlH\nG9tWJE4Uit1kMjVeWRcYu75jTGt1ZnD8e0II6/X8oCG5OzJmDopfLoTwWPAOblfjGWfYaOOA\n/v379+/fP6Cfz32ZNwAA8iPYqU7WgeIHfDTHy8qN9hOmGWsythWbG69lJ3w1/YnNQgh78760\nnJJdZ6+azfVvjb30cnb5xsMlQuNXUVGh13guaKndsHf48gtV7zosJZ0LdmZvPqRo/KePjA1+\nYOCYpOe/aGjrvckCAKAqBDv1urrvNf349+cmhAmNfvYbOxrK3rS6Ff2AqXU1pxKM/rbm6zZf\nrfWKtds6Wt+B2156PECn8Viw8/h25/XHHpv8q31fNTZfXppw+slJK3thcgAAqBHX2KlXc3lT\n0/lXTabVHR9HjohrcLqjde0frFyQv/+sITI2Pry1y52VvyQ2vWG8tuuCMX53nmn9h8iMg99d\nszf/lzsW5j5ca9/0UKdhAADgXhHs1Msw1BAxNbdiZ6IQQnHf+rL06xg/n+qCtLzSceer9hh1\n2prCpEffuWOn2zfS2psuCxHX8bVGo7tLwc7Hrf+ioNjnsdkTjEIIjdZfo/Ht59PFWV4AAHAv\nOBWrXtGzshqKXtx1rl4IcTg3JWVZiRDCecOqD4oO0Wld1qrcNWfa25xCCKG4bG7FRx9lMxdU\n21xuV9PGxZ97WbAzt3Lg2ZmLzjXbheL4/bp5A8evM+r4PQQAoAfwB1W9/IOfKtv+woY5Y0Mj\nIjIPRhUeeF4IMWT+1kQlLyp2+MSkpaNWZ9hPLsisbFmYEDp46DBdaPr61MAJMTERgZGXn0zz\nsmBn4ZO35L8csmD80NCwwW//7+Sjh5b27jwBAFANjaJ4uLwd6Jq7pdUZOMCvr9vwypEjR/q6\nBWlNmzatr1sAANyJa+xwr7RepjqbueCVVYfu2Ojbf8TmTUt6oSsAAMCKHaTGil3vYcUOAP4G\ncY0dAACAJAh2AAAAkiDYAQAASIJgBwAAIAmCHQAAgCS4KxYAAEASrNgBAABIgmAHAAAgCd48\nAZnxgOIOPEwYAFSCFTsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGwAwAA\nkATBTo0+fG5yWHR8ncPt5finZ6X2bAOVO389Jv7BoOCwiSnLKttcPVscAADVItipUfanJ05X\nXnxQ7+1Pv7CwqNsxbrvVy2rOmxUTn3tr5Wdnms2XF/X7rx//7LCXOwIAgLsj2KlOTvLkJpc7\necKYaw533bEtU0bFGYONo2csPm1xdAzYvXZRXLjRaDQ+PGn20ca2FYkThWI3mUyNV9YFxq7v\nGNNanRkc/54Qwno9P2hI7o6MmYPilwshPBa8g63x91ZD8pxHQoS2X2r2mPo/HL0f0wYAQAUI\ndqqTdaD4AR/N8bJyo/2EacaajG3F5sZr2QlfTX9isxDC3rwvLadk19mrZnP9W2MvvZxdvvFw\nidD4VVRU6DWeC1pqN+wdvvxC1bsOS0nngp0FhP400rovv7jKfrP+o9dPDEmZ0XuTBQBAVQh2\n6nV132v68e/PTQgTGv3sN3Y0lL1pdSv6AVPrak4lGP1tzddtvlrrle5PsGp9B2576fEAncZj\nwc7jdf2G/iZr9AuPDn8oelD2ocAta8b1wuQAAFAjXV83gD7TXN7UdP5Vk2l1x8eRI+IanO5o\nXfsHKxfk7z9riIyND2/tcmflL4lNbxiv7bpgjJ/PHbvWn1gxa4vxXOONYYH60o9/9oTp+dbq\n3/bkxAAAUCuCnXoZhhoipuZW7EwUQijuW1+Wfh3j51NdkJZXOu581R6jTltTmPToO3fsdPtG\nWnvTZSHiOr7WaHR3Kdj5uNXbix6a+cGwQL0QYtyCTTcWhlS3bYv19zASAADcE07Fqlf0rKyG\nohd3nasXQhzOTUlZViKEcN6w6oOiQ3Ral7Uqd82Z9janEEIoLptb8dFH2cwF1TaX29W0cfHn\nXhbsbNAzj9V8tv6P9W1CuD/P/3n/8DmkOgAAegTBTr38g58q2/7ChjljQyMiMg9GFR54Xggx\nZP7WRCUvKnb4xKSlo1Zn2E8uyKxsWZgQOnjoMF1o+vrUwAkxMRGBkZefTPOyYGdhk9753coh\nz/5oSOjAyFd2+e0++VGvThMAAPXQKIqHy9uBrrlbWp2BA/z6ug2vHDlypK9b+Jswbdq0vm4B\nAHA/cI0d7pXWy1RnMxe8surQHRt9+4/YvGlJL3QFAABYsYPUWLHrwIodAKgE19gBAABIgmAH\nAAAgCYIdAACAJAh2AAAAkiDYAQAASIK7YgEAACTBih0AAIAkCHYAAACSINgBAABIgleKQWYq\nf/MEL5wAALVhxQ4AAEASBDsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGw\nk9yHz00Oi46vc7i9HP/0rNTeaMPtbFi1MufPH89+8ropNjQwKOSRKfP2XrnZG0cEAECFNIqi\n9HUP6EWhet2Zm45wvbcJXusT4G633n2M227V+gXcUxu/Wzz82R0ax61zQghH67GgsLn5X5TN\nM0Xs2zQz7e0Iy9X/vKdq3uMBxX3dAgDgvmLFTmY5yZObXO7kCWOuOdxCiLpjW6aMijMGG0fP\nWHza4hBC7F67KC7caDQaH540+2hj24rEiUKxm0wmhyIstesCY9d31GmtzgyOf896PT9oSO6O\njJmD4pd7rNaVa/+zMuPCP/35Y0tlnv+gdfNGPyQ0uuRfbL1Zt7XZxf9dAADQAwh2Mss6UPyA\nj+Z4WXmkXuuwlJhmrMnYVmxuvJad8NX0Jzbbm/el5ZTsOnvVbK5/a+yll7PLNx4uERq/iooK\nvcZzQUvthr3Dl1+oerdzta56cN6qmPnTY/t2vvDnLSGP5J/7Q3rH1y0XP/Yb8FiQrovjAQCA\ne0GwU4ur+17Tj39/bkKY0Ohnv7GjoexNl2FKXc2pBKO/rfm6zVdrvdLNGVghhNZ34LaXHg/Q\naTpXs7o9rropv/7xUzN3/b9hAb5/3uTjFxJu9BNCnP4sd+qUdct++0mPTRIAAHXT9XUDuE+a\ny5uazr9qMq3u+DhyRFyDw7l9ZXr+/rOGyNj48Na77fzdhZh6w3htV9Wc7hg/nzv2u/Cb1N8N\n+4+zU8Ltree/v91l+yb72dT8i5Gb91/86Y/C//rZAQAAQbBTD8NQQ8TU3IqdiUIIxX3ry9Kv\nlaJ5eaXjzlftMeq0NYVJj77Teafb99Lamy4LESeE0Gh0XVXrnOqEEJUfltdcOGjYvlCIdpfN\nbjAY8qvr04Jc6aPG3kjbXLcz3Z8lYwAAeg5/V9UielZWQ9GLu87VCyEO56akLCtx3rDqg6JD\ndFqXtSp3zZn2NqcQQigum1sRQvjoo2zmgmqby+1q2rj4826reTzoUycvWywWi8Vivvrfun7/\naLFY5hr7fVu8ZL9m6Z61C3yV9g69OnEAANSDYKcW/sFPlW1/YcOcsaEREZkHowoPPD9k/tZE\nJS8qdvjEpKWjVmfYTy7IrGxZmBA6eOgwhyICwtLXpwZOiImJCIy8/GRat9W876Tm0/LmS2t0\n39OjEwUAQL14jh3uzt3S6gwc4NfXbfxAPMeur1sAANxXLJbg7rRepjqbueCVVYfu2Ojbf8Tm\nTUt6oSsAAOABK3aQGSt2fd0CAOC+4ho7AAAASRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAElw\nVywAAIAkWLEDAACQBMEOAABAEgQ7AAAASfBKMchMzW+e4LUTAKBCrNgBAABIgmAHAAAgCYId\nAACAJAh2AAAAkiDYAQAASIJgBwAAIAmCHQAAgCQIdmr04XOTw6Lj6xxuL8c/PSu1N9pwOxtW\nrczpjcoAAKiTRlGUvu4B91uoXnfmpiNc722s1/oEuNutdx/jtlu1fgH31MbvFg9/dofGcevc\nPe11T3hAMQBAVVixU52c5MlNLnfyhDHXHO66Y1umjIozBhtHz1h82uLoGBamWoAAACAASURB\nVLB77aK4cKPRaHx40uyjjW0rEicKxW4ymRqvrAuMXd8xprU6Mzj+PSGE9Xp+0JDcHRkzB8Uv\nF0J4LOjRtf9ZmXHhn3p5rgAAqAvBTnWyDhQ/4KM5XlZutJ8wzViTsa3Y3HgtO+Gr6U9sFkLY\nm/el5ZTsOnvVbK5/a+yll7PLNx4uERq/iooKvcZzQUvthr3Dl1+oetdhKelc0CPnrYqZPz22\nb+cLvTRHAADUiWCnXlf3vaYf//7chDCh0c9+Y0dD2ZtWt6IfMLWu5lSC0d/WfN3mq7Ve6eYM\nrBBC6ztw20uPB+g0Hgt62kP59Y+fmrnr/w0L8O3xSQEAoGa6vm4Afaa5vKnp/Ksm0+qOjyNH\nxDU43dG69g9WLsjff9YQGRsf3trlzt+7NFNvGK/tumCMn88du174Tervhv3H2Snh9tbzPTkf\nAABUj2CnXoahhoipuRU7E4UQivvWl6Vfx/j5VBek5ZWOO1+1x6jT1hQmPfrOHTvdvpHW3nRZ\niLiOrzUa3V0Kdj5u5YflNRcOGrYvFKLdZbMbDIb86vq5xn69M0sAAFSEU7HqFT0rq6HoxV3n\n6oUQh3NTUpaVCCGcN6z6oOgQndZlrcpdc6a9zSmEEIrL5lZ89FE2c0G1zeV2NW1c/LmXBTt7\n6uRli8VisVjMV/9b1+8fLRYLqQ4AgB5BsFMv/+Cnyra/sGHO2NCIiMyDUYUHnhdCDJm/NVHJ\ni4odPjFp6ajVGfaTCzIrWxYmhA4eOkwXmr4+NXBCTExEYOTlJ9O8LAgAAO4bnmOHe+VuaXUG\nDvDr6za8wnPsAACqwjV2uFdaL1OdzVzwyqpDd2z07T9i86YlvdAVAABgxQ5SY8UOAKAqXGMH\nAAAgCYIdAACAJAh2AAAAkiDYAQAASIJgBwAAIAnuigUAAJAEK3YAAACSINgBAABIgmAHAAAg\nCV4pBpmp8M0TvHACANSMFTsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASRDsAAAAJEGw\nAwAAkATBTnIfPjc5LDq+zuH2cvzTs1J7toGzn7xuig0NDAp5ZMq8vVdudmy8WbN3btL4sJDg\n4VPmH2+29+wRAQBQLYKd5LI/PXG68uKDem9/0IWFRd2OcdutXlZztB6b8MIHWQWnWpqub5rV\n+sykDCGE0m5JHZv2jz/P/1Nj07up5rTpW72sBgAA7o5gJ7Oc5MlNLnfyhDHXHG4hRN2xLVNG\nxRmDjaNnLD5tcQghdq9dFBduNBqND0+afbSxbUXiRKHYTSaTQxGW2nWBses76rRWZwbHv2e9\nnh80JHdHxsxB8cs9VuuspTLPf9C6eaMfEhpd8i+23qzb2uxSWiqzS/xfWvuTEVohHvvF7tKC\nn9yv7wcAAJIj2Mks60DxAz6a42XlkXqtw1JimrEmY1uxufFadsJX05/YbG/el5ZTsuvsVbO5\n/q2xl17OLt94uERo/CoqKvQazwUttRv2Dl9+oerdztU8jg95JP/cH9I7vm65+LHfgMeCdJr6\nzysCIge/kpoUHz0kMSXLbAjupekDAKA2BDu1uLrvNf349+cmhAmNfvYbOxrK3nQZptTVnEow\n+tuar9t8tdYr3Z9g1foO3PbS4wE6TedqVrfSebyPX0i40U8Icfqz3KlT1i377SdCiLaGtvpT\nKx5anPvHC6UvDjv5L0m/7vHJAgCgTrq+bgD3SXN5U9P5V02m1R0fR46Ia3A4t69Mz99/1hAZ\nGx/eeredlduhTW8Yr+2qmtMd4+fTeVeX7ZvsZ1PzL0Zu3n/xpz8KF0Log/RBcXnLfjxSCJG2\neseCnLhrjlWRXl8FCAAAukKwUwvDUEPE1NyKnYlCCMV968vSr5WieXml485X7THqtDWFSY++\n03mn2/fS2psuCxEnhNBodF1V85jqlHZL+qixN9I21+1M9/8uuQ2cFC/e/m55T6MVQqvr4swv\nAAC4JyyTqEX0rKyGohd3nasXQhzOTUlZVuK8YdUHRYfotC5rVe6aM+1tTiGEUFw2tyKE8NFH\n2cwF1TaX29W0cfHn3VbzeNBvi5fs1yzds3aBr9LeQQgRNGx9yNWsD764KhTnnl/NDxm1KtSX\n30MAAHoAf1DVwj/4qbLtL2yYMzY0IiLzYFThgeeHzN+aqORFxQ6fmLR01OoM+8kFmZUtCxNC\nBw8d5lBEQFj6+tTACTExEYGRl59M67aax4PWfFrefGmN7nuEED76B4/ufnPbkseNodE55aMP\nHvv3Xp88AADqoFEUD9e8A99xt7Q6Awf49XUbP9CRI0f6uoX7bdq0aX3dAgCgz3CNHe5O62Wq\ns5kLXll16I6Nvv1HbN60pBe6AgAAHrBiB5mxYgcAUBWusQMAAJAEwQ4AAEASBDsAAABJEOwA\nAAAkQbADAACQBHfFAgAASIIVOwAAAEkQ7AAAACRBsAMAAJAErxSDzFT15gneOQEAYMUOAABA\nEgQ7AAAASRDsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACRBsJPch89NDouOr3O4vRz/\n9KzUnm3gZs3euUnjw0KCh0+Zf7zZ/v1/Kl3/anVbe88eDgAANSPYSS770xOnKy8+qPf2B11Y\nWNTtGLfd6mU1pd2SOjbtH3+e/6fGpndTzWnTt/75n66VfzZnbd63ToIdAAA9hmAns5zkyU0u\nd/KEMdccbiFE3bEtU0bFGYONo2csPm1xCCF2r10UF240Go0PT5p9tLFtReJEodhNJpNDEZba\ndYGx6zvqtFZnBse/Z72eHzQkd0fGzEHxyz1W66ylMrvE/6W1PxmhFeKxX+wuLfhJx/aNyWMm\nzMr41kGqAwCgJxHsZJZ1oPgBH83xsvJIvdZhKTHNWJOxrdjceC074avpT2y2N+9LyynZdfaq\n2Vz/1thLL2eXbzxcIjR+FRUVeo3ngpbaDXuHL79Q9W7nah7H139eERA5+JXUpPjoIYkpWWZD\ncMf2FQdO1dbWxvfjjXYAAPQkgp1aXN33mn78+3MTwoRGP/uNHQ1lb7oMU+pqTiUY/W3N122+\nWuuV7k+wan0Hbnvp8QCdpnM1q1vpPL6toa3+1IqHFuf+8ULpi8NO/kvSr3thZgAA4DaWTNSi\nubyp6fyrJtPqjo8jR8Q1OJzbV6bn7z9riIyND2+9287K7dCmN4zXdlXN6Y7x87ljP32QPigu\nb9mPRwoh0lbvWJATd82xKtLrC/4AAMA9IdiphWGoIWJqbsXORCGE4r71ZenXStG8vNJx56v2\nGHXamsKkR9/pvNPte2ntTZeFiBNCaDS6rqp1TnVCiIGT4sXb363kabRCaHVdnOQFAAB/PdZO\n1CJ6VlZD0Yu7ztULIQ7npqQsK3HesOqDokN0Wpe1KnfNmfY2pxBCKC6bWxFC+OijbOaCapvL\n7WrauPjzbqt5PGjQsPUhV7M++OKqUJx7fjU/ZNSqUF9+5QAA6C38lVUL/+Cnyra/sGHO2NCI\niMyDUYUHnh8yf2uikhcVO3xi0tJRqzPsJxdkVrYsTAgdPHSYQxEBYenrUwMnxMREBEZefjKt\n22oeD+qjf/Do7je3LXncGBqdUz764LF/7/2JAgCgXhpF8XDNO/Add0urM3CAX1+38QMdOXKk\nr1u4f6ZNm9bXLQAA+hjX2OHutF6mOpu54JVVh+7Y6Nt/xOZNS3qhKwAA4AErdpAZK3YAAFXh\nGjsAAABJEOwAAAAkQbADAACQBMEOAABAEgQ7AAAASXBXLAAAgCRYsQMAAJAEwQ4AAEASBDsA\nAABJ8EoxyEyON0/wSgkAgJdYsQMAAJAEwQ4AAEASBDsAAABJEOwAAAAkQbADAACQBMEOAABA\nEgQ7AAAASRDsJPfhc5PDouPrHG4vxz89K7U32ihd/2p1W/ufP1bu/PWY+AeDgsMmpiyrbHP1\nxhEBAFAhgp3ksj89cbry4oN6b3/QhYVF3Y5x26331MO18s/mrM371nk72DlvVkx87q2Vn51p\nNl9e1O+/fvyzw/dUDQAAdIVgJ7Oc5MlNLnfyhDHXHG4hRN2xLVNGxRmDjaNnLD5tcQghdq9d\nFBduNBqND0+afbSxbUXiRKHYTSaTQxGW2nWBses76rRWZwbHv2e9nh80JHdHxsxB8cs9VvNo\nY/KYCbMyvnX8ZbnO1vh7qyF5ziMhQtsvNXtM/R+O9uo3AQAA9SDYySzrQPEDPprjZeWReq3D\nUmKasSZjW7G58Vp2wlfTn9hsb96XllOy6+xVs7n+rbGXXs4u33i4RGj8Kioq9BrPBS21G/YO\nX36h6t3O1brqYcWBU7W1tfH9/vLyuoDQn0Za9+UXV9lv1n/0+okhKTN6fOIAAKgT74pVi6v7\nXtOPf39uQpgQYvYbO555a5TL8Ke6mlMDjf625j/ZfLXWr7s/war1Hbjtpce1QlR1qmZ1Lw/Q\ndpEH/y9dv6G/yRqd9OjwlQO0rc6hx2vH/ZVTAwAAHQh2atFc3tR0/lWTaXXHx5Ej4hoczu0r\n0/P3nzVExsaHt95tZ0Xp+K/eMF7bVTWnO8bPx5tO6k+smLXFeK7xxrBAfenHP3vC9Hxr9W9/\n4KwAAMD3EOzUwjDUEDE1t2JnohBCcd/6svRrpWheXum481V7jDptTWHSo+903un2vbT2pstC\nxAkhNBpdV9W8THVCiOrtRQ/N/GBYoF4IMW7BphsLQ6rbtsX6e7s7AADoCtfYqUX0rKyGohd3\nnasXQhzOTUlZVuK8YdUHRYfotC5rVe6aM+1tTiGEUFw2tyKE8NFH2cwF1TaX29W0cfHn3Vbz\nvpNBzzxW89n6P9a3CeH+PP/n/cPnkOoAAOgRBDu18A9+qmz7CxvmjA2NiMg8GFV44Pkh87cm\nKnlRscMnJi0dtTrDfnJBZmXLwoTQwUOHORQREJa+PjVwQkxMRGDk5SfTuq3mfSdhk9753coh\nz/5oSOjAyFd2+e0++VFPzhMAABXTKN9dPgV44m5pdQYO8OvrNn6gI0eO9HULPWDatGl93QIA\n4O8D19jh7rRepjqbueCVVYfu2Ojbf8TmTUt6oSsAAOABK3aQGSt2AABV4Ro7AAAASRDsAAAA\nJEGwAwAAkATBDgAAQBIEOwAAAElwVywAAIAkWLEDAACQBMEOAABAEgQ7AAAASfBKMcjs7+7N\nE7xkAgDw12DFDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4AAEASBDsAAABJEOwAAAAkQbCT\n3IfPTQ6Ljq9zuL0c//Ss1J5t4Ownr5tiQwODQh6ZMm/vlZtCiHNv/0jzf+1osPXsQQEAUCeN\noih93QN6Uahed+amI1zvbYLX+gS42613H+O2W7V+Ad5Uc7QeCwqbm/9F2TxTxL5NM9PejrBc\n/U/F1Wa1t3cMaP3mw4R5ldfO/YdO42WD94YHFAMAVIUVO5nlJE9ucrmTJ4y55nALIeqObZky\nKs4YbBw9Y/Fpi0MIsXvtorhwo9FofHjS7KONbSsSJwrFbjKZHIqw1K4LjF3fUae1OjM4/j3r\n9fygIbk7MmYOil/usVpnLZV5/oPWzRv9kNDokn+x9Wbd1maXotH59+8Q4LcmNec/Dm7opVQH\nAIDaEOxklnWg+AEfzfGy8ki91mEpMc1Yk7Gt2Nx4LTvhq+lPbLY370vLKdl19qrZXP/W2Esv\nZ5dvPFwiNH4VFRX6LpKWpXbD3uHLL1S927max/Ehj+Sf+0N6x9ctFz/2G/BY0PdCXM1/PXd4\n3JZnHuzf0/MGAEClCHZqcXXfa/rx789NCBMa/ew3djSUvekyTKmrOZVg9Lc1X7f5aq1XujkD\nK4TQ+g7c9tLjATpN52pWt4dz+j5+IeFGPyHE6c9yp05Zt+y3n/zl3xT7vy3e/5t3p/fcFAEA\nUDtdXzeA+6S5vKnp/Ksm0+qOjyNHxDU4nNtXpufvP2uIjI0Pb73bzt9diKk3jNd2Vc3pjvHz\n6byry/ZN9rOp+RcjN++/+NMfhf95e/3JjC8fXLsn0O+vnxoAAOhAsFMLw1BDxNTcip2JQgjF\nfevL0q+Vonl5pePOV+0x6rQ1hUmPvtN5p9v30tqbLgsRJ4TQaHRdVfOY6pR2S/qosTfSNtft\nTPf/v6vDezL2JOa+0YMTBAAAnIpVi+hZWQ1FL+46Vy+EOJybkrKsxHnDqg+KDtFpXdaq3DVn\n2tucQgihuGxuRQjho4+ymQuqbS63q2nj4s+7rebxoN8WL9mvWbpn7QJfpb1Dx3al3bKyvGHV\nhLBemy4AAGpEsFML/+Cnyra/sGHO2NCIiMyDUYUHnh8yf2uikhcVO3xi0tJRqzPsJxdkVrYs\nTAgdPHSYQxEBYenrUwMnxMREBEZefjKt22oeD1rzaXnzpTW67+nYbqnddMvwL/8UwIIxAAA9\niefY4e7cLa3OwAF/r1fC8Rw7AICqsGSCu9N6meps5oJXVh26Y6Nv/xGbNy3pha4AAIAHrNhB\nZqzYAQBUhWvsAAAAJEGwAwAAkATBDgAAQBIEOwAAAEkQ7AAAACTBXbEAAACSYMUOAABAEgQ7\nAAAASRDsAAAAJMErxSCzv/03T/CqCQBAD2LFDgAAQBIEOwAAAEkQ7AAAACRBsAMAAJAEwQ4A\nAEASBDsAAABJEOwAAAAkQbCT3IfPTQ6Ljq9zuL0c//Ss1J5t4Ownr5tiQwODQh6ZMm/vlZsd\nGyt3/npM/INBwWETU5ZVtrl69ogAAKgWwU5y2Z+eOF158UG9tz/owsKibse47VYvqzlaj014\n4YOsglMtTdc3zWp9ZlKGEMJ5s2Lic2+t/OxMs/nyon7/9eOfHfayGgAAuDuCncxykic3udzJ\nE8Zcc7iFEHXHtkwZFWcMNo6esfi0xSGE2L12UVy40Wg0Pjxp9tHGthWJE4ViN5lMDkVYatcF\nxq7vqNNanRkc/571en7QkNwdGTMHxS/3WK2zlso8/0Hr5o1+SGh0yb/YerNua7NLsTX+3mpI\nnvNIiND2S80eU/+Ho/fp2wEAgOwIdjLLOlD8gI/meFl5pF7rsJSYZqzJ2FZsbryWnfDV9Cc2\n25v3peWU7Dp71Wyuf2vspZezyzceLhEav4qKCr3Gc0FL7Ya9w5dfqHq3czWP40MeyT/3h/SO\nr1sufuw34LEgnSYg9KeR1n35xVX2m/UfvX5iSMqMXpo+AABqw7ti1eLqvtf049+fmxAmhJj9\nxo5n3hrlMvyprubUQKO/rflPNl+t9evuT7BqfQdue+lxrRBVnapZ3csDtHfmQR+/kHA/IYQ4\n/Vnugn9dt+z/OyuE0PUb+pus0UmPDl85QNvqHHq8dlyPTxYAAHUi2KlFc3lT0/lXTabVHR9H\njohrcDi3r0zP33/WEBkbH956t50VpeO/esN4bVfVnO4YP5/Ou7ps32Q/m5p/MXLz/os//VG4\nEKL+xIpZW4znGm8MC9SXfvyzJ0zPt1b/tqemCQCAmhHs1MIw1BAxNbdiZ6IQQnHf+rL0a6Vo\nXl7puPNVe4w6bU1h0qPvdN7p9r209qbLQsQJITQaXVfVPKY6pd2SPmrsjbTNdTvT/b877V+9\nveihmR8MC9QLIcYt2HRjYUh127ZYfw+7AwCAe8I1dmoRPSuroejFXefqhRCHc1NSlpU4b1j1\nQdEhOq3LWpW75kx7m1MIIRSXza0IIXz0UTZzQbXN5XY1bVz8ebfVPB702+Il+zVL96xd4Ku0\ndxBCDHrmsZrP1v+xvk0I9+f5P+8fPodUBwBAjyDYqYV/8FNl21/YMGdsaERE5sGowgPPD5m/\nNVHJi4odPjFp6ajVGfaTCzIrWxYmhA4eOsyhiICw9PWpgRNiYiICIy8/mdZtNY8Hrfm0vPnS\nGt33CCHCJr3zu5VDnv3RkNCBka/s8tt98qPenjsAACqhUb67fArwxN3S6gwc4NfXbfxAR44c\n6esWujFt2rS+bgEAIA+uscPdab1MdTZzwSurDt2x0bf/iM2blvRCVwAAwANW7CAzVuwAAKrC\nNXYAAACSINgBAABIgmAHAAAgCYIdAACAJAh2AAAAkuCuWAAAAEmwYgcAACAJgh0AAIAkCHYA\nAACS4JVikNnf7JsneOEEAKA3sGIHAAAgCYIdAACAJAh2AAAAkiDYAQAASIJgBwAAIAmCHQAA\ngCQIdgAAAJIg2Enuw+cmh0XH1zncXo5/elZqb7RRuv7V6rZ2bzYCAIAfjGAnuexPT5yuvPig\n3tsfdGFhUbdj3HbrPfVwrfyzOWvzvnW2d7sRAAD8NQh2MstJntzkcidPGHPN4RZC1B3bMmVU\nnDHYOHrG4tMWhxBi99pFceFGo9H48KTZRxvbViROFIrdZDI5FGGpXRcYu76jTmt1ZnD8e9br\n+UFDcndkzBwUv9xjNY82Jo+ZMCvjW0d7txsBAMBfiWAns6wDxQ/4aI6XlUfqtQ5LiWnGmoxt\nxebGa9kJX01/YrO9eV9aTsmus1fN5vq3xl56Obt84+ESofGrqKjQazwXtNRu2Dt8+YWqdztX\n66qHFQdO1dbWxvfTdbsRAAD8lQh2anF132v68e/PTQgTGv3sN3Y0lL3pMkypqzmVYPS3NV+3\n+WqtV7o/war1HbjtpccDdJrO1axu5T7MAgAA3AVLJmrRXN7UdP5Vk2l1x8eRI+IaHM7tK9Pz\n9581RMbGh7febWfldmjTG8Zru6rmdMf4+fRa+wAAoHsEO7UwDDVETM2t2JkohFDct74s/Vop\nmpdXOu581R6jTltTmPToO513un0vrb3pshBxQgiNRtdVNVIdAAB9jlOxahE9K6uh6MVd5+qF\nEIdzU1KWlThvWPVB0SE6rctalbvmTHubUwghFJfNrQghfPRRNnNBtc3ldjVtXPx5t9Xu93wA\nAEAnBDu18A9+qmz7CxvmjA2NiMg8GFV44Pkh87cmKnlRscMnJi0dtTrDfnJBZmXLwoTQwUOH\nORQREJa+PjVwQkxMRGDk5SfTuq3WJ5MCAADfp1EUrnnHXbhbWp2BA/z6uo0f6MiRI33dgmfT\npk3r6xYAABLiGjvcndbLVGczF7yy6tAdG337j9i8aUkvdAUAADxgxQ4yY8UOAKAqXGMHAAAg\nCYIdAACAJAh2AAAAkiDYAQAASIJgBwAAIAnuigUAAJAEK3YAAACSINgBAABIgmAHAAAgCV4p\nBpn9Db55gndOAAB6Dyt2AAAAkiDYAQAASIJgBwAAIAmCHQAAgCQIdgAAAJIg2AEAAEiCYAcA\nACAJgp3kPnxuclh0fJ3D7eX4p2el9kYbpetfrW5r//PHw2//29DoUOPAiIkpSyvbXL1xRAAA\nVIhgJ7nsT0+crrz4oN7bH3RhYVG3Y9x26z31cK38szlr87513g521vqPZ67+4/YvvjFfr/pX\n3/968qU/3FM1AADQFYKdzHKSJze53MkTxlxzuIUQdce2TBkVZww2jp6x+LTFIYTYvXZRXLjR\naDQ+PGn20ca2FYkThWI3mUwORVhq1wXGru+o01qdGRz/nvV6ftCQ3B0ZMwfFL/dYzaONyWMm\nzMr41vGX5TrLNyVT5uSMjTIIbb/ZWf/UUPy/vftdAABANQh2Mss6UPyAj+Z4WXmkXuuwlJhm\nrMnYVmxuvJad8NX0Jzbbm/el5ZTsOnvVbK5/a+yll7PLNx4uERq/iooKvcZzQUvthr3Dl1+o\nerdzta56WHHgVG1tbXy/v7y8Lmzie/s/nOS69U3JsaK1S04krfjnHp84AADqRLBTi6v7XtOP\nf39uQpjQ6Ge/saOh7E2XYUpdzakEo7+t+brNV2u90v0JVq3vwG0vPR6g03SuZnUr99RPa9VH\nr7++evulh2aMDvmhcwIAAP+HrvshkEJzeVPT+VdNptUdH0eOiGtwOLevTM/ff9YQGRsf3nq3\nnZXboU1vGK/tqprTHePn430/ISN++T/Hf/lt8aux09P/tbH7C/sAAEC3CHZqYRhqiJiaW7Ez\nUQihuG99Wfq1UjQvr3Tc+ao9Rp22pjDp0Xc673T7Xlp702Uh4oQQGo2uq2rep7qKN1LyYnM+\nXhQvhAgeOdtx4/2/dm4AAEAIwalY9YieldVQ9OKuc/VCiMO5KSnLSpw3rPqg6BCd1mWtyl1z\npr3NKYQQisvmVoQQPvoom7mg2uZyu5o2Lv6822redxI27R8+e2P11zecQnHte3tl8PDMnpoj\nAAAqR7BTC//gp8q2v7BhztjQiIjMg1GFB54fMn9ropIXFTt8YtLSUasz7CcXZFa2LEwIHTx0\nmEMRAWHp61MDJ8TERARGXn4yrdtq3ncSOe2j3Gdc/zw4NDQ8en3pI/99bEWPThQAAPXSKMq9\nXfMOlXG3tDoDB/j1dRs/0JEjR/q6hTtNmzatr1sAAEiLa+xwd1ovU53NXPDKqkN3bPTtP2Lz\npiW90BUAAPCAFTvIjBU7AICqcI0dAACAJAh2AAAAkiDYAQAASIJgBwAAIAmCHQAAgCS4KxYA\nAEASrNgBAABIgmAHAAAgCd48AZn17QOKeRYxAOA+Y8UOAABAEgQ7AAAASRDsAAAAJEGwAwAA\nkATBDgAAQBIEOwAAAEkQ7AAAACRBsJPch89NDouOr3O4vRz/9KzUnm3g8Nv/NjQ61DgwYmLK\n0so2V8fGmzV75yaNDwsJHj5l/vFme88eEQAA1SLYSS770xOnKy8+qPf2B11YWNTtGLfd6mU1\na/3HM///9u49vKky3fv4nTRNSzHYQ+jJ0lpoYUCBFjlYDgoVwQERC9TTiMoWL9F5p1tsAUGl\nyGwYKux2y2wPW2UUZ18w7wivFwwjJ0VRmSpgOwgMCFiO7QgthRJp2iTNev8oOtpTQmiIfdb3\n84cXXa7nzrNuH+DnOmTl/33V376pOl32SPBfxs/4TES0Btvkgdm/+D8r/nm2+pXJVdlj3/Ky\nGgAAaBvBTmUFY4ZVu9xjhtxU4XCLSPn214b3T7FGWgeMm77H5hCRdQunpcRarVZrr6FZH5+t\nm5WZIVp9enq6QxPbyUXhyYsb69QczYtMfbX29IqIHoWrcyZcn5rb+CGoXwAAG91JREFUYrXm\nbN8UD59SMDDBIsZOWXNuqNzxDxE5f2ReceiMhXf3NYrc+u/rdq69+yq1AwAA1RHsVDZny44u\nQYZPd5fEm40OW3H6uAU5K3dUna2Yl7Z37O3L689tyi4oXrPvVFXVmRcHHnpiXsnSbcViCCkt\nLTUbWi5oO7nk/T65B8teaV6txf1jMl7d/OZQ18VvirdvXPjkF6Nn3SYiZz4pDYvv/vTk0amJ\nPTInzamyRPqvAwAA6ArBTi9ObXrWPPj1e9JixGDOmr+6cvcLLsvw8uNfpllD7edO24ONtSc8\nX2A1BnddOWNkmMnQvFqtW2ttVE3Z2889l7/qULdxA6JEpK6y7syXs7pNL/z7wZ2P9951x+jf\ntedxAgCgY6ZATwBXybmS6uoDs9PT8xt/7Nc3pdLhXDV36orN+yzxyamxNW0N1i6FNrNlsLG1\nak53UkhQi6Oj+v72w09/++2O2cljpz5ydqM5whyRUjTzl/1EJDt/9QMFKRWO5+O9vgsQAAC0\nhmCnF5aelrgRhaXvZoqI5r74+c7D2sb7inYOOlC23moyHt8w+paXmw+69CxtffUxkRQRMRhM\nrVVrMdWVzp9UlFzwzrRUEYnsl+W48LqIdB2aKi99f3rPYBQxmlq58gsAAC4Lp0n0InHinMqN\nj6/Zf0ZEthVOmjSz2Hmh1hyRGGUyumrLChd81VDnFBHRXHa3JiJB5gR71dqjdpfbVb10+ice\nq7X4oTGjrnlvfv7hC07RXJtemhvZJ09EInovjjo1542/nRLNuf4/7o/q/3x0MOsQAIB2wF+o\nehEaeefuVY8tmTIwOi4ub2vChi2P9rj/rUytKCG5T8bop/rn59TveiDvyPmH06K79+zt0CQs\nZuriyeFDkpLiwuOPjc/2WK3FD40f9Xbhva7bukdHxyYu3nnjX7fPEpEg83Ufr3th5ZMjrdGJ\nBSUDtm5/xu8HDwCAPhg0rdV73gER9/kaZ/i1IYGeho8++uijAH76qFGjAvjpAAAd4h47tM3o\nZaqzV619+vkPmmwM7tx3+bIn/TArAADQAs7YQWWcsQMA6Ar32AEAACiCYAcAAKAIgh0AAIAi\nCHYAAACKINgBAAAogmAHAACgCL7uBAAAQBGcsQMAAFAEwQ4AAEARvFIMKgvImyd44QQAIFA4\nYwcAAKAIgh0AAIAiCHYAAACKINgBAAAogmAHAACgCIIdAACAIgh2AAAAiiDYKe7Nh4bFJKaW\nO9xe7n/XxMn+mIbbWfn83AJvNgIAAJ/xrljFRZtNX33niDV7m+CNQWHuhtq293HX1xpDwi5r\nGn+e3ufB1QbHxf0eN7YvvqAYAKArnLFTWcGYYdUu95ghN1U43CJSvv214f1TrJHWAeOm77E5\nRGTdwmkpsVar1dpraNbHZ+tmZWaIVp+enu7QxHZyUXjy4sY6NUfzIlNfrT29IqJH4eqcCden\n5rZYrTUVH87NOXiDNxsBAMCVINipbM6WHV2CDJ/uLok3Gx224vRxC3JW7qg6WzEvbe/Y25fX\nn9uUXVC8Zt+pqqozLw489MS8kqXbisUQUlpaaja0XNB2csn7fXIPlr3SvFprc3BeLJ3wq+2b\n3n3M40YAAHCFCHZ6cWrTs+bBr9+TFiMGc9b81ZW7X3BZhpcf/zLNGmo/d9oebKw94eEKrIgY\ng7uunDEyzGRoXq3W3eI1fe13v7xzwpr/1zss2NNGAABwpUyBngCuknMl1dUHZqen5zf+2K9v\nSqXDuWru1BWb91nik1Nja9oa/P2NmGbLYGNr1ZzupJCgJuMO/s/kP/f+/b7hsfU1B9reCAAA\nrhzBTi8sPS1xIwpL380UEc198fOdh7WN9xXtHHSgbL3VZDy+YfQtLzcfdOlZ2vrqYyIpImIw\nmFqr1jzViciRN0uOH9xqWfWwSIPLXm+xWFYcPRPW0sZ7rJ38c9wAAOgIl2L1InHinMqNj6/Z\nf0ZEthVOmjSz2Hmh1hyRGGUyumrLChd81VDnFBHRXHa3JiJB5gR71dqjdpfbVb10+iceq7X4\noXfuOmaz2Ww2W9Wpv5o6/cJms91j7dTiRr8eOwAAOkGw04vQyDt3r3psyZSB0XFxeVsTNmx5\ntMf9b2VqRQnJfTJGP9U/P6d+1wN5R84/nBbdvWdvhyZhMVMXTw4fkpQUFx5/bHy2x2oBOSgA\nAPBjfI8d2uY+X+MMvzYk0NPwEd9jBwDQFe6xQ9uMXqY6e9Xap5//oMnG4M59ly970g+zAgAA\nLeCMHVTGGTsAgK5wjx0AAIAiCHYAAACKINgBAAAogmAHAACgCIIdAACAIngqFgAAQBGcsQMA\nAFAEwQ4AAEARBDsAAABF8EoxqOzqv3mC104AAAKIM3YAAACKINgBAAAogmAHAACgCIIdAACA\nIgh2AAAAiiDYAQAAKIJgBwAAoAiCnR69+dCwmMTUcofby/3vmji5fSew7aXf9EyMtnaNy5j0\n1JE6V/sWBwBAtwh2ejTvT1/sOfL1dWZv/+tv2LDR4z7u+lovq9WeeWdC/t9X/e2bqtNljwT/\nZfyMz7wcCAAA2kaw052CMcOqXe4xQ26qcLjLt782vH+KNdI6YNz0PTZH4w7rFk5LibVardZe\nQ7M+Pls3KzNDtPr09PSzJxaFJy9u3KfmaF5k6qsiUnt6RUSPwtU5E65PzRWRFgs2YfumePiU\ngoEJFjF2yppzQ+WOf1yV4wYAQH0EO92Zs2VHlyDDp7tLrPVfpI9bkLNyR9XZinlpe8fevlxE\n6s9tyi4oXrPvVFXVmRcHHnpiXsnSbcViCCktLTUbWi5oO7nk/T65B8tecdiKmxdsLibj1c1v\nDnVd/KZ4+8aFT34xetZt/jtYAAB0hWCnX6c2PWse/Po9aTFiMGfNX125+4Vat2a+dkT58S/T\nrKH2c6ftwcbaE54vsBqDu66cMTLMZGixYGujasrefu65/FWHuo0bENWuhwUAgH6ZAj0BBMy5\nkurqA7PT0/Mbf+zXN6XS6U40Nbwx94EVm/dZ4pNTY2taHaz9K7GZLYONrRdMCglqsUBU399+\n+Olvv90xO3ns1EfOer6HDwAAeESw0y9LT0vciMLSdzNFRHNf/Hzn4aSQoKNrs4t2DjpQtt5q\nMh7fMPqWl5sMuvQgbX31MZGUxl8bDKY2Cjb/3NL5k4qSC96Zlioikf2yHBde98vhAQCgP1yK\n1a/EiXMqNz6+Zv8ZEdlWOGnSzGIRcV6oNUckRpmMrtqywgVfNdQ5RUQ0l92tBZkT7FVrj9pd\nblf10umfeFmwuZhR17w3P//wBadork0vzY3sk+e/YwQAQFcIdvoVGnnn7lWPLZkyMDouLm9r\nwoYtj4pIj/vfytSKEpL7ZIx+qn9+Tv2uB/KOnH84Lbp7z96m6KmLJ4cPSUqKC48/Nj7by4LN\nxY96u/Be123do6NjExfvvPGv22f59zgBANANg6a1ens70BL3+Rpn+LUhgZ6GVz766KOr/Imj\nRo26yp8IAMAPuMcOl8voZaqzV619+vkPmmwM7tx3+bIn/TArAADAGTsojTN2AABd4R47AAAA\nRRDsAAAAFEGwAwAAUATBDgAAQBEEOwAAAEXwVCwAAIAiOGMHAACgCIIdAACAIgh2AAAAiuCV\nYlCZv988wXsmAAA/K5yxAwAAUATBDgAAQBEEOwAAAEUQ7AAAABRBsAMAAFAEwQ4AAEARBDsA\nAABFEOwU9+ZDw2ISU8sdbi/3v2viZH9MY+fi2UfrGn74cd8fn0tPjg6PiLpx+H3vn/jOH58I\nAIAOEewUN+9PX+w58vV1Zm//Q2/YsNHjPu762suaQ0XJe1MWFn3rvBTsHDXbhzz2xpy1X56v\nPr1sYs29Q3MuqxoAAGgNwU5lBWOGVbvcY4bcVOFwi0j59teG90+xRloHjJu+x+YQkXULp6XE\nWq1Wa6+hWR+frZuVmSFafXp6ukMT28lF4cmLG+vUHM2LTH219vSKiB6Fq3MmXJ+a22K1Fi0d\nc9OQiTnfOv51uu78kaLQ6xfdN6CbGExj/v2t78rfOufS/NsIAAD0gWCnsjlbdnQJMny6uyTe\nbHTYitPHLchZuaPqbMW8tL1jb19ef25TdkHxmn2nqqrOvDjw0BPzSpZuKxZDSGlpqdnQckHb\nySXv98k9WPZK82qtzWHWli9PnjyZ2ulfL6+LunHF/s+mNv76/NfvhFx7a4Splc8DAACXg2Cn\nF6c2PWse/Po9aTFiMGfNX125+wWXZXj58S/TrKH2c6ftwcbaE54vsBqDu66cMTLMZGherdbt\n7Vm3oJCoWGuIiOx5r3DE8EUz//ePV3RgAADgeybPu0AJ50qqqw/MTk/Pb/yxX9+USodz1dyp\nKzbvs8Qnp8bWtDVYuxTazJbBxtaqOd1JIUFeTsZl/2beg5NXfB2/fPPXv7o51pfjAQAAzRDs\n9MLS0xI3orD03UwR0dwXP995WNt4X9HOQQfK1ltNxuMbRt/ycvNBl56lra8+JpIiIgaDqbVq\n3qc6rcE2tf/AC9nLy9+dGsopYwAA2g9/r+pF4sQ5lRsfX7P/jIhsK5w0aWax80KtOSIxymR0\n1ZYVLviqoc4pIqK57G5NRILMCfaqtUftLrereun0TzxW834m3+54crPhqfULHwjWGhq11zEC\nAKBzBDu9CI28c/eqx5ZMGRgdF5e3NWHDlkd73P9WplaUkNwnY/RT/fNz6nc9kHfk/MNp0d17\n9nZoEhYzdfHk8CFJSXHh8cfGZ3us5v1Mjv+p5NyhBaYfadcDBQBAvwyaxjdNoA3u8zXO8GtD\nAj0NH3300Ud+rT9q1Ci/1gcA4LJwsgRtM3qZ6uxVa59+/oMmG4M7912+7Ek/zAoAALSAM3ZQ\nGWfsAAC6wj12AAAAiiDYAQAAKIJgBwAAoAiCHQAAgCIIdgAAAIrgqVgAAABFcMYOAABAEQQ7\nAAAARRDsAAAAFMErxaAyv755gtdOAAB+bjhjBwAAoAiCHQAAgCIIdgAAAIog2AEAACiCYAcA\nAKAIgh0AAIAiCHYAAACKINjp0ZsPDYtJTC13uL3c/66Jk9vx0/e/dLPhp1ZX2tuxPgAAumXQ\nNC3Qc8DVFm02ffWdI9bsbaw3BoW5G2rb3sddX2sMCfOmmuaqq61vaPx1zTdvpt13pGL/700G\nL+dyefiCYgCArnDGTncKxgyrdrnHDLmpwuEu3/7a8P4p1kjrgHHT99gcjTusWzgtJdZqtVp7\nDc36+GzdrMwM0erT09PPnlgUnry4cZ+ao3mRqa+KSO3pFRE9ClfnTLg+NVdEWizYhMEU2rlR\nWMiCyQW/37rET6kOAAC9IdjpzpwtO7oEGT7dXWKt/yJ93IKclTuqzlbMS9s79vblIlJ/blN2\nQfGafaeqqs68OPDQE/NKlm4rFkNIaWmpuZX4ZTu55P0+uQfLXnHYipsXbMPxvzy0bdBr917X\nud2PEQAAfSLY6depTc+aB79+T1qMGMxZ81dX7n6h1q2Zrx1RfvzLNGuo/dxpe7Cx9oSHK7Ai\nYgzuunLGyDCTocWCrQ7T6n8zffP/vDK2PQ8JAAB9MwV6AgiYcyXV1Qdmp6fnN/7Yr29KpdOd\naGp4Y+4DKzbvs8Qnp8bWtDr4R7dmmi2Dja0XTAoJarHAmV05n1+3cH14SPscDAAAINjpmaWn\nJW5EYem7mSKiuS9+vvNwUkjQ0bXZRTsHHShbbzUZj28YfcvLTQZdepC2vvqYSErjrw0GUxsF\nW/v09TnrMwvnt/cxAQCga1yK1a/EiXMqNz6+Zv8ZEdlWOGnSzGIRcV6oNUckRpmMrtqywgVf\nNdQ5RUQ0l92tBZkT7FVrj9pdblf10umfeFmwRVqDbW5J5fNDYvx0aAAA6BPBTr9CI+/cveqx\nJVMGRsfF5W1N2LDlURHpcf9bmVpRQnKfjNFP9c/Pqd/1QN6R8w+nRXfv2dsUPXXx5PAhSUlx\n4fHHxmd7WbBFtpPLLlruuCGME8YAALQnvscOl8t9vsYZfm3HuDeO77EDAOgKp0xwuYxepjp7\n1dqnn/+gycbgzn2XL3vSD7MCAACcsYPSOGMHANAV7rEDAABQBMEOAABAEQQ7AAAARRDsAAAA\nFEGwAwAAUARPxQIAACiCM3YAAACKINgBAAAogmAHAACgCIIdAACAIgh2AAAAiiDYAQAAKIJg\nBwAAoAiCHQAAgCIIdgAAAIog2AEAACiCYAcAAKAIgh0AAIAiCHYAAACKINgBAAAogmAHAACg\nCIIdAACAIgh2AAAAiiDYAQAAKIJgBwAAoAiCHQAAgCJMgZ4A4C8Oh2PJkiW9evUyGvkfmKvH\n7Xbv3bu3b9++tP1qou0BQdsDwu12f/31188884zZbA70XH6OCHZQ1osvvpifnx/oWQAA2p/R\naHzuuecCPYufI4IdlJWamioiM2fOzMjICPRcdKS4uLioqIi2X2W0PSBoe0A0tr3xT3g0R7CD\nshovjmRkZGRnZwd6LvpSVFRE268+2h4QtD0gioqKuPzdGvoCAACgCIIdAACAIgh2AAAAiiDY\nAQAAKIJgBwAAoAiCHQAAgCIIdgAAAIog2AEAACiCYAcAAKAIgh2U1alTpx/+iauGtgcEbQ8I\n2h4QtL1tBk3TAj0HwC8aGho+/PDD2267LSgoKNBz0RHaHhC0PSBoe0DQ9rYR7AAAABTBpVgA\nAABFEOwAAAAUQbADAABQBMEOAABAEQQ7AAAARRDsAAAAFEGwAwAAUATBDgAAQBEEOwAAAEUQ\n7AAAABRBsAMAAFAEwQ4AAEARBDsAAABFEOwAAAAUQbADAABQBMEOAABAEQQ7dCDura8/O7Jf\nsiUkNLpbn4fyXqpwuK94iA819ab92z4t9hpDM+HJi/13DB2Q7yuz9sw76enpey4627Gmbvij\n7Sx4jy677W5n5avPzhjc6/prw8ydw7sOysx+Y/ORKy+rCA3oIP7860Ei0jk+/d6pD95+UzcR\nibzxoRqX+0qG+FBTb/zR9jhzkCm0+8CfGnnXH/x/NB3GlazM9x//hYj87UJ9O9bUCX+0XWPB\ne3K5bW9wVj7cJ0JELEmDfvVvj2WNGRZiNBgMQY+8sfdKyiqDYIeO4cKxV4IMhi7dH66ob2jc\n8scZN4jIyKJ9Pg/xoabe+KPtDluJiCSN/8Dfk++4fF6Z350+srrw1yaDoXnCYLV75I+2ayx4\nT3xo+57f3SwiiRN+Z/s+pZ3eteq6kKAgc8z+i06fyyqDYIeOYUt2dxF5ek/VD1tcdUcjg42d\nrFk+D/Ghpt74o+0XTiwSkZv/e7//pt3R+bYyRyZG/vhqTJOEwWr3yB9t11jwnvjQ9twEi8EQ\ntKPmJ63+7Nd9ROTuTyp8LqsMgh06hrutnYym8As/PYte0CNcRHbaHL4N8aGm3vij7eXbx4nI\n/V9V+m/aHZ1vK/Otl4qWLVu2bNmye7qGNU8YrHaP/NF2jQXviQ9tH3CNOaRLRpONR9/LFJFh\nbxz0uawyeHgCHYDmrt1YXRcaeYclyPDj7UNuihKR96rsPgzxoabe+KPtIvLt1goRidu5ckJG\n/+guoV2i4m65a9qaL07770A6Fp9X5iM5T+Xm5ubm5t4REdpeNfXDH21vxIJvg29tX7lj167i\n/9tk4553jopIz0FRPpdVBsEOHUBD/Yl6txYcdmOT7V36dBGRw7UtPIbmcYgPNfXGH20XkYoP\nT4tI0WOzjgV3uyMrK717+GcbVt47rPu8jaf8cRQdjj9WJqvdI/+1iAXfBt/afmO/fn37dPvx\nlm93FD24/nhIl6GFN0T5XFYZBDt0AG5nlYgYg7o02R58TbCI1Na08LvU4xAfauqNP9ouIjur\nxdLFmvv27r2fbHhn5ertuw4c/uviYM2+bMqYb3XyZQRt8sfKZLV75L8WseDbcOVt1xpq/nfR\no6m35tmNUUs/XBduMrRL2Q6NYIcOwGiKEBF3g63Jdud3ThEJsZh8GOJDTb3xR9tFZOHBigs1\nlUsfGvDDv+3xy2f+OKabs/bA7L1V7XkAHZM/Viar3SP/tYgF34YrbPuhza+NTOk29bk/BKfe\nvnr3wd8MtLZL2Y6OYIcOICj0+lCjwWU/2GS77aBNRFI6B/swxIeaeuOPtrf2WUNyeorIoc8q\nr3DOCvDHymS1e3SVW8SCb+Rz292u6qWPjuh1xxPFVV1zX3qvfP/G7H7/ejxZ5wueYIcOwGDs\nPDYitK56U91PL1zs+fKsiEyydvJhiA819cYfbRdxNzQ0uLWmA4NCgkQkuIvif+B6wx8rk9Xu\nkd9axIJvi29t19wXczNvnP2Hz/pNmbfvnweX5dzdyfiThyR0vuAJdugYfn1rbIOz8sWy8z9s\ncTurCk5c6GS9+2aL2bchPtTUm3Zvu73qPZPJFJNW2GTU3189LCKjRsb44SA6Hn+sTFa7R/5o\nEQveIx/a/vclY//r03+m56za8+6inte0HI51veAD/X0rgFcuHH3FYDB0vWmu/dK3iGsf/8cI\nEbn1vy59jbjbdeHYsWPHT/zT+yEed4A/2n5//DUGQ9CcdQd/GFL+yX93MRk7x2U71X/Zj1d8\naPuP/aFnpDR/8wSr3RNvWtRG51tsu8aC9+Ty2+4aaDEHd77hXJvt0/OCJ9ihw/jTjP4iEn9z\n1tz58x+fMtxgMET0fqT6+9/btlPLRMR8zQDvh3izA9q97dX7VsSHBBkMhr6jxj887cHbh6WZ\nDIbgsNRVR2qu9rH9jPnQ9h+0ljBY7R55bFEbnW+t7Sx4jy6r7fazfxERU2jyyJY8849q78uq\nimCHDsS17j+fHpyaEBZsjorrcd9vCk59/xJArdU/cNsa4t0OaP+22459mvfIxJR4a0hQcGRs\nyt3/Nu/ziotX5Vg6EB/afklrCYPV7gVPS/fyg53GgvfsMtp+/puZbVyEHP/5t96XVZVB05rd\n1QkAAIAOiIcnAAAAFEGwAwAAUATBDgAAQBEEOwAAAEUQ7AAAABRBsAMAAFAEwQ4AAEARBDsA\nAABFEOwAAAAUQbADAABQBMEOAABAEQQ7AAAARRDsAAAAFEGwAwAAUATBDgAAQBEEOwAAAEUQ\n7AAAABRBsAMAAFAEwQ4AAEARBDsAAABFEOwAAAAUQbADAABQBMEOAABAEQQ7AAAARRDsAAAA\nFEGwAwAAUATBDgAAQBEEOwAAAEUQ7AAAABRBsAMAAFAEwQ4AAEARBDsAAABFEOwAAAAUQbAD\nAABQBMEOAABAEQQ7AAAARRDsAAAAFEGwAwAAUATBDgAAQBEEOwAAAEUQ7AAAABRBsAMAAFAE\nwQ4AAEARBDsAAABFEOwAAAAUQbADAABQBMEOAABAEQQ7AAAARRDsAAAAFEGwAwAAUATBDgAA\nQBEEOwAAAEUQ7AAAABRBsAMAAFAEwQ4AAEARBDsAAABFEOwAAAAUQbADAABQBMEOAABAEQQ7\nAAAARRDsAAAAFEGwAwAAUATBDgAAQBEEOwAAAEUQ7AAAABRBsAMAAFAEwQ4AAEARBDsAAABF\nEOwAAAAUQbADAABQBMEOAABAEQQ7AAAARRDsAAAAFEGwAwAAUATBDgAAQBEEOwAAAEX8f+ql\nsB4PSRxxAAAAAElFTkSuQmCC"
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare DMatrix objects\n",
    "train_matrix <- xgb.DMatrix(data = as.matrix(train_data_filtered), label = train_labels, weight = train_weights)\n",
    "validation_matrix <- xgb.DMatrix(data = as.matrix(validation_data_filtered), label = validation_labels)\n",
    "test_matrix <- xgb.DMatrix(data = as.matrix(test_data_filtered), label = test_labels)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params <- list(\n",
    "  objective = \"multi:softmax\",\n",
    "  num_class = 4,\n",
    "  eval_metric = c(\"merror\", \"mlogloss\"),\n",
    "  max_depth = 6,\n",
    "  eta = 0.1,\n",
    "  subsample = 0.8,\n",
    "  colsample_bytree = 0.8,\n",
    "  min_child_weight = 1,\n",
    "  nthread = parallel::detectCores() - 1\n",
    ")\n",
    "\n",
    "# Create watchlist for early stopping\n",
    "watchlist <- list(train = train_matrix, val = validation_matrix)\n",
    "\n",
    "# Train model with early stopping\n",
    "xgb_model <- xgb.train(\n",
    "  params = params,\n",
    "  data = train_matrix,\n",
    "  nrounds = 1000,\n",
    "  watchlist = watchlist,\n",
    "  early_stopping_rounds = 20,\n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "# Make predictions on test matrix\n",
    "test_predictions <- predict(xgb_model, test_matrix)\n",
    "\n",
    "# Make predictions on train matrix\n",
    "train_predictions <- predict(xgb_model, train_matrix)\n",
    "\n",
    "# Evaluate results\n",
    "cm_test <- confusionMatrix(\n",
    "  factor(test_predictions, levels = 0:3, labels = levels(full_df$class)),\n",
    "  factor(test_df$class_numeric, levels = 0:3, labels = levels(full_df$class))\n",
    ")\n",
    "\n",
    "# Evaluate results\n",
    "cm_train <- confusionMatrix(\n",
    "  factor(train_predictions, levels = 0:3, labels = levels(full_df$class)),\n",
    "  factor(train_df$class_numeric, levels = 0:3, labels = levels(full_df$class))\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(cm_test)\n",
    "\n",
    "cat(\"\\nClass-Specific Statistics:\\n\")\n",
    "print(cm_test$byClass)\n",
    "\n",
    "cat(\"\\nConfusion Matrix Mode:\\n\")\n",
    "print(cm_test$mode)\n",
    "\n",
    "cat(\"\\nAdditional Arguments Passed to confusionMatrix():\\n\")\n",
    "print(cm_test$dots)\n",
    "\n",
    "# Print overall accuracy results\n",
    "accuracy_test <- cm_test$overall['Accuracy']\n",
    "accuracy_train <- cm_train$overall['Accuracy']\n",
    "cat(\"Final Test Accuracy:\", accuracy_test, \"\\n\")\n",
    "cat(\"Final Train Accuracy:\", accuracy_train, \"\\n\")\n",
    "\n",
    "feature_names <- colnames(train_data_filtered)\n",
    "xgb_importance <- xgb.importance(feature_names = feature_names, model = xgb_model)\n",
    "\n",
    "# Select Top 20 Features\n",
    "top_20_features <- head(xgb_importance, 20)\n",
    "\n",
    "# Plot the top 20 features\n",
    "xgb.plot.importance(top_20_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb02997e",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "execution": {
     "iopub.execute_input": "2024-11-19T15:02:01.926598Z",
     "iopub.status.busy": "2024-11-19T15:02:01.924942Z",
     "iopub.status.idle": "2024-11-19T15:02:02.030215Z",
     "shell.execute_reply": "2024-11-19T15:02:02.028424Z"
    },
    "papermill": {
     "duration": 0.117715,
     "end_time": "2024-11-19T15:02:02.032622",
     "exception": false,
     "start_time": "2024-11-19T15:02:01.914907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost-Weighted Misclassification Matrix:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Benign Early Pre Pro\n",
      "Benign      0    20   0   0\n",
      "Early     100     0  90   0\n",
      "Pre        20    30   0   0\n",
      "Pro        90     0  30   0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassification Cost: 380 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cost Per Sample: 0.7335907 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost-Sensitive Accuracy: 0.986662 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1-score 0.9419104 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the cost matrix (replace with your actual costs)\n",
    "cost_matrix <- matrix(\n",
    "  c(0, 10, 20, 30,   # Benign -> Benign, Early, Pre, Pro\n",
    "    10, 0, 15, 25,    # Early -> Benign, Early, Pre, Pro\n",
    "    20, 15, 0, 10,    # Pre -> Benign, Early, Pre, Pro\n",
    "    30, 25, 10, 0),   # Pro -> Benign, Early, Pre, Pro\n",
    "  nrow = 4, byrow = TRUE\n",
    ")\n",
    "rownames(cost_matrix) <- c(\"Benign\", \"Early\", \"Pre\", \"Pro\")\n",
    "colnames(cost_matrix) <- c(\"Benign\", \"Early\", \"Pre\", \"Pro\")\n",
    "\n",
    "# Extract confusion matrix data\n",
    "cm_table <- cm_test$table\n",
    "classes <- rownames(cm_table)\n",
    "\n",
    "visualize_cost_confusion_matrix <- function(cm_test, cost_matrix) {\n",
    "  # Extract the confusion matrix from caret's result\n",
    "  cm_table <- cm_test$table\n",
    "  \n",
    "  # Create cost-weighted matrix\n",
    "  cost_weighted_cm <- matrix(0, \n",
    "                              nrow = nrow(cost_matrix), \n",
    "                              ncol = ncol(cost_matrix))\n",
    "  \n",
    "  for (i in 1:nrow(cm_table)) {\n",
    "    for (j in 1:ncol(cm_table)) {\n",
    "      if (i != j) {\n",
    "        cost_weighted_cm[i, j] <- cm_table[i, j] * cost_matrix[i, j]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Set row and column names\n",
    "  rownames(cost_weighted_cm) <- rownames(cost_matrix)\n",
    "  colnames(cost_weighted_cm) <- colnames(cost_matrix)\n",
    "  \n",
    "  return(cost_weighted_cm)\n",
    "}\n",
    "\n",
    "# Usage\n",
    "cost_weighted_matrix <- visualize_cost_confusion_matrix(cm_test, cost_matrix)\n",
    "cat(\"\\nCost-Weighted Misclassification Matrix:\\n\")\n",
    "print(cost_weighted_matrix)\n",
    "    \n",
    "# Initialize total misclassification cost\n",
    "total_cost <- 0\n",
    "\n",
    "# Calculate total misclassification cost by iterating over confusion matrix entries\n",
    "for (i in 1:nrow(cm_table)) {\n",
    "  for (j in 1:ncol(cm_table)) {\n",
    "    if (i != j) {  # Misclassifications (i != j)\n",
    "      total_cost <- total_cost + cm_table[i, j] * cost_matrix[i, j]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Output the total misclassification cost\n",
    "cat(\"Total Misclassification Cost:\", total_cost, \"\\n\")\n",
    "\n",
    "# Calculate average cost per sample\n",
    "total_samples <- sum(cm_table)\n",
    "avg_cost_per_sample <- total_cost / total_samples\n",
    "cat(\"Average Cost Per Sample:\", avg_cost_per_sample, \"\\n\")\n",
    "\n",
    "# Calculate cost-sensitive accuracy\n",
    "# Compute the maximum possible cost (if all instances were misclassified to the most costly class)\n",
    "max_possible_cost <- sum(cost_matrix) * (total_samples / nrow(cost_matrix))\n",
    "cost_sensitive_accuracy <- 1 - (total_cost / max_possible_cost)\n",
    "cat(\"Cost-Sensitive Accuracy:\", cost_sensitive_accuracy, \"\\n\")\n",
    "\n",
    "# Using cost matrix for cost-weighted F1 calculation\n",
    "f1_scores <- cm_test$byClass[, \"F1\"]\n",
    "weights <- 1 / (diag(cost_matrix) + 1)  # Add 1 to avoid divide-by-zero\n",
    "weights <- weights / sum(weights)\n",
    "weighted_f1 <- sum(f1_scores * weights, na.rm = TRUE)\n",
    "cat(\"Weighted F1-score\", weighted_f1, \"\\n\")\n",
    "\n",
    "# Save the trained model\n",
    "xgb.save(xgb_model, \"xgb_model.model\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5938024,
     "sourceId": 9708505,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30751,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "r",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 204.412909,
   "end_time": "2024-11-19T15:02:02.163000",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T14:58:37.750091",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
