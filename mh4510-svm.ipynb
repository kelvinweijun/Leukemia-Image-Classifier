{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d0b00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T15:48:08.300768Z",
     "iopub.status.busy": "2024-11-19T15:48:08.298987Z",
     "iopub.status.idle": "2024-11-19T15:48:11.929136Z",
     "shell.execute_reply": "2024-11-19T15:48:11.927681Z"
    },
    "papermill": {
     "duration": 3.636364,
     "end_time": "2024-11-19T15:48:11.931166",
     "exception": false,
     "start_time": "2024-11-19T15:48:08.294802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: magrittr\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘imager’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:magrittr’:\n",
      "\n",
      "    add\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    where\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    convolve, spectrum\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:graphics’:\n",
      "\n",
      "    frame\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    save.image\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:tensorflow’:\n",
      "\n",
      "    train\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following object is masked from ‘package:httr’:\n",
      "\n",
      "    progress\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘EBImage’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from ‘package:imager’:\n",
      "\n",
      "    channel, dilate, display, erode, resize, watershed\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>image_filename</th><th scope=col>class</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-002.jpg</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-004.jpg</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-005.jpg</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg</td><td>Benign</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & image\\_filename & class\\\\\n",
       "  & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg & Benign\\\\\n",
       "\t2 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-002.jpg & Benign\\\\\n",
       "\t3 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg & Benign\\\\\n",
       "\t4 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-004.jpg & Benign\\\\\n",
       "\t5 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-005.jpg & Benign\\\\\n",
       "\t6 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg & Benign\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | image_filename &lt;chr&gt; | class &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| 1 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg | Benign |\n",
       "| 2 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-002.jpg | Benign |\n",
       "| 3 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg | Benign |\n",
       "| 4 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-004.jpg | Benign |\n",
       "| 5 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-005.jpg | Benign |\n",
       "| 6 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg | Benign |\n",
       "\n"
      ],
      "text/plain": [
       "  image_filename                                                   class \n",
       "1 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg Benign\n",
       "2 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-002.jpg Benign\n",
       "3 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg Benign\n",
       "4 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-004.jpg Benign\n",
       "5 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-005.jpg Benign\n",
       "6 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg Benign"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load necessary libraries\n",
    "library(keras)\n",
    "library(tensorflow)\n",
    "library(dplyr) # For data manipulation\n",
    "library(imager)\n",
    "library(caret)\n",
    "library(e1071)\n",
    "library(ggplot2)\n",
    "library(EBImage)  # For bwlabel and shape feature extraction\n",
    "\n",
    "# Set the path to your dataset\n",
    "data_dir <- \"/kaggle/input/leukemia-images/Original\"\n",
    "\n",
    "# Define image size and parameters\n",
    "img_height <- 150\n",
    "img_width <- 150\n",
    "batch_size <- 32\n",
    "\n",
    "# Create a list to hold all file paths and corresponding labels\n",
    "file_paths <- c()\n",
    "class_labels <- c()\n",
    "\n",
    "# Loop through each class to gather file paths and labels\n",
    "for (class in c(\"Benign\", \"Early\", \"Pre\", \"Pro\")) {\n",
    "  class_path <- file.path(data_dir, class)\n",
    "  files <- list.files(class_path, full.names = TRUE)\n",
    "  \n",
    "  # Ensure files are unique\n",
    "  unique_files <- unique(files)\n",
    "\n",
    "  file_paths <- c(file_paths, unique_files)\n",
    "  class_labels <- c(class_labels, rep(class, length(unique_files)))  # Store the class name\n",
    "}\n",
    "\n",
    "# Create a DataFrame with only the required structure\n",
    "full_df <- data.frame(\n",
    "  image_filename = file_paths,\n",
    "  class = class_labels,\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "head(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d3b7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T15:48:11.965420Z",
     "iopub.status.busy": "2024-11-19T15:48:11.938376Z",
     "iopub.status.idle": "2024-11-19T15:48:12.028185Z",
     "shell.execute_reply": "2024-11-19T15:48:12.026853Z"
    },
    "papermill": {
     "duration": 0.095738,
     "end_time": "2024-11-19T15:48:12.030002",
     "exception": false,
     "start_time": "2024-11-19T15:48:11.934264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>image_filename</th><th scope=col>class</th><th scope=col>class_numeric</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1618</th><td>/kaggle/input/leukemia-images/Original/Pre/WBC-Malignant-Pre-129.jpg    </td><td>Pre   </td><td>2</td></tr>\n",
       "\t<tr><th scope=row>1098</th><td>/kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-594.jpg</td><td>Early </td><td>1</td></tr>\n",
       "\t<tr><th scope=row>3176</th><td>/kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-724.jpg    </td><td>Pro   </td><td>3</td></tr>\n",
       "\t<tr><th scope=row>104</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-104.jpg        </td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>514</th><td>/kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-010.jpg</td><td>Early </td><td>1</td></tr>\n",
       "\t<tr><th scope=row>3161</th><td>/kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-709.jpg    </td><td>Pro   </td><td>3</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & image\\_filename & class & class\\_numeric\\\\\n",
       "  & <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1618 & /kaggle/input/leukemia-images/Original/Pre/WBC-Malignant-Pre-129.jpg     & Pre    & 2\\\\\n",
       "\t1098 & /kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-594.jpg & Early  & 1\\\\\n",
       "\t3176 & /kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-724.jpg     & Pro    & 3\\\\\n",
       "\t104 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-104.jpg         & Benign & 0\\\\\n",
       "\t514 & /kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-010.jpg & Early  & 1\\\\\n",
       "\t3161 & /kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-709.jpg     & Pro    & 3\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 3\n",
       "\n",
       "| <!--/--> | image_filename &lt;chr&gt; | class &lt;chr&gt; | class_numeric &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 1618 | /kaggle/input/leukemia-images/Original/Pre/WBC-Malignant-Pre-129.jpg     | Pre    | 2 |\n",
       "| 1098 | /kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-594.jpg | Early  | 1 |\n",
       "| 3176 | /kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-724.jpg     | Pro    | 3 |\n",
       "| 104 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-104.jpg         | Benign | 0 |\n",
       "| 514 | /kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-010.jpg | Early  | 1 |\n",
       "| 3161 | /kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-709.jpg     | Pro    | 3 |\n",
       "\n"
      ],
      "text/plain": [
       "     image_filename                                                          \n",
       "1618 /kaggle/input/leukemia-images/Original/Pre/WBC-Malignant-Pre-129.jpg    \n",
       "1098 /kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-594.jpg\n",
       "3176 /kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-724.jpg    \n",
       "104  /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-104.jpg        \n",
       "514  /kaggle/input/leukemia-images/Original/Early/WBC-Malignant-Early-010.jpg\n",
       "3161 /kaggle/input/leukemia-images/Original/Pro/WBC-Malignant-Pro-709.jpg    \n",
       "     class  class_numeric\n",
       "1618 Pre    2            \n",
       "1098 Early  1            \n",
       "3176 Pro    3            \n",
       "104  Benign 0            \n",
       "514  Early  1            \n",
       "3161 Pro    3            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>image_filename</th><th scope=col>class</th><th scope=col>class_numeric</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>6</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-008.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>15</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-015.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-019.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-021.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>28</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-028.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & image\\_filename & class & class\\_numeric\\\\\n",
       "  & <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t6 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg & Benign & 0\\\\\n",
       "\t8 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-008.jpg & Benign & 0\\\\\n",
       "\t15 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-015.jpg & Benign & 0\\\\\n",
       "\t19 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-019.jpg & Benign & 0\\\\\n",
       "\t21 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-021.jpg & Benign & 0\\\\\n",
       "\t28 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-028.jpg & Benign & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 3\n",
       "\n",
       "| <!--/--> | image_filename &lt;chr&gt; | class &lt;chr&gt; | class_numeric &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 6 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg | Benign | 0 |\n",
       "| 8 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-008.jpg | Benign | 0 |\n",
       "| 15 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-015.jpg | Benign | 0 |\n",
       "| 19 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-019.jpg | Benign | 0 |\n",
       "| 21 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-021.jpg | Benign | 0 |\n",
       "| 28 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-028.jpg | Benign | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "   image_filename                                                   class \n",
       "6  /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-006.jpg Benign\n",
       "8  /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-008.jpg Benign\n",
       "15 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-015.jpg Benign\n",
       "19 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-019.jpg Benign\n",
       "21 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-021.jpg Benign\n",
       "28 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-028.jpg Benign\n",
       "   class_numeric\n",
       "6  0            \n",
       "8  0            \n",
       "15 0            \n",
       "19 0            \n",
       "21 0            \n",
       "28 0            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>image_filename</th><th scope=col>class</th><th scope=col>class_numeric</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-009.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-012.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-017.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>18</th><td>/kaggle/input/leukemia-images/Original/Benign/WBC-Benign-018.jpg</td><td>Benign</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & image\\_filename & class & class\\_numeric\\\\\n",
       "  & <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg & Benign & 0\\\\\n",
       "\t3 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg & Benign & 0\\\\\n",
       "\t9 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-009.jpg & Benign & 0\\\\\n",
       "\t12 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-012.jpg & Benign & 0\\\\\n",
       "\t17 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-017.jpg & Benign & 0\\\\\n",
       "\t18 & /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-018.jpg & Benign & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 3\n",
       "\n",
       "| <!--/--> | image_filename &lt;chr&gt; | class &lt;chr&gt; | class_numeric &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg | Benign | 0 |\n",
       "| 3 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg | Benign | 0 |\n",
       "| 9 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-009.jpg | Benign | 0 |\n",
       "| 12 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-012.jpg | Benign | 0 |\n",
       "| 17 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-017.jpg | Benign | 0 |\n",
       "| 18 | /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-018.jpg | Benign | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "   image_filename                                                   class \n",
       "1  /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-001.jpg Benign\n",
       "3  /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-003.jpg Benign\n",
       "9  /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-009.jpg Benign\n",
       "12 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-012.jpg Benign\n",
       "17 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-017.jpg Benign\n",
       "18 /kaggle/input/leukemia-images/Original/Benign/WBC-Benign-018.jpg Benign\n",
       "   class_numeric\n",
       "1  0            \n",
       "3  0            \n",
       "9  0            \n",
       "12 0            \n",
       "17 0            \n",
       "18 0            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define class-to-index mapping\n",
    "classes <- c(\"Benign\", \"Early\", \"Pre\", \"Pro\")\n",
    "class_to_index <- setNames(0:(length(classes) - 1), classes)\n",
    "\n",
    "# Add numeric columns to the full and training DataFrames\n",
    "full_df$class_numeric <- as.numeric(factor(full_df$class, levels = classes)) - 1\n",
    "\n",
    "# Stratified split into training (64%), validation (20%), and test (16%) sets\n",
    "set.seed(123)\n",
    "trainIndex <- createDataPartition(full_df$class, p = 0.64, list = FALSE)\n",
    "train_df <- full_df[trainIndex, ]\n",
    "remaining_df <- full_df[-trainIndex, ]\n",
    "\n",
    "# Shuffle only train_df\n",
    "train_df <- train_df[sample(nrow(train_df)), ]\n",
    "\n",
    "# Split the remaining 36% into validation (20%) and test (16%)\n",
    "validationIndex <- createDataPartition(remaining_df$class, p = 20 / (20 + 16), list = FALSE)\n",
    "val_df <- remaining_df[validationIndex, ]\n",
    "test_df <- remaining_df[-validationIndex, ]\n",
    "\n",
    "head(train_df)\n",
    "head(val_df)\n",
    "head(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394b62b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T15:48:12.039430Z",
     "iopub.status.busy": "2024-11-19T15:48:12.038246Z",
     "iopub.status.idle": "2024-11-19T15:50:58.066268Z",
     "shell.execute_reply": "2024-11-19T15:50:58.064856Z"
    },
    "papermill": {
     "duration": 166.035088,
     "end_time": "2024-11-19T15:50:58.068462",
     "exception": false,
     "start_time": "2024-11-19T15:48:12.033374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in training dataset:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benign  Early    Pre    Pro \n",
      "   323    631    617    515 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Class Weights (based on training dataset):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$`0`\n",
      "[1] 1.614551\n",
      "\n",
      "$`1`\n",
      "[1] 0.8264659\n",
      "\n",
      "$`2`\n",
      "[1] 0.8452188\n",
      "\n",
      "$`3`\n",
      "[1] 1.012621\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in training dataset: 2086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of class counts: 2086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum weight ratio: 1.95356 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of weight calculation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign: count=323, weight=1.6146, count*weight=521.5000\n",
      "Early: count=631, weight=0.8265, count*weight=521.5000\n",
      "Pre: count=617, weight=0.8452, count*weight=521.5000\n",
      "Pro: count=515, weight=1.0126, count*weight=521.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing features from 1000 training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in get_feature_params(train_df$image_filename, sample_size):\n",
      "“Inconsistent feature lengths detected in training data!”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_lengths\n",
      " 50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69 \n",
      "  2   7  25  49  53  38  46  26  28  19  15  51 218 206  34  22  12   1   1 106 \n",
      " 70  71 \n",
      " 39   2 \n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "  50.00   58.00   62.00   61.33   63.00   71.00 \n",
      "\n",
      "Feature extraction analysis summary:\n",
      "Modal feature length: 62 \n",
      "Success rate: 100.00% \n",
      "Length consistency: Inconsistent \n",
      "\n",
      "Feature length distribution:\n",
      "feature_lengths\n",
      " 50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69 \n",
      "  2   7  25  49  53  38  46  26  28  19  15  51 218 206  34  22  12   1   1 106 \n",
      " 70  71 \n",
      " 39   2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training image 100 of 2086\n",
      "Processing training image 200 of 2086\n",
      "Processing training image 300 of 2086\n",
      "Processing training image 400 of 2086\n",
      "Processing training image 500 of 2086\n",
      "Processing training image 600 of 2086\n",
      "Processing training image 700 of 2086\n",
      "Processing training image 800 of 2086\n",
      "Processing training image 900 of 2086\n",
      "Processing training image 1000 of 2086\n",
      "Processing training image 1100 of 2086\n",
      "Processing training image 1200 of 2086\n",
      "Processing training image 1300 of 2086\n",
      "Processing training image 1400 of 2086\n",
      "Processing training image 1500 of 2086\n",
      "Processing training image 1600 of 2086\n",
      "Processing training image 1700 of 2086\n",
      "Processing training image 1800 of 2086\n",
      "Processing training image 1900 of 2086\n",
      "Processing training image 2000 of 2086\n",
      "\n",
      "NA counts in training dataset:\n",
      "feature_51 feature_52 feature_53 feature_54 feature_55 feature_56 feature_57 \n",
      "         3         13         58        162        272        373        460 \n",
      "feature_58 feature_59 feature_60 feature_61 feature_62 \n",
      "       514        563        606        647        743 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting validation features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation image 100 of 652\n",
      "Processing validation image 200 of 652\n",
      "Processing validation image 300 of 652\n",
      "Processing validation image 400 of 652\n",
      "Processing validation image 500 of 652\n",
      "Processing validation image 600 of 652\n",
      "\n",
      "NA counts in validation dataset:\n",
      "feature_51 feature_52 feature_53 feature_54 feature_55 feature_56 feature_57 \n",
      "         1          4         15         59         86        120        142 \n",
      "feature_58 feature_59 feature_60 feature_61 feature_62 \n",
      "       158        171        186        199        220 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test image 100 of 518\n",
      "Processing test image 200 of 518\n",
      "Processing test image 300 of 518\n",
      "Processing test image 400 of 518\n",
      "Processing test image 500 of 518\n",
      "\n",
      "NA counts in test dataset:\n",
      "feature_52 feature_53 feature_54 feature_55 feature_56 feature_57 feature_58 \n",
      "         1          9         26         58         82        101        117 \n",
      "feature_59 feature_60 feature_61 feature_62 \n",
      "       121        130        138        169 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features removed due to near-zero variance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 64</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>feature_1</th><th scope=col>feature_2</th><th scope=col>feature_3</th><th scope=col>feature_4</th><th scope=col>feature_5</th><th scope=col>feature_6</th><th scope=col>feature_7</th><th scope=col>feature_8</th><th scope=col>feature_9</th><th scope=col>feature_10</th><th scope=col>⋯</th><th scope=col>feature_55</th><th scope=col>feature_56</th><th scope=col>feature_57</th><th scope=col>feature_58</th><th scope=col>feature_59</th><th scope=col>feature_60</th><th scope=col>feature_61</th><th scope=col>feature_62</th><th scope=col>class_numeric</th><th scope=col>class</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td> 0.03163656</td><td> 2.6236003</td><td> 3.1068061</td><td> 1.5554515</td><td> 0.59113280</td><td> 0.1519493</td><td>-0.1634713</td><td>-0.3242494</td><td>-0.50210272</td><td>-0.6200269</td><td>⋯</td><td>-0.08694183</td><td>-0.1310055</td><td>-0.1227530</td><td>-0.1248577</td><td>-0.1868748</td><td>-0.4557918</td><td>-0.4828846</td><td>-0.1807713</td><td>2</td><td>Pre   </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>-0.06400671</td><td>-0.2602499</td><td>-0.3348147</td><td>-0.4120166</td><td>-0.46574645</td><td>-0.5371078</td><td>-0.6452848</td><td>-0.6382741</td><td>-0.09371064</td><td> 0.3021439</td><td>⋯</td><td>-0.14496976</td><td>-0.1334710</td><td>-0.1227402</td><td>-0.1248448</td><td> 6.1239650</td><td>-0.3783744</td><td>-0.4827923</td><td>-0.1807713</td><td>1</td><td>Early </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>-0.25529325</td><td>-0.2883850</td><td>-0.3505538</td><td>-0.4146364</td><td>-0.46873620</td><td>-0.5939824</td><td>-0.7390973</td><td>-0.8789369</td><td>-0.94534665</td><td>-0.7833372</td><td>⋯</td><td>-0.14500556</td><td>-0.1335207</td><td>-0.1222629</td><td>-0.1247699</td><td>-0.1868666</td><td>-0.4557878</td><td> 2.2416456</td><td> 0.1394499</td><td>3</td><td>Pro   </td></tr>\n",
       "\t<tr><th scope=row>4</th><td> 0.22292310</td><td>-0.1195743</td><td>-0.1721771</td><td>-0.1395576</td><td>-0.02176747</td><td> 0.8902248</td><td> 2.9614860</td><td> 3.2212775</td><td> 1.79365058</td><td> 0.6953313</td><td>⋯</td><td>-0.14539556</td><td>-0.1335620</td><td>-0.1227530</td><td>-0.1248577</td><td>-0.1868748</td><td>-0.4557918</td><td>-0.4828846</td><td>-0.1807713</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>-0.25529325</td><td>-0.2602499</td><td>-0.3453074</td><td>-0.3989176</td><td>-0.29234052</td><td> 0.2481986</td><td> 2.2455966</td><td> 2.0868520</td><td> 0.49340883</td><td> 0.1228576</td><td>⋯</td><td>-0.14539556</td><td>-0.1335620</td><td>-0.1227530</td><td>-0.1248577</td><td>-0.1868748</td><td>-0.4557918</td><td>-0.4828846</td><td>-0.1807713</td><td>1</td><td>Early </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>-0.25529325</td><td>-0.2883850</td><td>-0.3505538</td><td>-0.4146364</td><td>-0.46873620</td><td>-0.5939824</td><td>-0.7390973</td><td>-0.8798315</td><td>-0.95696393</td><td>-0.9164706</td><td>⋯</td><td>-0.14535618</td><td>-0.1331336</td><td>-0.1226554</td><td>-0.1248451</td><td>-0.1868664</td><td> 2.4129871</td><td>-0.4106924</td><td>-0.1803437</td><td>3</td><td>Pro   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 64\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & feature\\_1 & feature\\_2 & feature\\_3 & feature\\_4 & feature\\_5 & feature\\_6 & feature\\_7 & feature\\_8 & feature\\_9 & feature\\_10 & ⋯ & feature\\_55 & feature\\_56 & feature\\_57 & feature\\_58 & feature\\_59 & feature\\_60 & feature\\_61 & feature\\_62 & class\\_numeric & class\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 &  0.03163656 &  2.6236003 &  3.1068061 &  1.5554515 &  0.59113280 &  0.1519493 & -0.1634713 & -0.3242494 & -0.50210272 & -0.6200269 & ⋯ & -0.08694183 & -0.1310055 & -0.1227530 & -0.1248577 & -0.1868748 & -0.4557918 & -0.4828846 & -0.1807713 & 2 & Pre   \\\\\n",
       "\t2 & -0.06400671 & -0.2602499 & -0.3348147 & -0.4120166 & -0.46574645 & -0.5371078 & -0.6452848 & -0.6382741 & -0.09371064 &  0.3021439 & ⋯ & -0.14496976 & -0.1334710 & -0.1227402 & -0.1248448 &  6.1239650 & -0.3783744 & -0.4827923 & -0.1807713 & 1 & Early \\\\\n",
       "\t3 & -0.25529325 & -0.2883850 & -0.3505538 & -0.4146364 & -0.46873620 & -0.5939824 & -0.7390973 & -0.8789369 & -0.94534665 & -0.7833372 & ⋯ & -0.14500556 & -0.1335207 & -0.1222629 & -0.1247699 & -0.1868666 & -0.4557878 &  2.2416456 &  0.1394499 & 3 & Pro   \\\\\n",
       "\t4 &  0.22292310 & -0.1195743 & -0.1721771 & -0.1395576 & -0.02176747 &  0.8902248 &  2.9614860 &  3.2212775 &  1.79365058 &  0.6953313 & ⋯ & -0.14539556 & -0.1335620 & -0.1227530 & -0.1248577 & -0.1868748 & -0.4557918 & -0.4828846 & -0.1807713 & 0 & Benign\\\\\n",
       "\t5 & -0.25529325 & -0.2602499 & -0.3453074 & -0.3989176 & -0.29234052 &  0.2481986 &  2.2455966 &  2.0868520 &  0.49340883 &  0.1228576 & ⋯ & -0.14539556 & -0.1335620 & -0.1227530 & -0.1248577 & -0.1868748 & -0.4557918 & -0.4828846 & -0.1807713 & 1 & Early \\\\\n",
       "\t6 & -0.25529325 & -0.2883850 & -0.3505538 & -0.4146364 & -0.46873620 & -0.5939824 & -0.7390973 & -0.8798315 & -0.95696393 & -0.9164706 & ⋯ & -0.14535618 & -0.1331336 & -0.1226554 & -0.1248451 & -0.1868664 &  2.4129871 & -0.4106924 & -0.1803437 & 3 & Pro   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 64\n",
       "\n",
       "| <!--/--> | feature_1 &lt;dbl&gt; | feature_2 &lt;dbl&gt; | feature_3 &lt;dbl&gt; | feature_4 &lt;dbl&gt; | feature_5 &lt;dbl&gt; | feature_6 &lt;dbl&gt; | feature_7 &lt;dbl&gt; | feature_8 &lt;dbl&gt; | feature_9 &lt;dbl&gt; | feature_10 &lt;dbl&gt; | ⋯ ⋯ | feature_55 &lt;dbl&gt; | feature_56 &lt;dbl&gt; | feature_57 &lt;dbl&gt; | feature_58 &lt;dbl&gt; | feature_59 &lt;dbl&gt; | feature_60 &lt;dbl&gt; | feature_61 &lt;dbl&gt; | feature_62 &lt;dbl&gt; | class_numeric &lt;dbl&gt; | class &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 |  0.03163656 |  2.6236003 |  3.1068061 |  1.5554515 |  0.59113280 |  0.1519493 | -0.1634713 | -0.3242494 | -0.50210272 | -0.6200269 | ⋯ | -0.08694183 | -0.1310055 | -0.1227530 | -0.1248577 | -0.1868748 | -0.4557918 | -0.4828846 | -0.1807713 | 2 | Pre    |\n",
       "| 2 | -0.06400671 | -0.2602499 | -0.3348147 | -0.4120166 | -0.46574645 | -0.5371078 | -0.6452848 | -0.6382741 | -0.09371064 |  0.3021439 | ⋯ | -0.14496976 | -0.1334710 | -0.1227402 | -0.1248448 |  6.1239650 | -0.3783744 | -0.4827923 | -0.1807713 | 1 | Early  |\n",
       "| 3 | -0.25529325 | -0.2883850 | -0.3505538 | -0.4146364 | -0.46873620 | -0.5939824 | -0.7390973 | -0.8789369 | -0.94534665 | -0.7833372 | ⋯ | -0.14500556 | -0.1335207 | -0.1222629 | -0.1247699 | -0.1868666 | -0.4557878 |  2.2416456 |  0.1394499 | 3 | Pro    |\n",
       "| 4 |  0.22292310 | -0.1195743 | -0.1721771 | -0.1395576 | -0.02176747 |  0.8902248 |  2.9614860 |  3.2212775 |  1.79365058 |  0.6953313 | ⋯ | -0.14539556 | -0.1335620 | -0.1227530 | -0.1248577 | -0.1868748 | -0.4557918 | -0.4828846 | -0.1807713 | 0 | Benign |\n",
       "| 5 | -0.25529325 | -0.2602499 | -0.3453074 | -0.3989176 | -0.29234052 |  0.2481986 |  2.2455966 |  2.0868520 |  0.49340883 |  0.1228576 | ⋯ | -0.14539556 | -0.1335620 | -0.1227530 | -0.1248577 | -0.1868748 | -0.4557918 | -0.4828846 | -0.1807713 | 1 | Early  |\n",
       "| 6 | -0.25529325 | -0.2883850 | -0.3505538 | -0.4146364 | -0.46873620 | -0.5939824 | -0.7390973 | -0.8798315 | -0.95696393 | -0.9164706 | ⋯ | -0.14535618 | -0.1331336 | -0.1226554 | -0.1248451 | -0.1868664 |  2.4129871 | -0.4106924 | -0.1803437 | 3 | Pro    |\n",
       "\n"
      ],
      "text/plain": [
       "  feature_1   feature_2  feature_3  feature_4  feature_5   feature_6 \n",
       "1  0.03163656  2.6236003  3.1068061  1.5554515  0.59113280  0.1519493\n",
       "2 -0.06400671 -0.2602499 -0.3348147 -0.4120166 -0.46574645 -0.5371078\n",
       "3 -0.25529325 -0.2883850 -0.3505538 -0.4146364 -0.46873620 -0.5939824\n",
       "4  0.22292310 -0.1195743 -0.1721771 -0.1395576 -0.02176747  0.8902248\n",
       "5 -0.25529325 -0.2602499 -0.3453074 -0.3989176 -0.29234052  0.2481986\n",
       "6 -0.25529325 -0.2883850 -0.3505538 -0.4146364 -0.46873620 -0.5939824\n",
       "  feature_7  feature_8  feature_9   feature_10 ⋯ feature_55  feature_56\n",
       "1 -0.1634713 -0.3242494 -0.50210272 -0.6200269 ⋯ -0.08694183 -0.1310055\n",
       "2 -0.6452848 -0.6382741 -0.09371064  0.3021439 ⋯ -0.14496976 -0.1334710\n",
       "3 -0.7390973 -0.8789369 -0.94534665 -0.7833372 ⋯ -0.14500556 -0.1335207\n",
       "4  2.9614860  3.2212775  1.79365058  0.6953313 ⋯ -0.14539556 -0.1335620\n",
       "5  2.2455966  2.0868520  0.49340883  0.1228576 ⋯ -0.14539556 -0.1335620\n",
       "6 -0.7390973 -0.8798315 -0.95696393 -0.9164706 ⋯ -0.14535618 -0.1331336\n",
       "  feature_57 feature_58 feature_59 feature_60 feature_61 feature_62\n",
       "1 -0.1227530 -0.1248577 -0.1868748 -0.4557918 -0.4828846 -0.1807713\n",
       "2 -0.1227402 -0.1248448  6.1239650 -0.3783744 -0.4827923 -0.1807713\n",
       "3 -0.1222629 -0.1247699 -0.1868666 -0.4557878  2.2416456  0.1394499\n",
       "4 -0.1227530 -0.1248577 -0.1868748 -0.4557918 -0.4828846 -0.1807713\n",
       "5 -0.1227530 -0.1248577 -0.1868748 -0.4557918 -0.4828846 -0.1807713\n",
       "6 -0.1226554 -0.1248451 -0.1868664  2.4129871 -0.4106924 -0.1803437\n",
       "  class_numeric class \n",
       "1 2             Pre   \n",
       "2 1             Early \n",
       "3 3             Pro   \n",
       "4 0             Benign\n",
       "5 1             Early \n",
       "6 3             Pro   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 64</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>feature_1</th><th scope=col>feature_2</th><th scope=col>feature_3</th><th scope=col>feature_4</th><th scope=col>feature_5</th><th scope=col>feature_6</th><th scope=col>feature_7</th><th scope=col>feature_8</th><th scope=col>feature_9</th><th scope=col>feature_10</th><th scope=col>⋯</th><th scope=col>feature_55</th><th scope=col>feature_56</th><th scope=col>feature_57</th><th scope=col>feature_58</th><th scope=col>feature_59</th><th scope=col>feature_60</th><th scope=col>feature_61</th><th scope=col>feature_62</th><th scope=col>class_numeric</th><th scope=col>class</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>-0.2552933</td><td>-0.2883850</td><td>-0.2875973</td><td>-0.32294344</td><td>-0.41492057</td><td>-0.4167963</td><td>-0.4458195</td><td>-0.07463998</td><td> 0.2360414</td><td> 0.5506596</td><td>⋯</td><td>-0.1450724</td><td>-0.1335044</td><td>-0.1222631</td><td>-0.1247701</td><td>-0.1868660</td><td>-0.4557879</td><td>-0.2232348</td><td>-0.1476746</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>-0.2552933</td><td>-0.2883850</td><td>-0.3505538</td><td>-0.41463635</td><td>-0.42986936</td><td>-0.5403891</td><td>-0.6644117</td><td>-0.67495502</td><td>-0.7737684</td><td>-0.9404346</td><td>⋯</td><td>-0.1420055</td><td>-0.1331637</td><td>-0.1226909</td><td>-0.1244115</td><td>-0.1868187</td><td>-0.4557888</td><td>-0.4828796</td><td> 1.4970126</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>-0.2552933</td><td>-0.2883850</td><td>-0.3505538</td><td>-0.35962060</td><td>-0.09800628</td><td>-0.2407039</td><td>-0.4385331</td><td>-0.60696107</td><td>-0.7496402</td><td>-0.6413282</td><td>⋯</td><td>-0.1446751</td><td>-0.1331992</td><td>-0.1226744</td><td>-0.1244115</td><td>-0.1868187</td><td>-0.4557866</td><td>-0.4828787</td><td> 1.4430539</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>-0.2552933</td><td>-0.2883850</td><td>-0.2036554</td><td>-0.19981295</td><td>-0.17723486</td><td>-0.3675779</td><td>-0.5797072</td><td>-0.81541621</td><td>-0.8988775</td><td>-0.9155831</td><td>⋯</td><td>-0.1428318</td><td>-0.1331798</td><td>-0.1226843</td><td>-0.1244115</td><td>-0.1868188</td><td>-0.4557872</td><td>-0.4828788</td><td> 1.7570324</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>-0.2552933</td><td>-0.2883850</td><td>-0.3505538</td><td>-0.41463635</td><td>-0.45378742</td><td>-0.5217954</td><td>-0.5605804</td><td>-0.61590764</td><td>-0.7317674</td><td>-0.8179519</td><td>⋯</td><td>-0.1450164</td><td>-0.1335027</td><td>-0.1222629</td><td>-0.1247687</td><td>-0.1868646</td><td>-0.4557864</td><td> 2.1455912</td><td> 0.1394499</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>-0.2552933</td><td>-0.1758445</td><td> 0.1950690</td><td> 0.07264601</td><td>-0.24300952</td><td>-0.3850778</td><td>-0.4667679</td><td>-0.53986177</td><td>-0.4002281</td><td>-0.1913373</td><td>⋯</td><td>-0.1459526</td><td>-0.1330058</td><td>-0.1223547</td><td>-0.1247846</td><td>-0.1865923</td><td>-0.4557667</td><td>-0.4828798</td><td>-0.1807476</td><td>0</td><td>Benign</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 64\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & feature\\_1 & feature\\_2 & feature\\_3 & feature\\_4 & feature\\_5 & feature\\_6 & feature\\_7 & feature\\_8 & feature\\_9 & feature\\_10 & ⋯ & feature\\_55 & feature\\_56 & feature\\_57 & feature\\_58 & feature\\_59 & feature\\_60 & feature\\_61 & feature\\_62 & class\\_numeric & class\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & -0.2552933 & -0.2883850 & -0.2875973 & -0.32294344 & -0.41492057 & -0.4167963 & -0.4458195 & -0.07463998 &  0.2360414 &  0.5506596 & ⋯ & -0.1450724 & -0.1335044 & -0.1222631 & -0.1247701 & -0.1868660 & -0.4557879 & -0.2232348 & -0.1476746 & 0 & Benign\\\\\n",
       "\t2 & -0.2552933 & -0.2883850 & -0.3505538 & -0.41463635 & -0.42986936 & -0.5403891 & -0.6644117 & -0.67495502 & -0.7737684 & -0.9404346 & ⋯ & -0.1420055 & -0.1331637 & -0.1226909 & -0.1244115 & -0.1868187 & -0.4557888 & -0.4828796 &  1.4970126 & 0 & Benign\\\\\n",
       "\t3 & -0.2552933 & -0.2883850 & -0.3505538 & -0.35962060 & -0.09800628 & -0.2407039 & -0.4385331 & -0.60696107 & -0.7496402 & -0.6413282 & ⋯ & -0.1446751 & -0.1331992 & -0.1226744 & -0.1244115 & -0.1868187 & -0.4557866 & -0.4828787 &  1.4430539 & 0 & Benign\\\\\n",
       "\t4 & -0.2552933 & -0.2883850 & -0.2036554 & -0.19981295 & -0.17723486 & -0.3675779 & -0.5797072 & -0.81541621 & -0.8988775 & -0.9155831 & ⋯ & -0.1428318 & -0.1331798 & -0.1226843 & -0.1244115 & -0.1868188 & -0.4557872 & -0.4828788 &  1.7570324 & 0 & Benign\\\\\n",
       "\t5 & -0.2552933 & -0.2883850 & -0.3505538 & -0.41463635 & -0.45378742 & -0.5217954 & -0.5605804 & -0.61590764 & -0.7317674 & -0.8179519 & ⋯ & -0.1450164 & -0.1335027 & -0.1222629 & -0.1247687 & -0.1868646 & -0.4557864 &  2.1455912 &  0.1394499 & 0 & Benign\\\\\n",
       "\t6 & -0.2552933 & -0.1758445 &  0.1950690 &  0.07264601 & -0.24300952 & -0.3850778 & -0.4667679 & -0.53986177 & -0.4002281 & -0.1913373 & ⋯ & -0.1459526 & -0.1330058 & -0.1223547 & -0.1247846 & -0.1865923 & -0.4557667 & -0.4828798 & -0.1807476 & 0 & Benign\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 64\n",
       "\n",
       "| <!--/--> | feature_1 &lt;dbl&gt; | feature_2 &lt;dbl&gt; | feature_3 &lt;dbl&gt; | feature_4 &lt;dbl&gt; | feature_5 &lt;dbl&gt; | feature_6 &lt;dbl&gt; | feature_7 &lt;dbl&gt; | feature_8 &lt;dbl&gt; | feature_9 &lt;dbl&gt; | feature_10 &lt;dbl&gt; | ⋯ ⋯ | feature_55 &lt;dbl&gt; | feature_56 &lt;dbl&gt; | feature_57 &lt;dbl&gt; | feature_58 &lt;dbl&gt; | feature_59 &lt;dbl&gt; | feature_60 &lt;dbl&gt; | feature_61 &lt;dbl&gt; | feature_62 &lt;dbl&gt; | class_numeric &lt;dbl&gt; | class &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | -0.2552933 | -0.2883850 | -0.2875973 | -0.32294344 | -0.41492057 | -0.4167963 | -0.4458195 | -0.07463998 |  0.2360414 |  0.5506596 | ⋯ | -0.1450724 | -0.1335044 | -0.1222631 | -0.1247701 | -0.1868660 | -0.4557879 | -0.2232348 | -0.1476746 | 0 | Benign |\n",
       "| 2 | -0.2552933 | -0.2883850 | -0.3505538 | -0.41463635 | -0.42986936 | -0.5403891 | -0.6644117 | -0.67495502 | -0.7737684 | -0.9404346 | ⋯ | -0.1420055 | -0.1331637 | -0.1226909 | -0.1244115 | -0.1868187 | -0.4557888 | -0.4828796 |  1.4970126 | 0 | Benign |\n",
       "| 3 | -0.2552933 | -0.2883850 | -0.3505538 | -0.35962060 | -0.09800628 | -0.2407039 | -0.4385331 | -0.60696107 | -0.7496402 | -0.6413282 | ⋯ | -0.1446751 | -0.1331992 | -0.1226744 | -0.1244115 | -0.1868187 | -0.4557866 | -0.4828787 |  1.4430539 | 0 | Benign |\n",
       "| 4 | -0.2552933 | -0.2883850 | -0.2036554 | -0.19981295 | -0.17723486 | -0.3675779 | -0.5797072 | -0.81541621 | -0.8988775 | -0.9155831 | ⋯ | -0.1428318 | -0.1331798 | -0.1226843 | -0.1244115 | -0.1868188 | -0.4557872 | -0.4828788 |  1.7570324 | 0 | Benign |\n",
       "| 5 | -0.2552933 | -0.2883850 | -0.3505538 | -0.41463635 | -0.45378742 | -0.5217954 | -0.5605804 | -0.61590764 | -0.7317674 | -0.8179519 | ⋯ | -0.1450164 | -0.1335027 | -0.1222629 | -0.1247687 | -0.1868646 | -0.4557864 |  2.1455912 |  0.1394499 | 0 | Benign |\n",
       "| 6 | -0.2552933 | -0.1758445 |  0.1950690 |  0.07264601 | -0.24300952 | -0.3850778 | -0.4667679 | -0.53986177 | -0.4002281 | -0.1913373 | ⋯ | -0.1459526 | -0.1330058 | -0.1223547 | -0.1247846 | -0.1865923 | -0.4557667 | -0.4828798 | -0.1807476 | 0 | Benign |\n",
       "\n"
      ],
      "text/plain": [
       "  feature_1  feature_2  feature_3  feature_4   feature_5   feature_6 \n",
       "1 -0.2552933 -0.2883850 -0.2875973 -0.32294344 -0.41492057 -0.4167963\n",
       "2 -0.2552933 -0.2883850 -0.3505538 -0.41463635 -0.42986936 -0.5403891\n",
       "3 -0.2552933 -0.2883850 -0.3505538 -0.35962060 -0.09800628 -0.2407039\n",
       "4 -0.2552933 -0.2883850 -0.2036554 -0.19981295 -0.17723486 -0.3675779\n",
       "5 -0.2552933 -0.2883850 -0.3505538 -0.41463635 -0.45378742 -0.5217954\n",
       "6 -0.2552933 -0.1758445  0.1950690  0.07264601 -0.24300952 -0.3850778\n",
       "  feature_7  feature_8   feature_9  feature_10 ⋯ feature_55 feature_56\n",
       "1 -0.4458195 -0.07463998  0.2360414  0.5506596 ⋯ -0.1450724 -0.1335044\n",
       "2 -0.6644117 -0.67495502 -0.7737684 -0.9404346 ⋯ -0.1420055 -0.1331637\n",
       "3 -0.4385331 -0.60696107 -0.7496402 -0.6413282 ⋯ -0.1446751 -0.1331992\n",
       "4 -0.5797072 -0.81541621 -0.8988775 -0.9155831 ⋯ -0.1428318 -0.1331798\n",
       "5 -0.5605804 -0.61590764 -0.7317674 -0.8179519 ⋯ -0.1450164 -0.1335027\n",
       "6 -0.4667679 -0.53986177 -0.4002281 -0.1913373 ⋯ -0.1459526 -0.1330058\n",
       "  feature_57 feature_58 feature_59 feature_60 feature_61 feature_62\n",
       "1 -0.1222631 -0.1247701 -0.1868660 -0.4557879 -0.2232348 -0.1476746\n",
       "2 -0.1226909 -0.1244115 -0.1868187 -0.4557888 -0.4828796  1.4970126\n",
       "3 -0.1226744 -0.1244115 -0.1868187 -0.4557866 -0.4828787  1.4430539\n",
       "4 -0.1226843 -0.1244115 -0.1868188 -0.4557872 -0.4828788  1.7570324\n",
       "5 -0.1222629 -0.1247687 -0.1868646 -0.4557864  2.1455912  0.1394499\n",
       "6 -0.1223547 -0.1247846 -0.1865923 -0.4557667 -0.4828798 -0.1807476\n",
       "  class_numeric class \n",
       "1 0             Benign\n",
       "2 0             Benign\n",
       "3 0             Benign\n",
       "4 0             Benign\n",
       "5 0             Benign\n",
       "6 0             Benign"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 64</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>feature_1</th><th scope=col>feature_2</th><th scope=col>feature_3</th><th scope=col>feature_4</th><th scope=col>feature_5</th><th scope=col>feature_6</th><th scope=col>feature_7</th><th scope=col>feature_8</th><th scope=col>feature_9</th><th scope=col>feature_10</th><th scope=col>⋯</th><th scope=col>feature_55</th><th scope=col>feature_56</th><th scope=col>feature_57</th><th scope=col>feature_58</th><th scope=col>feature_59</th><th scope=col>feature_60</th><th scope=col>feature_61</th><th scope=col>feature_62</th><th scope=col>class_numeric</th><th scope=col>class</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>-0.2552933</td><td>-0.28838501</td><td>-0.35055381</td><td>-0.41201656</td><td>-0.4522925</td><td>-0.5480452</td><td>-0.6206932</td><td>-0.6168023</td><td>-0.5986155</td><td>-0.7682487</td><td>⋯</td><td>-0.1450000</td><td>-0.1335072</td><td>-0.1222629</td><td>-0.1247696</td><td>-0.1868679</td><td>-0.4557870</td><td>-0.1544160</td><td>-0.1392661</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>-0.2552933</td><td> 0.04923647</td><td>-0.05675691</td><td> 0.03858864</td><td> 0.3220546</td><td>-0.2385164</td><td>-0.5642236</td><td>-0.7411597</td><td>-0.7281929</td><td>-0.6581918</td><td>⋯</td><td>-0.1461094</td><td>-0.1325405</td><td>-0.1223414</td><td>-0.1247806</td><td>-0.1865922</td><td>-0.4557668</td><td>-0.4828790</td><td>-0.1807418</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>-0.2552933</td><td>-0.28838501</td><td>-0.33481469</td><td>-0.23387032</td><td>-0.1607912</td><td>-0.3708591</td><td>-0.4549275</td><td>-0.5872786</td><td>-0.7978966</td><td>-0.8330403</td><td>⋯</td><td>-0.1435910</td><td>-0.1332172</td><td>-0.1226898</td><td>-0.1244115</td><td>-0.1868191</td><td>-0.4557877</td><td>-0.4828799</td><td> 1.4547207</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>-0.2552933</td><td>-0.28838501</td><td>-0.35055381</td><td>-0.37795919</td><td>-0.4223950</td><td>-0.5272642</td><td>-0.6589469</td><td>-0.6883749</td><td>-0.7693002</td><td>-0.9892502</td><td>⋯</td><td>-0.1453479</td><td>-0.1331336</td><td>-0.1226558</td><td>-0.1248443</td><td>-0.1868644</td><td>-0.1450028</td><td>-0.4740961</td><td>-0.1775628</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>-0.2552933</td><td>-0.23211477</td><td>-0.30858282</td><td>-0.37795919</td><td>-0.4149206</td><td>-0.4507022</td><td>-0.6197824</td><td>-0.7313184</td><td>-0.7594702</td><td>-0.7726865</td><td>⋯</td><td>-0.1437448</td><td>-0.1331838</td><td>-0.1226887</td><td>-0.1244115</td><td>-0.1868190</td><td>-0.4557875</td><td>-0.4828799</td><td> 2.1722095</td><td>0</td><td>Benign</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>-0.2552933</td><td>-0.28838501</td><td>-0.35055381</td><td>-0.08716165</td><td>-0.1084704</td><td>-0.4539835</td><td>-0.6270688</td><td>-0.7921551</td><td>-0.8175566</td><td>-0.9280088</td><td>⋯</td><td>-0.1453869</td><td>-0.1345130</td><td>-0.1199973</td><td>-0.1244625</td><td>-0.1868346</td><td>-0.4556643</td><td>-0.4828608</td><td>-0.1807514</td><td>0</td><td>Benign</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 64\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & feature\\_1 & feature\\_2 & feature\\_3 & feature\\_4 & feature\\_5 & feature\\_6 & feature\\_7 & feature\\_8 & feature\\_9 & feature\\_10 & ⋯ & feature\\_55 & feature\\_56 & feature\\_57 & feature\\_58 & feature\\_59 & feature\\_60 & feature\\_61 & feature\\_62 & class\\_numeric & class\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & -0.2552933 & -0.28838501 & -0.35055381 & -0.41201656 & -0.4522925 & -0.5480452 & -0.6206932 & -0.6168023 & -0.5986155 & -0.7682487 & ⋯ & -0.1450000 & -0.1335072 & -0.1222629 & -0.1247696 & -0.1868679 & -0.4557870 & -0.1544160 & -0.1392661 & 0 & Benign\\\\\n",
       "\t2 & -0.2552933 &  0.04923647 & -0.05675691 &  0.03858864 &  0.3220546 & -0.2385164 & -0.5642236 & -0.7411597 & -0.7281929 & -0.6581918 & ⋯ & -0.1461094 & -0.1325405 & -0.1223414 & -0.1247806 & -0.1865922 & -0.4557668 & -0.4828790 & -0.1807418 & 0 & Benign\\\\\n",
       "\t3 & -0.2552933 & -0.28838501 & -0.33481469 & -0.23387032 & -0.1607912 & -0.3708591 & -0.4549275 & -0.5872786 & -0.7978966 & -0.8330403 & ⋯ & -0.1435910 & -0.1332172 & -0.1226898 & -0.1244115 & -0.1868191 & -0.4557877 & -0.4828799 &  1.4547207 & 0 & Benign\\\\\n",
       "\t4 & -0.2552933 & -0.28838501 & -0.35055381 & -0.37795919 & -0.4223950 & -0.5272642 & -0.6589469 & -0.6883749 & -0.7693002 & -0.9892502 & ⋯ & -0.1453479 & -0.1331336 & -0.1226558 & -0.1248443 & -0.1868644 & -0.1450028 & -0.4740961 & -0.1775628 & 0 & Benign\\\\\n",
       "\t5 & -0.2552933 & -0.23211477 & -0.30858282 & -0.37795919 & -0.4149206 & -0.4507022 & -0.6197824 & -0.7313184 & -0.7594702 & -0.7726865 & ⋯ & -0.1437448 & -0.1331838 & -0.1226887 & -0.1244115 & -0.1868190 & -0.4557875 & -0.4828799 &  2.1722095 & 0 & Benign\\\\\n",
       "\t6 & -0.2552933 & -0.28838501 & -0.35055381 & -0.08716165 & -0.1084704 & -0.4539835 & -0.6270688 & -0.7921551 & -0.8175566 & -0.9280088 & ⋯ & -0.1453869 & -0.1345130 & -0.1199973 & -0.1244625 & -0.1868346 & -0.4556643 & -0.4828608 & -0.1807514 & 0 & Benign\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 64\n",
       "\n",
       "| <!--/--> | feature_1 &lt;dbl&gt; | feature_2 &lt;dbl&gt; | feature_3 &lt;dbl&gt; | feature_4 &lt;dbl&gt; | feature_5 &lt;dbl&gt; | feature_6 &lt;dbl&gt; | feature_7 &lt;dbl&gt; | feature_8 &lt;dbl&gt; | feature_9 &lt;dbl&gt; | feature_10 &lt;dbl&gt; | ⋯ ⋯ | feature_55 &lt;dbl&gt; | feature_56 &lt;dbl&gt; | feature_57 &lt;dbl&gt; | feature_58 &lt;dbl&gt; | feature_59 &lt;dbl&gt; | feature_60 &lt;dbl&gt; | feature_61 &lt;dbl&gt; | feature_62 &lt;dbl&gt; | class_numeric &lt;dbl&gt; | class &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | -0.2552933 | -0.28838501 | -0.35055381 | -0.41201656 | -0.4522925 | -0.5480452 | -0.6206932 | -0.6168023 | -0.5986155 | -0.7682487 | ⋯ | -0.1450000 | -0.1335072 | -0.1222629 | -0.1247696 | -0.1868679 | -0.4557870 | -0.1544160 | -0.1392661 | 0 | Benign |\n",
       "| 2 | -0.2552933 |  0.04923647 | -0.05675691 |  0.03858864 |  0.3220546 | -0.2385164 | -0.5642236 | -0.7411597 | -0.7281929 | -0.6581918 | ⋯ | -0.1461094 | -0.1325405 | -0.1223414 | -0.1247806 | -0.1865922 | -0.4557668 | -0.4828790 | -0.1807418 | 0 | Benign |\n",
       "| 3 | -0.2552933 | -0.28838501 | -0.33481469 | -0.23387032 | -0.1607912 | -0.3708591 | -0.4549275 | -0.5872786 | -0.7978966 | -0.8330403 | ⋯ | -0.1435910 | -0.1332172 | -0.1226898 | -0.1244115 | -0.1868191 | -0.4557877 | -0.4828799 |  1.4547207 | 0 | Benign |\n",
       "| 4 | -0.2552933 | -0.28838501 | -0.35055381 | -0.37795919 | -0.4223950 | -0.5272642 | -0.6589469 | -0.6883749 | -0.7693002 | -0.9892502 | ⋯ | -0.1453479 | -0.1331336 | -0.1226558 | -0.1248443 | -0.1868644 | -0.1450028 | -0.4740961 | -0.1775628 | 0 | Benign |\n",
       "| 5 | -0.2552933 | -0.23211477 | -0.30858282 | -0.37795919 | -0.4149206 | -0.4507022 | -0.6197824 | -0.7313184 | -0.7594702 | -0.7726865 | ⋯ | -0.1437448 | -0.1331838 | -0.1226887 | -0.1244115 | -0.1868190 | -0.4557875 | -0.4828799 |  2.1722095 | 0 | Benign |\n",
       "| 6 | -0.2552933 | -0.28838501 | -0.35055381 | -0.08716165 | -0.1084704 | -0.4539835 | -0.6270688 | -0.7921551 | -0.8175566 | -0.9280088 | ⋯ | -0.1453869 | -0.1345130 | -0.1199973 | -0.1244625 | -0.1868346 | -0.4556643 | -0.4828608 | -0.1807514 | 0 | Benign |\n",
       "\n"
      ],
      "text/plain": [
       "  feature_1  feature_2   feature_3   feature_4   feature_5  feature_6 \n",
       "1 -0.2552933 -0.28838501 -0.35055381 -0.41201656 -0.4522925 -0.5480452\n",
       "2 -0.2552933  0.04923647 -0.05675691  0.03858864  0.3220546 -0.2385164\n",
       "3 -0.2552933 -0.28838501 -0.33481469 -0.23387032 -0.1607912 -0.3708591\n",
       "4 -0.2552933 -0.28838501 -0.35055381 -0.37795919 -0.4223950 -0.5272642\n",
       "5 -0.2552933 -0.23211477 -0.30858282 -0.37795919 -0.4149206 -0.4507022\n",
       "6 -0.2552933 -0.28838501 -0.35055381 -0.08716165 -0.1084704 -0.4539835\n",
       "  feature_7  feature_8  feature_9  feature_10 ⋯ feature_55 feature_56\n",
       "1 -0.6206932 -0.6168023 -0.5986155 -0.7682487 ⋯ -0.1450000 -0.1335072\n",
       "2 -0.5642236 -0.7411597 -0.7281929 -0.6581918 ⋯ -0.1461094 -0.1325405\n",
       "3 -0.4549275 -0.5872786 -0.7978966 -0.8330403 ⋯ -0.1435910 -0.1332172\n",
       "4 -0.6589469 -0.6883749 -0.7693002 -0.9892502 ⋯ -0.1453479 -0.1331336\n",
       "5 -0.6197824 -0.7313184 -0.7594702 -0.7726865 ⋯ -0.1437448 -0.1331838\n",
       "6 -0.6270688 -0.7921551 -0.8175566 -0.9280088 ⋯ -0.1453869 -0.1345130\n",
       "  feature_57 feature_58 feature_59 feature_60 feature_61 feature_62\n",
       "1 -0.1222629 -0.1247696 -0.1868679 -0.4557870 -0.1544160 -0.1392661\n",
       "2 -0.1223414 -0.1247806 -0.1865922 -0.4557668 -0.4828790 -0.1807418\n",
       "3 -0.1226898 -0.1244115 -0.1868191 -0.4557877 -0.4828799  1.4547207\n",
       "4 -0.1226558 -0.1248443 -0.1868644 -0.1450028 -0.4740961 -0.1775628\n",
       "5 -0.1226887 -0.1244115 -0.1868190 -0.4557875 -0.4828799  2.1722095\n",
       "6 -0.1199973 -0.1244625 -0.1868346 -0.4556643 -0.4828608 -0.1807514\n",
       "  class_numeric class \n",
       "1 0             Benign\n",
       "2 0             Benign\n",
       "3 0             Benign\n",
       "4 0             Benign\n",
       "5 0             Benign\n",
       "6 0             Benign"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate class weights using only the training dataset\n",
    "n_samples <- nrow(train_df)  \n",
    "n_classes <- length(unique(train_df$class))\n",
    "\n",
    "# Get class frequencies from training dataset\n",
    "class_counts <- table(train_df$class_numeric)\n",
    "\n",
    "# Calculate balanced weights\n",
    "class_weights <- n_samples / (n_classes * class_counts)\n",
    "\n",
    "# Convert to a named list with numeric indices as names\n",
    "class_weights <- as.list(class_weights)\n",
    "names(class_weights) <- as.character(0:(n_classes-1))\n",
    "\n",
    "# Print diagnostics\n",
    "cat(\"\\nClass distribution in training dataset:\\n\")\n",
    "print(table(train_df$class))\n",
    "\n",
    "cat(\"\\nFinal Class Weights (based on training dataset):\\n\")\n",
    "print(class_weights)\n",
    "\n",
    "# Validation steps\n",
    "cat(\"\\nValidation:\\n\")\n",
    "cat(\"Total samples in training dataset:\", n_samples, \"\\n\")\n",
    "cat(\"Sum of class counts:\", sum(class_counts), \"\\n\")\n",
    "cat(\"Number of classes:\", n_classes, \"\\n\")\n",
    "cat(\"Maximum weight ratio:\", max(unlist(class_weights)) / min(unlist(class_weights)), \"\\n\\n\")\n",
    "\n",
    "# Additional validation checks\n",
    "cat(\"Verification of weight calculation:\\n\")\n",
    "for(i in 0:(n_classes-1)) {\n",
    "  class_name <- classes[i+1]\n",
    "  weight <- class_weights[[as.character(i)]]\n",
    "  count <- class_counts[as.character(i)]\n",
    "  cat(sprintf(\"%s: count=%d, weight=%.4f, count*weight=%.4f\\n\", \n",
    "              class_name, count, weight, count*weight))\n",
    "}\n",
    "\n",
    "# Features extraction function \n",
    "extract_features <- function(image_path, bins = 16, feature_params = NULL) {\n",
    "  # Load and resize the image using imager\n",
    "  image <- load.image(image_path)\n",
    "  image <- resize(image, img_width, img_height)\n",
    "  \n",
    "  # Convert image to array and split into color channels\n",
    "  red_channel <- as.vector(image[,,1,1])\n",
    "  green_channel <- as.vector(image[,,1,2])\n",
    "  blue_channel <- as.vector(image[,,1,3])\n",
    "  \n",
    "  # Color histogram features for each channel\n",
    "  red_hist <- hist(red_channel, breaks = bins, plot = FALSE)$counts / length(red_channel)\n",
    "  green_hist <- hist(green_channel, breaks = bins, plot = FALSE)$counts / length(green_channel)\n",
    "  blue_hist <- hist(blue_channel, breaks = bins, plot = FALSE)$counts / length(blue_channel)\n",
    "  \n",
    "  # Convert to grayscale for texture and edge features\n",
    "  gray_image <- 0.299 * red_channel + 0.587 * green_channel + 0.114 * blue_channel\n",
    "  \n",
    "  # Texture features: contrast, skewness, and kurtosis\n",
    "  contrast <- var(gray_image)\n",
    "  skewness <- e1071::skewness(gray_image)\n",
    "  kurtosis <- e1071::kurtosis(gray_image)\n",
    "  \n",
    "  # Statistical descriptors\n",
    "  mean_intensity <- mean(gray_image)\n",
    "  sd_intensity <- sd(gray_image)\n",
    "  max_intensity <- max(gray_image)\n",
    "  min_intensity <- min(gray_image)\n",
    "  \n",
    "  # Edge detection using gradient magnitude approximation (Sobel-like)\n",
    "  gray_img_cimg <- as.cimg(gray_image, x = img_width, y = img_height)\n",
    "  grad_x <- imgradient(gray_img_cimg, \"x\")\n",
    "  grad_y <- imgradient(gray_img_cimg, \"y\")\n",
    "  edge_magnitude <- sqrt(grad_x^2 + grad_y^2)  # Approximate edge strength\n",
    "  edge_intensity_mean <- mean(edge_magnitude)\n",
    "  edge_intensity_sd <- sd(edge_magnitude)\n",
    "  \n",
    "  # Shape features using segmentation (using EBImage)\n",
    "  gray_img_matrix <- matrix(gray_image, nrow = img_height, ncol = img_width)\n",
    "  binary_image <- gray_img_matrix > 0.5  # Apply threshold to get binary mask\n",
    "  binary_image <- as.Image(binary_image)  # Convert to EBImage object\n",
    "  \n",
    "  # Label the binary image\n",
    "  labeled_image <- bwlabel(binary_image)\n",
    "  \n",
    "  # Compute shape features\n",
    "  shape_features <- computeFeatures.shape(labeled_image)\n",
    "  \n",
    "  # Shape descriptors - Mean values of area, perimeter, and circularity\n",
    "  area_mean <- mean(shape_features[,\"s.area\"], na.rm = TRUE)\n",
    "  perimeter_mean <- mean(shape_features[,\"s.perimeter\"], na.rm = TRUE)\n",
    "  circularity_mean <- mean((4 * pi * shape_features[,\"s.area\"]) / (shape_features[,\"s.perimeter\"]^2), na.rm = TRUE)\n",
    "  \n",
    "  # Combine all features into a single vector\n",
    "  feature_vector <- c(red_hist, green_hist, blue_hist, contrast, skewness, kurtosis, \n",
    "                      mean_intensity, sd_intensity, max_intensity, min_intensity,\n",
    "                      edge_intensity_mean, edge_intensity_sd,\n",
    "                      area_mean, perimeter_mean, circularity_mean)\n",
    "  \n",
    "  return(feature_vector)\n",
    "}\n",
    "\n",
    "# Function to analyze feature lengths across training data\n",
    "get_feature_params <- function(train_image_paths, sample_size = NULL) {\n",
    "  # If sample_size is provided, randomly sample images for efficiency\n",
    "  if (!is.null(sample_size) && sample_size < length(train_image_paths)) {\n",
    "    set.seed(123)  # for reproducibility\n",
    "    image_paths <- sample(train_image_paths, sample_size)\n",
    "  } else {\n",
    "    image_paths <- train_image_paths\n",
    "  }\n",
    "  \n",
    "  cat(\"Analyzing features from\", length(image_paths), \"training images...\\n\")\n",
    "  \n",
    "  # Extract features from all training images and analyze\n",
    "  feature_lengths <- sapply(image_paths, function(path) {\n",
    "    tryCatch({\n",
    "      features <- extract_features(path)\n",
    "      length(features)\n",
    "    }, error = function(e) {\n",
    "      warning(sprintf(\"Error processing image %s: %s\", path, e$message))\n",
    "      return(NA)\n",
    "    })\n",
    "  })\n",
    "  \n",
    "  # Analyze feature length distribution\n",
    "  length_summary <- summary(feature_lengths)\n",
    "  length_table <- table(feature_lengths)\n",
    "  \n",
    "  # Check for inconsistencies\n",
    "  if (length(unique(feature_lengths[!is.na(feature_lengths)])) > 1) {\n",
    "    warning(\"Inconsistent feature lengths detected in training data!\")\n",
    "    print(length_table)\n",
    "    print(length_summary)\n",
    "  }\n",
    "  \n",
    "  # Get most common feature length (mode)\n",
    "  modal_length <- as.numeric(names(length_table)[which.max(length_table)])\n",
    "  \n",
    "  # Get feature names from a successful extraction\n",
    "  sample_features <- NULL\n",
    "  for (path in image_paths) {\n",
    "    tryCatch({\n",
    "      sample_features <- extract_features(path)\n",
    "      if (length(sample_features) == modal_length) break\n",
    "    }, error = function(e) {\n",
    "      next\n",
    "    })\n",
    "  }\n",
    "  \n",
    "  params <- list(\n",
    "    feature_length = modal_length,\n",
    "    feature_names = paste0(\"feature_\", seq_len(modal_length)),\n",
    "    length_distribution = length_table,\n",
    "    length_summary = length_summary,\n",
    "    sample_size = length(image_paths),\n",
    "    success_rate = sum(!is.na(feature_lengths)) / length(feature_lengths),\n",
    "    consistent_length = length(unique(feature_lengths[!is.na(feature_lengths)])) == 1\n",
    "  )\n",
    "  \n",
    "  # Print summary\n",
    "  cat(\"\\nFeature extraction analysis summary:\\n\")\n",
    "  cat(\"Modal feature length:\", params$feature_length, \"\\n\")\n",
    "  cat(\"Success rate:\", sprintf(\"%.2f%%\", params$success_rate * 100), \"\\n\")\n",
    "  cat(\"Length consistency:\", ifelse(params$consistent_length, \"Consistent\", \"Inconsistent\"), \"\\n\")\n",
    "  \n",
    "  if (!params$consistent_length) {\n",
    "    cat(\"\\nFeature length distribution:\\n\")\n",
    "    print(params$length_distribution)\n",
    "  }\n",
    "  \n",
    "  return(params)\n",
    "}\n",
    "\n",
    "# Modified feature extraction pipeline\n",
    "set.seed(123)  # for reproducibility\n",
    "\n",
    "# Step 1: Get feature parameters from training data\n",
    "# Option to sample if training set is very large\n",
    "sample_size <- if(nrow(train_df) > 1000) 1000 else NULL\n",
    "feature_params <- get_feature_params(train_df$image_filename, sample_size)\n",
    "\n",
    "# Enhanced feature extraction function with validation\n",
    "extract_and_validate_features <- function(image_paths, feature_params, dataset_name) {\n",
    "  features_list <- lapply(seq_along(image_paths), function(i) {\n",
    "    path <- image_paths[i]\n",
    "    \n",
    "    if (i %% 100 == 0) {\n",
    "      cat(sprintf(\"Processing %s image %d of %d\\n\", \n",
    "                 dataset_name, i, length(image_paths)))\n",
    "    }\n",
    "    \n",
    "    tryCatch({\n",
    "      features <- extract_features(path)\n",
    "      \n",
    "      # Validate feature length\n",
    "      if (length(features) != feature_params$feature_length) {\n",
    "        \n",
    "        # Handle inconsistent length\n",
    "        if (length(features) < feature_params$feature_length) {\n",
    "          # Pad with NA if shorter\n",
    "          features <- c(features, rep(NA, feature_params$feature_length - length(features)))\n",
    "        } else {\n",
    "          # Truncate if longer\n",
    "          features <- features[1:feature_params$feature_length]\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      return(features)\n",
    "      \n",
    "    }, error = function(e) {\n",
    "      warning(sprintf(\"Error processing %s image %s: %s\", \n",
    "                     dataset_name, path, e$message))\n",
    "      return(rep(NA, feature_params$feature_length))\n",
    "    })\n",
    "  })\n",
    "  \n",
    "  # Convert to matrix/data frame\n",
    "  features_matrix <- do.call(rbind, features_list)\n",
    "  colnames(features_matrix) <- feature_params$feature_names\n",
    "  \n",
    "  # Report statistics about NA values\n",
    "  na_counts <- colSums(is.na(features_matrix))\n",
    "  if (any(na_counts > 0)) {\n",
    "    cat(\"\\nNA counts in\", dataset_name, \"dataset:\\n\")\n",
    "    print(na_counts[na_counts > 0])\n",
    "  }\n",
    "  \n",
    "  return(features_matrix)\n",
    "}\n",
    "\n",
    "# Extract features for each dataset\n",
    "cat(\"\\nExtracting training features...\\n\")\n",
    "train_features <- extract_and_validate_features(\n",
    "  train_df$image_filename, \n",
    "  feature_params,\n",
    "  \"training\"\n",
    ")\n",
    "\n",
    "cat(\"\\nExtracting validation features...\\n\")\n",
    "val_features <- extract_and_validate_features(\n",
    "  val_df$image_filename, \n",
    "  feature_params,\n",
    "  \"validation\"\n",
    ")\n",
    "\n",
    "cat(\"\\nExtracting test features...\\n\")\n",
    "test_features <- extract_and_validate_features(\n",
    "  test_df$image_filename, \n",
    "  feature_params,\n",
    "  \"test\"\n",
    ")\n",
    "\n",
    "# Ensure that class labels are factors with consistent levels\n",
    "train_df$class <- factor(train_df$class, levels = c(\"Benign\", \"Early\", \"Pre\", \"Pro\"))\n",
    "test_df$class <- factor(test_df$class, levels = c(\"Benign\", \"Early\", \"Pre\", \"Pro\"))\n",
    "val_df$class <- factor(val_df$class, levels = c(\"Benign\", \"Early\", \"Pre\", \"Pro\"))\n",
    "\n",
    "train_data <- cbind(as.data.frame(train_features), class_numeric = train_df$class_numeric)\n",
    "validation_data <- cbind(as.data.frame(val_features), class_numeric = val_df$class_numeric)\n",
    "test_data <- cbind(as.data.frame(test_features), class_numeric = test_df$class_numeric)\n",
    "\n",
    "train_data[is.na(train_data)] <- 0\n",
    "validation_data[is.na(validation_data)] <- 0\n",
    "test_data[is.na(test_data)] <- 0\n",
    "\n",
    "# Standardize the features\n",
    "preprocess_params <- preProcess(train_data[, -ncol(train_data)], method = c(\"center\", \"scale\"))\n",
    "train_data_scaled <- predict(preprocess_params, train_data)\n",
    "validation_data_scaled <- predict(preprocess_params, validation_data)\n",
    "test_data_scaled <- predict(preprocess_params, test_data)\n",
    "\n",
    "# Convert class_numeric to factor with proper labels for training and testing\n",
    "train_data_scaled$class <- factor(train_data_scaled$class_numeric, \n",
    "                                levels = 0:3, \n",
    "                                labels = classes)\n",
    "validation_data_scaled$class <- factor(validation_data_scaled$class_numeric, \n",
    "                                     levels = 0:3, \n",
    "                                     labels = classes)\n",
    "test_data_scaled$class <- factor(test_data_scaled$class_numeric, \n",
    "                                levels = 0:3, \n",
    "                                labels = classes)\n",
    "\n",
    "library(caret)\n",
    "library(e1071)\n",
    "\n",
    "\n",
    "# Function to identify NZV features from training data\n",
    "identify_nzv_features <- function(train_data) {\n",
    "  nzv <- nearZeroVar(train_data, saveMetrics = TRUE)\n",
    "  nzv_features <- rownames(nzv)[nzv$zeroVar | nzv$nzv]\n",
    "  \n",
    "  # Log the features being removed and their properties\n",
    "  cat(\"Features removed due to near-zero variance:\\n\")\n",
    "  for(feature in nzv_features) {\n",
    "    cat(sprintf(\"Feature: %s, Variance: %.6f\\n\", \n",
    "                feature, \n",
    "                var(train_data[[feature]], na.rm = TRUE)))\n",
    "  }\n",
    "  \n",
    "  return(nzv_features)\n",
    "}\n",
    "\n",
    "# Function to check for differences in feature variance between datasets\n",
    "check_feature_variance <- function(train_data, val_data, test_data, nzv_features) {\n",
    "  warning_features <- list()\n",
    "  \n",
    "  for(feature in nzv_features) {\n",
    "    if(feature %in% names(val_data) && feature %in% names(test_data)) {\n",
    "      train_var <- var(train_data[[feature]], na.rm = TRUE)\n",
    "      val_var <- var(val_data[[feature]], na.rm = TRUE)\n",
    "      test_var <- var(test_data[[feature]], na.rm = TRUE)\n",
    "      \n",
    "      # Check if feature has significant variance in validation or test\n",
    "      if(val_var > 0.1 || test_var > 0.1) {  # Threshold can be adjusted\n",
    "        warning_features[[feature]] <- list(\n",
    "          train_variance = train_var,\n",
    "          val_variance = val_var,\n",
    "          test_variance = test_var\n",
    "        )\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(warning_features)\n",
    "}\n",
    "\n",
    "# Apply the fixed NZV filtering process\n",
    "# First, identify NZV features from training data only\n",
    "nzv_features <- identify_nzv_features(train_data_scaled[, -ncol(train_data_scaled)])\n",
    "\n",
    "# Check for potential issues in validation and test sets\n",
    "variance_warnings <- check_feature_variance(\n",
    "  train_data_scaled[, -ncol(train_data_scaled)],\n",
    "  validation_data_scaled[, -ncol(validation_data_scaled)],\n",
    "  test_data_scaled[, -ncol(test_data_scaled)],\n",
    "  nzv_features\n",
    ")\n",
    "\n",
    "# Print warnings if any features show different behavior in val/test\n",
    "if(length(variance_warnings) > 0) {\n",
    "  cat(\"\\nWARNING: Some features show different variance patterns in validation/test sets:\\n\")\n",
    "  print(variance_warnings)\n",
    "  cat(\"\\nConsider keeping these features in the model.\\n\")\n",
    "}\n",
    "\n",
    "# Remove NZV features from all datasets\n",
    "train_data_filtered <- train_data_scaled[, !names(train_data_scaled) %in% nzv_features]\n",
    "validation_data_filtered <- validation_data_scaled[, !names(validation_data_scaled) %in% nzv_features]\n",
    "test_data_filtered <- test_data_scaled[, !names(test_data_scaled) %in% nzv_features]\n",
    "\n",
    "head(train_data_filtered)\n",
    "head(validation_data_filtered)\n",
    "head(test_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5875a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T15:50:58.086011Z",
     "iopub.status.busy": "2024-11-19T15:50:58.084752Z",
     "iopub.status.idle": "2024-11-19T16:34:29.517297Z",
     "shell.execute_reply": "2024-11-19T16:34:29.515846Z"
    },
    "papermill": {
     "duration": 2611.441108,
     "end_time": "2024-11-19T16:34:29.519130",
     "exception": false,
     "start_time": "2024-11-19T15:50:58.078022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 of 5 \n",
      "Best model for fold 1 :\n",
      " sigma = 0.01 \n",
      " C = 10 \n",
      " Training accuracy: 0.9807692 \n",
      " Validation accuracy: 0.8865031 \n",
      "\n",
      "Training fold 2 of 5 \n",
      "Best model for fold 2 :\n",
      " sigma = 0.01 \n",
      " C = 100 \n",
      " Training accuracy: 0.9976134 \n",
      " Validation accuracy: 0.8742331 \n",
      "\n",
      "Training fold 3 of 5 \n",
      "Best model for fold 3 :\n",
      " sigma = 0.01 \n",
      " C = 100 \n",
      " Training accuracy: 0.9952038 \n",
      " Validation accuracy: 0.8895706 \n",
      "\n",
      "Training fold 4 of 5 \n",
      "Best model for fold 4 :\n",
      " sigma = 0.01 \n",
      " C = 1000 \n",
      " Training accuracy: 1 \n",
      " Validation accuracy: 0.8757669 \n",
      "\n",
      "Training fold 5 of 5 \n",
      "Best model for fold 5 :\n",
      " sigma = 0.01 \n",
      " C = 100 \n",
      " Training accuracy: 0.9976077 \n",
      " Validation accuracy: 0.9064417 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          fold train_accuracy valid_accuracy sigma    C\n",
      "Accuracy     1      0.9807692      0.8865031  0.01   10\n",
      "Accuracy1    2      0.9976134      0.8742331  0.01  100\n",
      "Accuracy2    3      0.9952038      0.8895706  0.01  100\n",
      "Accuracy3    4      1.0000000      0.8757669  0.01 1000\n",
      "Accuracy4    5      0.9976077      0.9064417  0.01  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training accuracy: 0.9942388 ± 0.007718304 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation accuracy: 0.8865031 ± 0.01296897 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set evaluation:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction Benign Early Pre Pro\n",
      "    Benign    318     1   0   0\n",
      "    Early       5   630   0   0\n",
      "    Pre         0     0 617   0\n",
      "    Pro         0     0   0 515\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.9971          \n",
      "                 95% CI : (0.9938, 0.9989)\n",
      "    No Information Rate : 0.3025          \n",
      "    P-Value [Acc > NIR] : < 2.2e-16       \n",
      "                                          \n",
      "                  Kappa : 0.9961          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: Benign Class: Early Class: Pre Class: Pro\n",
      "Sensitivity                 0.9845       0.9984     1.0000     1.0000\n",
      "Specificity                 0.9994       0.9966     1.0000     1.0000\n",
      "Pos Pred Value              0.9969       0.9921     1.0000     1.0000\n",
      "Neg Pred Value              0.9972       0.9993     1.0000     1.0000\n",
      "Prevalence                  0.1548       0.3025     0.2958     0.2469\n",
      "Detection Rate              0.1524       0.3020     0.2958     0.2469\n",
      "Detection Prevalence        0.1529       0.3044     0.2958     0.2469\n",
      "Balanced Accuracy           0.9920       0.9975     1.0000     1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Validation set evaluation:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction Benign Early Pre Pro\n",
      "    Benign     87     4   2   2\n",
      "    Early      12   191   5   0\n",
      "    Pre         2     2 185   1\n",
      "    Pro         0     0   1 158\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.9525          \n",
      "                 95% CI : (0.9332, 0.9675)\n",
      "    No Information Rate : 0.3021          \n",
      "    P-Value [Acc > NIR] : < 2.2e-16       \n",
      "                                          \n",
      "                  Kappa : 0.9353          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: Benign Class: Early Class: Pre Class: Pro\n",
      "Sensitivity                 0.8614       0.9695     0.9585     0.9814\n",
      "Specificity                 0.9855       0.9626     0.9891     0.9980\n",
      "Pos Pred Value              0.9158       0.9183     0.9737     0.9937\n",
      "Neg Pred Value              0.9749       0.9865     0.9827     0.9939\n",
      "Prevalence                  0.1549       0.3021     0.2960     0.2469\n",
      "Detection Rate              0.1334       0.2929     0.2837     0.2423\n",
      "Detection Prevalence        0.1457       0.3190     0.2914     0.2439\n",
      "Balanced Accuracy           0.9234       0.9661     0.9738     0.9897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set evaluation:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction Benign Early Pre Pro\n",
      "    Benign     70     5   1   3\n",
      "    Early      10   150   3   0\n",
      "    Pre         0     2 148   0\n",
      "    Pro         0     0   1 125\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.9517          \n",
      "                 95% CI : (0.9296, 0.9685)\n",
      "    No Information Rate : 0.3031          \n",
      "    P-Value [Acc > NIR] : < 2.2e-16       \n",
      "                                          \n",
      "                  Kappa : 0.9344          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: Benign Class: Early Class: Pre Class: Pro\n",
      "Sensitivity                 0.8750       0.9554     0.9673     0.9766\n",
      "Specificity                 0.9795       0.9640     0.9945     0.9974\n",
      "Pos Pred Value              0.8861       0.9202     0.9867     0.9921\n",
      "Neg Pred Value              0.9772       0.9803     0.9864     0.9923\n",
      "Prevalence                  0.1544       0.3031     0.2954     0.2471\n",
      "Detection Rate              0.1351       0.2896     0.2857     0.2413\n",
      "Detection Prevalence        0.1525       0.3147     0.2896     0.2432\n",
      "Balanced Accuracy           0.9272       0.9597     0.9809     0.9870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-Specific Statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sensitivity Specificity Pos Pred Value Neg Pred Value Precision\n",
      "Class: Benign   0.8750000   0.9794521      0.8860759      0.9772210 0.8860759\n",
      "Class: Early    0.9554140   0.9639889      0.9202454      0.9802817 0.9202454\n",
      "Class: Pre      0.9673203   0.9945205      0.9866667      0.9864130 0.9866667\n",
      "Class: Pro      0.9765625   0.9974359      0.9920635      0.9923469 0.9920635\n",
      "                 Recall        F1 Prevalence Detection Rate\n",
      "Class: Benign 0.8750000 0.8805031  0.1544402      0.1351351\n",
      "Class: Early  0.9554140 0.9375000  0.3030888      0.2895753\n",
      "Class: Pre    0.9673203 0.9768977  0.2953668      0.2857143\n",
      "Class: Pro    0.9765625 0.9842520  0.2471042      0.2413127\n",
      "              Detection Prevalence Balanced Accuracy\n",
      "Class: Benign            0.1525097         0.9272260\n",
      "Class: Early             0.3146718         0.9597015\n",
      "Class: Pre               0.2895753         0.9809204\n",
      "Class: Pro               0.2432432         0.9869992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Mode:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"sens_spec\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Arguments Passed to confusionMatrix():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9517375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train Accuracy: 0.9971237 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost-Weighted Misclassification Matrix:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Benign Early Pre Pro\n",
      "Benign      0    50  20  90\n",
      "Early     100     0  45   0\n",
      "Pre         0    30   0   0\n",
      "Pro         0     0  10   0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassification Cost: 345 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cost Per Sample: 0.6660232 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost-Sensitive Accuracy: 0.9878905 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1-score 0.9447882 \n"
     ]
    }
   ],
   "source": [
    "# Set the number of folds for cross-validation\n",
    "num_folds <- 5\n",
    "\n",
    "# Create a 5-fold cross-validation partition for the training set\n",
    "set.seed(123)  # For reproducibility\n",
    "folds <- createFolds(train_data_filtered$class, k = num_folds, list = TRUE)\n",
    "\n",
    "# Define the tune grid for SVM hyperparameters (sigma and C)\n",
    "tune_grid <- expand.grid(\n",
    "  sigma = 10^(-3:3),\n",
    "  C = 10^(-3:3)\n",
    ")\n",
    "\n",
    "# Store performance results for each fold\n",
    "cv_results <- data.frame(\n",
    "  fold = integer(),\n",
    "  train_accuracy = numeric(),\n",
    "  valid_accuracy = numeric(),\n",
    "  sigma = numeric(),\n",
    "  C = numeric(),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "# Loop through each fold to perform training and validation\n",
    "for (fold_idx in 1:num_folds) {\n",
    "  cat(\"Training fold\", fold_idx, \"of\", num_folds, \"\\n\")\n",
    "  \n",
    "  # Get the training data for the current fold\n",
    "  train_indices <- unlist(folds[[fold_idx]])\n",
    "  train_fold <- train_data_filtered[train_indices, ]\n",
    "  \n",
    "  # Create class weights for the training fold\n",
    "  class_weights_fold <- sapply(train_fold$class_numeric, \n",
    "                             function(x) class_weights[[as.character(x)]])\n",
    "  \n",
    "  # Initialize tracking for best model\n",
    "  best_valid_accuracy <- -Inf\n",
    "  best_train_accuracy <- -Inf\n",
    "  best_svm_model <- NULL\n",
    "  best_sigma <- NULL\n",
    "  best_C <- NULL\n",
    "  \n",
    "  # Grid search over hyperparameters\n",
    "  for (sigma_val in tune_grid$sigma) {\n",
    "    for (C_val in tune_grid$C) {\n",
    "      # Train the SVM model\n",
    "      tryCatch({\n",
    "        model <- svm(\n",
    "          class ~ ., \n",
    "          data = train_fold[, !names(train_fold) %in% c(\"class_numeric\")],\n",
    "          kernel = \"radial\",\n",
    "          cost = C_val,\n",
    "          gamma = sigma_val,\n",
    "          weights = class_weights_fold,\n",
    "          probability = TRUE,\n",
    "          scale = FALSE  # Disable internal scaling since data is already scaled\n",
    "        )\n",
    "        \n",
    "        # Make predictions on the external validation set\n",
    "        valid_predictions <- predict(model, validation_data_filtered)\n",
    "        valid_accuracy <- confusionMatrix(valid_predictions, \n",
    "                                        validation_data_filtered$class)$overall['Accuracy']\n",
    "        \n",
    "        # Make predictions on training fold\n",
    "        train_predictions <- predict(model, train_fold)\n",
    "        train_accuracy <- confusionMatrix(train_predictions, \n",
    "                                        train_fold$class)$overall['Accuracy']\n",
    "        \n",
    "        # Track the best model based on validation accuracy\n",
    "        if (valid_accuracy > best_valid_accuracy) {\n",
    "          best_valid_accuracy <- valid_accuracy\n",
    "          best_train_accuracy <- train_accuracy\n",
    "          best_svm_model <- model\n",
    "          best_sigma <- sigma_val\n",
    "          best_C <- C_val\n",
    "        }\n",
    "      }, error = function(e) {\n",
    "        cat(\"Error with sigma =\", sigma_val, \"and C =\", C_val, \":\", e$message, \"\\n\")\n",
    "      })\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Store the results for the current fold\n",
    "  cv_results <- rbind(cv_results, data.frame(\n",
    "    fold = fold_idx,\n",
    "    train_accuracy = best_train_accuracy,\n",
    "    valid_accuracy = best_valid_accuracy,\n",
    "    sigma = best_sigma,\n",
    "    C = best_C,\n",
    "    stringsAsFactors = FALSE\n",
    "  ))\n",
    "  \n",
    "  cat(\"Best model for fold\", fold_idx, \":\\n\",\n",
    "      \"sigma =\", best_sigma, \"\\n\",\n",
    "      \"C =\", best_C, \"\\n\",\n",
    "      \"Training accuracy:\", best_train_accuracy, \"\\n\",\n",
    "      \"Validation accuracy:\", best_valid_accuracy, \"\\n\\n\")\n",
    "}\n",
    "\n",
    "# Print cross-validation results summary\n",
    "cat(\"Cross-validation results:\\n\")\n",
    "print(cv_results)\n",
    "\n",
    "# Calculate summary statistics\n",
    "mean_train_accuracy <- mean(cv_results$train_accuracy)\n",
    "sd_train_accuracy <- sd(cv_results$train_accuracy)\n",
    "mean_valid_accuracy <- mean(cv_results$valid_accuracy)\n",
    "sd_valid_accuracy <- sd(cv_results$valid_accuracy)\n",
    "\n",
    "cat(\"\\nSummary Statistics:\\n\")\n",
    "cat(\"Mean training accuracy:\", mean_train_accuracy, \"±\", sd_train_accuracy, \"\\n\")\n",
    "cat(\"Mean validation accuracy:\", mean_valid_accuracy, \"±\", sd_valid_accuracy, \"\\n\")\n",
    "\n",
    "# Select best hyperparameters based on validation performance\n",
    "best_params <- cv_results[which.max(cv_results$valid_accuracy), ]\n",
    "\n",
    "# Train final model on full training data\n",
    "final_svm_model <- svm(\n",
    "  class ~ ., \n",
    "  data = train_data_filtered[, !names(train_data_filtered) %in% c(\"class_numeric\")],\n",
    "  kernel = \"radial\",\n",
    "  cost = best_params$C,\n",
    "  gamma = best_params$sigma,\n",
    "  weights = sapply(train_data_filtered$class_numeric, \n",
    "                  function(x) class_weights[[as.character(x)]]),\n",
    "  probability = TRUE,\n",
    "  scale = FALSE\n",
    ")\n",
    "\n",
    "# Evaluate final model on all datasets\n",
    "evaluate_model <- function(model, data, set_name) {\n",
    "  predictions <- predict(model, data)\n",
    "  cm <- confusionMatrix(predictions, data$class)\n",
    "  cat(\"\\n\", set_name, \"set evaluation:\\n\")\n",
    "  print(cm)\n",
    "  return(cm)\n",
    "}\n",
    "\n",
    "# Final evaluations\n",
    "train_cm <- evaluate_model(final_svm_model, train_data_filtered, \"Training\")\n",
    "validation_cm <- evaluate_model(final_svm_model, validation_data_filtered, \"Validation\")\n",
    "test_cm <- evaluate_model(final_svm_model, test_data_filtered, \"Test\")\n",
    "\n",
    "cat(\"\\nClass-Specific Statistics:\\n\")\n",
    "print(test_cm$byClass)\n",
    "\n",
    "cat(\"\\nConfusion Matrix Mode:\\n\")\n",
    "print(test_cm$mode)\n",
    "\n",
    "cat(\"\\nAdditional Arguments Passed to confusionMatrix():\\n\")\n",
    "print(test_cm$dots)\n",
    "\n",
    "# Print overall accuracy results\n",
    "accuracy_test <- test_cm$overall['Accuracy']\n",
    "accuracy_train <- train_cm$overall['Accuracy']\n",
    "cat(\"Final Test Accuracy:\", accuracy_test, \"\\n\")\n",
    "cat(\"Final Train Accuracy:\", accuracy_train, \"\\n\")\n",
    "\n",
    "# Evaluating misclassification cost \n",
    "# Define the cost matrix (replace with your actual costs)\n",
    "cost_matrix <- matrix(\n",
    "  c(0, 10, 20, 30,   # Benign -> Benign, Early, Pre, Pro\n",
    "    10, 0, 15, 25,    # Early -> Benign, Early, Pre, Pro\n",
    "    20, 15, 0, 10,    # Pre -> Benign, Early, Pre, Pro\n",
    "    30, 25, 10, 0),   # Pro -> Benign, Early, Pre, Pro\n",
    "  nrow = 4, byrow = TRUE\n",
    ")\n",
    "rownames(cost_matrix) <- c(\"Benign\", \"Early\", \"Pre\", \"Pro\")\n",
    "colnames(cost_matrix) <- c(\"Benign\", \"Early\", \"Pre\", \"Pro\")\n",
    "\n",
    "# Extract confusion matrix data\n",
    "cm_table <- test_cm$table\n",
    "classes <- rownames(cm_table)\n",
    "\n",
    "visualize_cost_confusion_matrix <- function(test_cm, cost_matrix) {\n",
    "  # Extract the confusion matrix from caret's result\n",
    "  cm_table <- test_cm$table\n",
    "  \n",
    "  # Create cost-weighted matrix\n",
    "  cost_weighted_cm <- matrix(0, \n",
    "                              nrow = nrow(cost_matrix), \n",
    "                              ncol = ncol(cost_matrix))\n",
    "  \n",
    "  for (i in 1:nrow(cm_table)) {\n",
    "    for (j in 1:ncol(cm_table)) {\n",
    "      if (i != j) {\n",
    "        cost_weighted_cm[i, j] <- cm_table[i, j] * cost_matrix[i, j]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Set row and column names\n",
    "  rownames(cost_weighted_cm) <- rownames(cost_matrix)\n",
    "  colnames(cost_weighted_cm) <- colnames(cost_matrix)\n",
    "  \n",
    "  return(cost_weighted_cm)\n",
    "}\n",
    "\n",
    "# Usage\n",
    "cost_weighted_matrix <- visualize_cost_confusion_matrix(test_cm, cost_matrix)\n",
    "cat(\"\\nCost-Weighted Misclassification Matrix:\\n\")\n",
    "print(cost_weighted_matrix)\n",
    "\n",
    "# Initialize total misclassification cost\n",
    "total_cost <- 0\n",
    "\n",
    "# Calculate total misclassification cost by iterating over confusion matrix entries\n",
    "for (i in 1:nrow(cm_table)) {\n",
    "  for (j in 1:ncol(cm_table)) {\n",
    "    if (i != j) {  # Misclassifications (i != j)\n",
    "      total_cost <- total_cost + cm_table[i, j] * cost_matrix[i, j]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Output the total misclassification cost\n",
    "cat(\"Total Misclassification Cost:\", total_cost, \"\\n\")\n",
    "\n",
    "# Calculate average cost per sample\n",
    "total_samples <- sum(cm_table)\n",
    "avg_cost_per_sample <- total_cost / total_samples\n",
    "cat(\"Average Cost Per Sample:\", avg_cost_per_sample, \"\\n\")\n",
    "\n",
    "# Calculate cost-sensitive accuracy\n",
    "# Compute the maximum possible cost (if all instances were misclassified to the most costly class)\n",
    "max_possible_cost <- sum(cost_matrix) * (total_samples / nrow(cost_matrix))\n",
    "cost_sensitive_accuracy <- 1 - (total_cost / max_possible_cost)\n",
    "cat(\"Cost-Sensitive Accuracy:\", cost_sensitive_accuracy, \"\\n\")\n",
    "\n",
    "# Using cost matrix for cost-weighted F1 calculation\n",
    "f1_scores <- test_cm$byClass[, \"F1\"]\n",
    "weights <- 1 / (diag(cost_matrix) + 1)  # Add 1 to avoid divide-by-zero\n",
    "weights <- weights / sum(weights)\n",
    "weighted_f1 <- sum(f1_scores * weights, na.rm = TRUE)\n",
    "cat(\"Weighted F1-score\", weighted_f1, \"\\n\")\n",
    "\n",
    "# Save final model and results\n",
    "save(final_svm_model, cv_results, train_cm, validation_cm, test_cm,\n",
    "     file = \"final_svm_model.RData\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5938024,
     "sourceId": 9708505,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30749,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "r",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2784.100715,
   "end_time": "2024-11-19T16:34:29.818533",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T15:48:05.717818",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
